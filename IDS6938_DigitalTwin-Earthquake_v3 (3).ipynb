{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN8bvFK+9sMKeXUecMAsFHQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Earthquake Prediction Pipeline using Transformer Models**\n","\n","A comprehensive system for predicting earthquake occurrences using USGS data and deep learning.\n","\n","Core Capabilities:\n","- Automated USGS data collection and processing\n","- Transformer-based sequence modeling\n","- Real-time prediction and evaluation\n","- Continuous model optimization\n","- Performance visualization\n","\n","Process Flow:\n","1. Data Collection\n","   - Pulls earthquake data from USGS API\n","   - Filters by magnitude threshold\n","   - Organizes by date\n","\n","2. Data Processing\n","   - Daily data segmentation\n","   - Feature extraction and scaling\n","   - Sequence creation\n","   - Tensor conversion\n","\n","3. Model Pipeline\n","   - Initial training on historical data\n","   - Daily predictions\n","   - Performance evaluation\n","   - Model optimization\n","   - Checkpoint management\n","\n","4. Monitoring System\n","   - Continuous data collection\n","   - Automated predictions\n","   - Real-time evaluation\n","   - Model updates\n","   - Performance tracking\n","\n","Authors: Stephen Moore / Steven Willhelm / Lynn Yingling\n","Date: November 2024\n","Version: 3.0"],"metadata":{"id":"GPhtaEIJBAVu"}},{"cell_type":"code","source":["# Required imports\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from datetime import datetime, timedelta\n","import requests\n","import os\n","import json\n","import glob\n","import time\n","from sklearn.preprocessing import MinMaxScaler\n","import pickle\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import drive\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)"],"metadata":{"id":"EnteAQmIWVm-","executionInfo":{"status":"ok","timestamp":1732020765119,"user_tz":300,"elapsed":185,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def setup_drive_directory(base_path='earthquake_data'):\n","    \"\"\"Mount Google Drive and create necessary directories\"\"\"\n","    drive.mount('/content/drive')\n","    full_path = f'/content/drive/My Drive/{base_path}'\n","    if not os.path.exists(full_path):\n","        os.makedirs(full_path)\n","        print(f\"Created directory: {full_path}\")\n","    else:\n","        print(f\"Directory already exists: {full_path}\")\n","    return full_path"],"metadata":{"id":"jjF-I5ief3Vk","executionInfo":{"status":"ok","timestamp":1732020766388,"user_tz":300,"elapsed":207,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def fetch_earthquake_data(self, start_time=None, end_time=None, min_magnitude=2.5, use_rolling_window=False):\n","    \"\"\"\n","    Fetch earthquake data from USGS API for a specific time period.\n","\n","    Args:\n","        start_time (datetime): Start of time period\n","        end_time (datetime): End of time period\n","        min_magnitude (float): Minimum earthquake magnitude to include\n","        use_rolling_window (bool): If True, use rolling 24-hour window instead of calendar day\n","\n","    Returns:\n","        pandas DataFrame with earthquake data\n","    \"\"\"\n","    try:\n","        base_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n","\n","        # Format dates for the API\n","        if start_time is None:\n","            start_time = datetime.now() - timedelta(hours=24)\n","        if end_time is None:\n","            end_time = datetime.now()\n","\n","        params = {\n","            'format': 'geojson',\n","            'starttime': start_time.strftime('%Y-%m-%dT%H:%M:%S'),  # Include time\n","            'endtime': end_time.strftime('%Y-%m-%dT%H:%M:%S'),      # Include time\n","            'minmagnitude': min_magnitude,\n","            'orderby': 'time'\n","        }\n","\n","        if use_rolling_window:\n","            print(f\"Fetching data from: {start_time} to {end_time}\")\n","        else:\n","            print(f\"Fetching data for: {start_time.strftime('%Y-%m-%d')}\")\n","\n","        # Make the API request\n","        response = requests.get(base_url, params=params)\n","        response.raise_for_status()\n","\n","        # Parse the JSON response\n","        data = response.json()\n","        earthquakes = data['features']\n","\n","        processed_data = []\n","        for quake in earthquakes:\n","            properties = quake['properties']\n","            coordinates = quake['geometry']['coordinates']\n","\n","            processed_data.append({\n","                'time': datetime.fromtimestamp(properties['time'] / 1000),\n","                'magnitude': properties['mag'],\n","                'place': properties['place'],\n","                'longitude': coordinates[0],\n","                'latitude': coordinates[1],\n","                'depth': coordinates[2],\n","                'type': properties['type'],\n","                'alert': properties.get('alert', 'none'),\n","                'tsunami': properties['tsunami'],\n","                'sig': properties['sig']\n","            })\n","\n","        df = pd.DataFrame(processed_data)\n","\n","        if not df.empty:\n","            df['time'] = pd.to_datetime(df['time'])\n","\n","            if not use_rolling_window:\n","                # Filter to calendar day if not using rolling window\n","                start_day = pd.Timestamp(start_time.strftime('%Y-%m-%d'))\n","                end_day = start_day + pd.Timedelta(days=1)\n","                df = df[\n","                    (df['time'] >= start_day) &\n","                    (df['time'] < end_day)\n","                ]\n","\n","            if len(df) > 0:\n","                print(\"\\nData Collection Summary:\")\n","                print(\"-\" * 30)\n","                print(f\"Total earthquakes collected: {len(df)}\")\n","                print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n","                print(f\"Magnitude range: {df['magnitude'].min():.1f} to {df['magnitude'].max():.1f}\")\n","                print(\"-\" * 30)\n","\n","        return df\n","\n","    except Exception as e:\n","        print(f\"Error fetching data: {e}\")\n","        return None"],"metadata":{"id":"MXTJ_CfRS5k7","executionInfo":{"status":"ok","timestamp":1732020769933,"user_tz":300,"elapsed":200,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["def fetch_training_data(self, start_date, end_date):\n","    \"\"\"Fetch training data for specified date range\"\"\"\n","    df = self.fetch_earthquake_data(\n","        start_time=start_date,\n","        end_time=end_date,\n","        min_magnitude=2.5\n","    )\n","\n","    if df is not None:\n","        # Save with date range\n","        filename = f'earthquake_data_{start_date.strftime(\"%Y%m%d\")}_to_{end_date.strftime(\"%Y%m%d\")}.csv'\n","        filepath = os.path.join(self.drive_path, filename)\n","        df.to_csv(filepath, index=False)\n","\n","        return df, filepath\n","    return None, None"],"metadata":{"id":"tb64jht8SuO9","executionInfo":{"status":"ok","timestamp":1732020774372,"user_tz":300,"elapsed":171,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["def fetch_new_data(self, last_timestamp):\n","    \"\"\"Fetch only new data since last recorded timestamp\"\"\"\n","    df = self.fetch_earthquake_data(\n","        start_time=last_timestamp,\n","        end_time=datetime.now(),\n","        min_magnitude=2.5\n","    )\n","\n","    if df is not None:\n","        # Filter to only new events\n","        new_data = df[df['time'] > last_timestamp]\n","\n","        if len(new_data) > 0:\n","            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","            filename = f'new_data_{timestamp}.csv'\n","            filepath = os.path.join(self.dirs['data'], filename)\n","            new_data.to_csv(filepath, index=False)\n","\n","            return new_data, filepath\n","    return None, None"],"metadata":{"id":"3U37EubPSwvi","executionInfo":{"status":"ok","timestamp":1732020775863,"user_tz":300,"elapsed":217,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["class EarthquakeDataset(Dataset):\n","    \"\"\"\n","    Custom Dataset class for handling earthquake sequence data.\n","\n","    This class creates sequences of earthquake data for training the transformer model.\n","    Each sequence consists of 'seq_length' days of data, with the target being the\n","    subsequent day's earthquake parameters.\n","\n","    Attributes:\n","        features (tensor): Input features for each earthquake event\n","        targets (tensor): Target values for prediction\n","        seq_length (int): Number of days in each sequence\n","\n","    Example:\n","        >>> features = torch.randn(100, 5)  # 100 events with 5 features each\n","        >>> targets = torch.randn(100, 3)   # 3 target parameters for each event\n","        >>> dataset = EarthquakeDataset(features, targets, seq_length=7)\n","        >>> sequence, target = dataset[0]  # Get first sequence and its target\n","    \"\"\"\n","\n","    def __init__(self, features, targets, seq_length):\n","        \"\"\"\n","        Initialize the dataset with features, targets, and sequence length.\n","\n","        Args:\n","            features (torch.Tensor): Input features for each earthquake event\n","            targets (torch.Tensor): Target values for prediction\n","            seq_length (int): Number of days to include in each sequence\n","        \"\"\"\n","        self.features = features\n","        self.targets = targets\n","        self.seq_length = seq_length\n","\n","    def __len__(self):\n","        \"\"\"Return the number of possible sequences in the dataset.\"\"\"\n","        return max(0, len(self.features) - self.seq_length)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Get a sequence of features and its corresponding target.\n","\n","        Args:\n","            idx (int): Index of the sequence to retrieve\n","\n","        Returns:\n","            tuple: (feature_sequence, target) where feature_sequence is a sequence of\n","                  'seq_length' days of data and target is the next day's parameters\n","        \"\"\"\n","        feature_seq = self.features[idx:idx + self.seq_length]\n","        target = self.targets[idx + self.seq_length - 1]\n","        return feature_seq, target"],"metadata":{"id":"miri2HbbefrN","executionInfo":{"status":"ok","timestamp":1732014403340,"user_tz":300,"elapsed":150,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class TransformerPredictor(nn.Module):\n","    \"\"\"\n","    Transformer-based earthquake prediction model.\n","\n","    Architecture:\n","    [Input Events] → [Projection Layer] → [Positional Encoding] →\n","    [Transformer Encoder] → [Output Heads]\n","\n","    Layer Dimensions:\n","    - Input: (batch_size, seq_length, max_earthquakes, input_dim)\n","    - Projection: input_dim → adjusted_dim\n","    - Encoder: adjusted_dim → adjusted_dim\n","    - Output: adjusted_dim → target_dim\n","\n","    Attention Mechanism:\n","    - Multi-head self-attention (num_heads=4)\n","    - Separate attention for temporal and spatial dimensions\n","    - Scaled dot-product attention with dropout\n","\n","    Loss Function:\n","    - MSE loss for parameter prediction\n","    - Binary cross-entropy for earthquake count\n","    - Combined loss with learned weights\n","\n","    Components:\n","    1. Input Projection\n","      - Linear projection to match transformer dimensions\n","      - Handles varying input features\n","\n","    2. Positional Encodings\n","      - Temporal encoding for sequence days\n","      - Spatial encoding for earthquake events\n","      - Learned parameters for both dimensions\n","\n","    3. Transformer Encoder\n","      - Multiple self-attention layers\n","      - Feed-forward networks\n","      - Layer normalization\n","      - Dropout for regularization\n","\n","    4. Prediction Heads\n","      - Separate heads for parameters and count\n","      - Non-linear activation functions\n","      - Confidence estimation\n","\n","    Args:\n","        input_dim (int): Number of input features\n","        hidden_dim (int): Dimension of hidden layers\n","        num_layers (int): Number of transformer layers\n","        num_heads (int): Number of attention heads\n","        max_earthquakes (int): Maximum events per day\n","        min_seq_length (int): Minimum sequence length\n","        max_seq_length (int): Maximum sequence length\n","    \"\"\"\n","\n","    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, max_earthquakes,\n","                 min_seq_length=1, max_seq_length=7):\n","        \"\"\"\n","        Initialize the transformer model.\n","\n","        Args:\n","            input_dim (int): Number of input features\n","            hidden_dim (int): Dimension of hidden layers\n","            num_layers (int): Number of transformer encoder layers\n","            num_heads (int): Number of attention heads\n","            max_earthquakes (int): Maximum number of earthquakes per day\n","            min_seq_length (int, optional): Minimum sequence length. Defaults to 1.\n","            max_seq_length (int, optional): Maximum sequence length. Defaults to 7.\n","        \"\"\"\n","        super().__init__()\n","\n","        # Adjust input dimension to be divisible by num_heads\n","        self.adjusted_dim = ((input_dim // num_heads) + 1) * num_heads\n","        self.min_seq_length = min_seq_length\n","        self.max_seq_length = max_seq_length\n","\n","        # Initial projection to match transformer dimensions\n","        self.input_projection = nn.Linear(input_dim, self.adjusted_dim)\n","\n","        # Positional encodings for temporal and spatial dimensions\n","        self.day_pos_encoding = nn.Parameter(torch.randn(1, max_seq_length, 1, self.adjusted_dim))\n","        self.eq_pos_encoding = nn.Parameter(torch.randn(1, 1, max_earthquakes, self.adjusted_dim))\n","\n","        # Transformer encoder layers\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=self.adjusted_dim,\n","            nhead=num_heads,\n","            dim_feedforward=hidden_dim,\n","            dropout=0.1,\n","            batch_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n","\n","        # Prediction heads\n","        self.earthquake_predictor = nn.Sequential(\n","            nn.Linear(self.adjusted_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, 3)  # [magnitude, latitude, longitude]\n","        )\n","\n","        self.num_eq_predictor = nn.Sequential(\n","            nn.Linear(self.adjusted_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Forward pass of the model.\n","\n","        Args:\n","            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, max_eq, features)\n","\n","        Returns:\n","            tuple: (earthquake_predictions, num_earthquakes) where earthquake_predictions\n","                  contains the predicted parameters for each potential earthquake and\n","                  num_earthquakes is the predicted number of earthquakes\n","        \"\"\"\n","        batch_size, seq_length, max_eq, features = x.shape\n","\n","        # Project input features\n","        x = x.view(-1, features)\n","        x = self.input_projection(x)\n","        x = x.view(batch_size, seq_length, max_eq, -1)\n","\n","        # Add positional encodings\n","        x = x + self.day_pos_encoding[:, :seq_length]  # Temporal encoding\n","        x = x + self.eq_pos_encoding                   # Spatial encoding\n","\n","        # Reshape for transformer\n","        x = x.view(batch_size, seq_length * max_eq, -1)\n","\n","        # Apply transformer encoder\n","        encoded = self.transformer(x)\n","\n","        # Reshape and generate predictions\n","        encoded = encoded.view(batch_size, seq_length, max_eq, -1)\n","        encoded = encoded[:, -1]  # Use last day's encodings\n","\n","        # Generate predictions\n","        eq_predictions = self.earthquake_predictor(encoded)\n","        num_earthquakes = self.num_eq_predictor(encoded.mean(dim=1))\n","\n","        return eq_predictions, num_earthquakes"],"metadata":{"id":"fWLUJMHMfC7x","executionInfo":{"status":"ok","timestamp":1732014405729,"user_tz":300,"elapsed":152,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class EarthquakePipeline:\n","    \"\"\"\n","    Earthquake prediction pipeline using transformer models and USGS data.\n","\n","    This pipeline implements:\n","    1. Automated data collection from USGS\n","    2. Daily data processing and storage\n","    3. Model training and optimization\n","    4. Prediction generation and evaluation\n","    5. Continuous monitoring capabilities\n","\n","    Configuration:\n","    - sequence_length: Number of days used for prediction (default: 7)\n","    - prediction_horizon: Days ahead to predict (default: 1)\n","    - feature_columns: ['magnitude', 'latitude', 'longitude', 'depth', 'sig']\n","    - target_columns: ['magnitude', 'latitude', 'longitude']\n","    - max_earthquakes: Maximum events per day (fixed at 60)\n","\n","    Directory Structure:\n","    /earthquake_data/\n","    ├── data/             # Raw daily earthquake data\n","    │   └── YYYY-MM/     # Organized by year-month\n","    ├── models/          # Saved model checkpoints\n","    ├── predictions/     # Daily prediction outputs\n","    ├── plots/          # Performance visualizations\n","    └── evaluations/    # Evaluation metrics\n","\n","    Data Persistence:\n","    - Daily data stored as CSV files\n","    - Model checkpoints saved after optimization\n","    - Predictions stored as dated CSV files\n","    - Performance metrics saved as JSON\n","    - Visualization plots saved as PNG\n","\n","    Performance Metrics:\n","    - Location error (km)\n","    - Magnitude error\n","    - Precision/Recall/F1\n","    - False positive/negative rates\n","    - Prediction confidence\n","\n","    Optimization Strategy:\n","    - Daily model updates using new data\n","    - Adaptive learning rate\n","    - Early stopping on validation loss\n","    - Performance-based checkpoint saving\n","\n","    Example:\n","        >>> pipeline = EarthquakePipeline('/content/drive/My Drive/earthquake_data')\n","        >>> pipeline.run_pipeline(days_to_process=30, continuous=True)\n","    \"\"\"\n","\n","    def __init__(self, drive_path, seq_length=7, prediction_horizon=1):\n","        \"\"\"\n","        Initialize the pipeline with directories and parameters.\n","\n","        Args:\n","            drive_path (str): Base path in Google Drive for storing all pipeline data\n","            seq_length (int, optional): Number of days in each sequence. Defaults to 7.\n","            prediction_horizon (int, optional): Days ahead to predict. Defaults to 1.\n","        \"\"\"\n","        self.drive_path = drive_path\n","\n","        # Create structured directories\n","        self.dirs = {\n","            'data': os.path.join(drive_path, 'data'),\n","            'models': os.path.join(drive_path, 'models'),\n","            'predictions': os.path.join(drive_path, 'predictions'),\n","            'plots': os.path.join(drive_path, 'plots'),\n","            'evaluations': os.path.join(drive_path, 'evaluations')\n","        }\n","\n","        # Create all directories\n","        for dir_path in self.dirs.values():\n","            os.makedirs(dir_path, exist_ok=True)\n","\n","        # Initialize parameters\n","        self.seq_length = seq_length\n","        self.prediction_horizon = prediction_horizon\n","        self.feature_scaler = MinMaxScaler()\n","        self.target_scaler = MinMaxScaler()\n","        self.model = None\n","        self.feature_columns = ['magnitude', 'latitude', 'longitude', 'depth', 'sig']\n","        self.target_columns = ['magnitude', 'latitude', 'longitude']\n","        self.performance_history = []\n","\n","        # Date tracking system\n","        self.model_dates = {\n","            'last_training_date': None,\n","            'last_optimization_date': None,\n","            'latest_data_date': None,\n","            'prediction_target_date': None\n","        }\n","\n","        # Initialize metadata\n","        self.metadata_path = os.path.join(drive_path, 'pipeline_metadata.json')\n","        self._load_or_create_metadata()\n","\n","    def _create_new_metadata(self):\n","        \"\"\"Create new metadata structure with all required fields.\"\"\"\n","        self.metadata = {\n","            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","            'data_dates': [],  # List to store dates of processed data\n","            'model_versions': [],  # List to store model version information\n","            'predictions': [],  # List to store prediction records\n","            'evaluations': [],  # List to store evaluation results\n","            'pipeline_config': {\n","                'sequence_length': self.seq_length,\n","                'prediction_horizon': self.prediction_horizon,\n","                'feature_columns': self.feature_columns,\n","                'target_columns': self.target_columns\n","            }\n","        }\n","        self._save_metadata()\n","\n","    def _ensure_metadata_structure(self):\n","        \"\"\"Ensure all required fields exist in metadata.\"\"\"\n","        required_fields = {\n","            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","            'data_dates': [],\n","            'model_versions': [],\n","            'predictions': [],\n","            'evaluations': [],\n","            'pipeline_config': {\n","                'sequence_length': self.seq_length,\n","                'prediction_horizon': self.prediction_horizon,\n","                'feature_columns': self.feature_columns,\n","                'target_columns': self.target_columns\n","            }\n","        }\n","\n","        # Add any missing fields\n","        for key, default_value in required_fields.items():\n","            if key not in self.metadata:\n","                self.metadata[key] = default_value\n","                print(f\"Added missing metadata field: {key}\")\n","\n","        # Add any missing nested fields in pipeline_config\n","        if 'pipeline_config' in self.metadata:\n","            for key, value in required_fields['pipeline_config'].items():\n","                if key not in self.metadata['pipeline_config']:\n","                    self.metadata['pipeline_config'][key] = value\n","                    print(f\"Added missing config field: {key}\")\n","\n","    def _save_metadata(self, verbose=False):\n","        \"\"\"\n","        Save pipeline metadata to JSON file.\n","\n","        Handles serialization of metadata including:\n","        - Pipeline configuration and status\n","        - Data tracking and ranges\n","        - Model versions and training history\n","        - Prediction history and performance metrics\n","        - File paths and timestamps\n","        \"\"\"\n","        try:\n","            from datetime import date, datetime\n","            import numpy as np\n","            import pandas as pd\n","\n","            # Create comprehensive metadata structure\n","            metadata = {\n","                'last_update': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","                'pipeline_info': {\n","                    'creation_date': self.metadata.get('creation_date'),\n","                    'sequence_length': self.seq_length,\n","                    'feature_columns': self.feature_columns,\n","                    'target_columns': self.target_columns\n","                },\n","                'data_range': {\n","                    'start': self.model_dates.get('first_data_date'),\n","                    'end': self.model_dates.get('latest_data_date'),\n","                    'total_days_processed': len(self.metadata.get('data_dates', []))\n","                },\n","                'training_info': {\n","                    'last_training': self.model_dates.get('last_training_date'),\n","                    'last_optimization': self.model_dates.get('last_optimization_date'),\n","                    'model_versions': self.metadata.get('model_versions', [])\n","                },\n","                'prediction_stats': self.performance_history,\n","                'file_paths': {\n","                    'data_directory': self.dirs['data'],\n","                    'model_directory': self.dirs['models'],\n","                    'predictions_directory': self.dirs['predictions'],\n","                    'plots_directory': self.dirs['plots']\n","                }\n","            }\n","\n","            # Ensure metadata is JSON serializable\n","            def convert_to_serializable(obj):\n","                if isinstance(obj, (np.integer, np.floating)):\n","                    return float(obj)\n","                elif isinstance(obj, np.ndarray):\n","                    return obj.tolist()\n","                elif isinstance(obj, datetime):\n","                    return obj.strftime('%Y-%m-%d %H:%M:%S')\n","                elif isinstance(obj, date):  # Add handling for date objects\n","                    return obj.strftime('%Y-%m-%d')\n","                elif pd.isnull(obj):  # Changed from isna to isnull\n","                    return None\n","                return obj\n","\n","            # Process all entries recursively\n","            def process_dict(d):\n","                result = {}\n","                for k, v in d.items():\n","                    if isinstance(v, dict):\n","                        result[k] = process_dict(v)\n","                    elif isinstance(v, list):\n","                        result[k] = [\n","                            process_dict(item) if isinstance(item, dict)\n","                            else convert_to_serializable(item)\n","                            for item in v\n","                        ]\n","                    else:\n","                        result[k] = convert_to_serializable(v)\n","                return result\n","\n","            # Process metadata\n","            print(\"\\nProcessing metadata for saving...\")\n","            serializable_metadata = process_dict(metadata)\n","\n","            # Save to file with pretty printing\n","            if verbose:\n","                print(\"Processing metadata for saving...\")\n","            with open(self.metadata_path, 'w') as f:\n","                json.dump(serializable_metadata, f, indent=4)\n","            if verbose:\n","                print(f\"Metadata saved successfully to: {self.metadata_path}\")\n","\n","            # Save a backup copy with timestamp\n","            backup_path = os.path.join(\n","                os.path.dirname(self.metadata_path),\n","                f'metadata_backup_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n","            )\n","            with open(backup_path, 'w') as f:\n","                json.dump(serializable_metadata, f, indent=4)\n","            if verbose:\n","                print(f\"Metadata backup saved to: {backup_path}\")\n","\n","        except Exception as e:\n","            print(f\"\\nError saving metadata: {str(e)}\")\n","            print(\"Metadata path:\", self.metadata_path)\n","            print(\"Error details:\", str(e))\n","\n","            # Additional debugging information\n","            print(\"\\nMetadata structure:\")\n","            for key, value in metadata.items():\n","                print(f\"{key}: {type(value)}\")\n","\n","            raise\n","\n","    def _load_or_create_metadata(self):\n","        \"\"\"Initialize or load existing metadata with all required fields.\"\"\"\n","        if os.path.exists(self.metadata_path):\n","            try:\n","                with open(self.metadata_path, 'r') as f:\n","                    self.metadata = json.load(f)\n","                # Ensure all required fields exist even in loaded metadata\n","                self._ensure_metadata_structure()\n","            except Exception as e:\n","                print(f\"Error loading metadata: {str(e)}. Creating new metadata.\")\n","                self._create_new_metadata()\n","        else:\n","            self._create_new_metadata()\n","\n","    def get_metadata_summary(self):\n","        \"\"\"\n","        Get a summary of current pipeline metadata.\n","\n","        Returns:\n","            dict: Summary of pipeline state and history\n","        \"\"\"\n","        return {\n","            'creation_date': self.metadata['creation_date'],\n","            'data_count': len(self.metadata['data_dates']),\n","            'model_versions': len(self.metadata['model_versions']),\n","            'predictions_made': len(self.metadata['predictions']),\n","            'latest_data': self.model_dates['latest_data_date'],\n","            'last_training': self.model_dates['last_training_date'],\n","            'last_optimization': self.model_dates['last_optimization_date']\n","        }\n","\n","    def fetch_earthquake_data(self, start_time=None, end_time=None, min_magnitude=2.5):\n","        \"\"\"\n","        Fetch earthquake data from USGS API for a specific time period.\n","\n","        Args:\n","            start_time (datetime): Start of time period\n","            end_time (datetime): End of time period\n","            min_magnitude (float): Minimum earthquake magnitude to include\n","        \"\"\"\n","        try:\n","            # Construct the query URL for the USGS API\n","            base_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n","\n","            # Format dates for the API\n","            if start_time is None:\n","                start_time = datetime.now() - timedelta(days=1)\n","            if end_time is None:\n","                end_time = datetime.now()\n","\n","            params = {\n","                'format': 'geojson',\n","                'starttime': start_time.strftime('%Y-%m-%d'),\n","                'endtime': (end_time + timedelta(days=1)).strftime('%Y-%m-%d'),  # Add 1 day to include full end date\n","                'minmagnitude': min_magnitude,\n","                'orderby': 'time'\n","            }\n","\n","            print(f\"Fetching data from {params['starttime']} to {params['endtime']}\")\n","\n","            # Make the API request\n","            response = requests.get(base_url, params=params)\n","            response.raise_for_status()\n","\n","            # Parse the JSON response\n","            data = response.json()\n","            earthquakes = data['features']\n","\n","            processed_data = []\n","            for quake in earthquakes:\n","                properties = quake['properties']\n","                coordinates = quake['geometry']['coordinates']\n","\n","                processed_data.append({\n","                    'time': datetime.fromtimestamp(properties['time'] / 1000),\n","                    'magnitude': properties['mag'],\n","                    'place': properties['place'],\n","                    'longitude': coordinates[0],\n","                    'latitude': coordinates[1],\n","                    'depth': coordinates[2],\n","                    'type': properties['type'],\n","                    'alert': properties.get('alert', 'none'),\n","                    'tsunami': properties['tsunami'],\n","                    'sig': properties['sig']\n","                })\n","\n","            df = pd.DataFrame(processed_data)\n","\n","            if len(df) > 0:\n","                print(\"\\nData Collection Summary:\")\n","                print(\"-\" * 30)\n","                print(f\"Total earthquakes collected: {len(df)}\")\n","                print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n","                print(f\"Magnitude range: {df['magnitude'].min():.1f} to {df['magnitude'].max():.1f}\")\n","                print(\"-\" * 30)\n","\n","            return df\n","\n","        except Exception as e:\n","            print(f\"Error fetching data: {e}\")\n","            return None\n","\n","    def prepare_data(self, df, for_training=True, min_seq_length=1):\n","        \"\"\"\n","        Prepare earthquake data for model training or prediction.\n","\n","        Feature Selection:\n","        - magnitude: Primary indicator of earthquake strength\n","        - latitude/longitude: Geographic location\n","        - depth: Subsurface location\n","        - sig: USGS significance value combining magnitude/impact\n","\n","        Sequence Construction:\n","        1. Group events by day\n","        2. Sort chronologically within each day\n","        3. Select up to max_earthquakes per day\n","        4. Create sliding windows of sequence_length days\n","        5. Generate target values from next day's data\n","\n","        Scaling Methodology:\n","        - Features scaled using MinMaxScaler\n","        - Separate scalers for features and targets\n","        - Scaling ranges preserved for prediction\n","\n","        Error Handling:\n","        - Minimum data threshold enforcement\n","        - Padding for undersized sequences\n","        - Invalid/missing value handling\n","        - Shape validation for tensor creation\n","\n","        Args:\n","            df (pandas.DataFrame): Raw earthquake data\n","            for_training (bool): If True, prepare both features and targets\n","            min_seq_length (int): Minimum sequence length to use\n","\n","        Returns:\n","            tuple: (sequence_tensor, target_tensor) for training\n","                  (sequence_tensor, None) for prediction\n","\n","        Raises:\n","            ValueError: If insufficient data or invalid format\n","        \"\"\"\n","        try:\n","            df = df.copy()\n","\n","            # Ensure we have enough data\n","            if len(df) < 10:  # Minimum threshold for meaningful prediction\n","                print(f\"Warning: Only {len(df)} events available. Need at least 10 for reliable prediction.\")\n","                return None, None\n","\n","            # Ensure time column is datetime\n","            df['time'] = pd.to_datetime(df['time'])\n","            # Create date column for grouping\n","            df['date'] = df['time'].dt.date\n","\n","            # Sort by time to ensure proper sequence order\n","            df = df.sort_values('time')\n","\n","            daily_groups = df.groupby('date')\n","            dates = sorted(df['date'].unique())\n","\n","            print(f\"Processing data from {dates[0]} to {dates[-1]}\")\n","\n","            # Ensure we have enough data for the sequence\n","            seq_length = min(len(dates) - 1, self.seq_length)\n","            seq_length = max(seq_length, min_seq_length)\n","            print(f\"Using sequence length: {seq_length}\")\n","\n","            max_earthquakes = 60  # Fixed size for consistent tensors\n","            print(f\"\\nMaximum earthquakes per day: {max_earthquakes}\")\n","\n","            sequences = []\n","            targets = []\n","\n","            # Create sequences and targets\n","            for i in range(len(dates) - seq_length):\n","                seq_dates = dates[i:i + seq_length]\n","                target_date = dates[i + seq_length]\n","\n","                sequence = []\n","                for date in seq_dates:\n","                    # Get data for this date and select feature columns\n","                    day_data = daily_groups.get_group(date)[self.feature_columns].values\n","\n","                    # Handle empty or undersized data\n","                    if len(day_data) == 0:\n","                        day_data = np.zeros((1, len(self.feature_columns)))\n","\n","                    # Pad or truncate to max_earthquakes\n","                    if len(day_data) < max_earthquakes:\n","                        padding = np.zeros((max_earthquakes - len(day_data),\n","                                        len(self.feature_columns)))\n","                        day_data = np.vstack([day_data, padding])\n","                    else:\n","                        day_data = day_data[:max_earthquakes]\n","\n","                    sequence.append(day_data)\n","\n","                # Handle target data\n","                target_data = daily_groups.get_group(target_date)[self.target_columns].values\n","                if len(target_data) == 0:\n","                    target_data = np.zeros((1, len(self.target_columns)))\n","\n","                if len(target_data) < max_earthquakes:\n","                    padding = np.zeros((max_earthquakes - len(target_data),\n","                                    len(self.target_columns)))\n","                    target_data = np.vstack([target_data, padding])\n","                else:\n","                    target_data = target_data[:max_earthquakes]\n","\n","                sequences.append(sequence)\n","                targets.append(target_data)\n","\n","            if not sequences:  # If no sequences could be created\n","                print(\"Warning: Could not create sequences from available data\")\n","                return None, None\n","\n","            # Convert to arrays and scale\n","            sequences = np.array(sequences)\n","            seq_shape = sequences.shape\n","\n","            # Reshape for scaling\n","            sequences = sequences.reshape(-1, len(self.feature_columns))\n","            # Only fit scaler if we have non-zero data and are in training mode\n","            if sequences.any():  # Check if we have any non-zero values\n","                if for_training:\n","                    sequences = self.feature_scaler.fit_transform(sequences)\n","                else:\n","                    sequences = self.feature_scaler.transform(sequences)\n","            sequences = sequences.reshape(seq_shape)\n","\n","            if not for_training:\n","                return torch.FloatTensor(sequences), None\n","\n","            targets = np.array(targets)\n","            target_shape = targets.shape\n","            targets = targets.reshape(-1, len(self.target_columns))\n","            if targets.any():  # Check if we have any non-zero values\n","                if for_training:\n","                    targets = self.target_scaler.fit_transform(targets)\n","                else:\n","                    targets = self.target_scaler.transform(targets)\n","            targets = targets.reshape(target_shape)\n","\n","            return torch.FloatTensor(sequences), torch.FloatTensor(targets)\n","\n","        except Exception as e:\n","            print(f\"Error in prepare_data: {str(e)}\")\n","            print(\"DataFrame info:\")\n","            print(df.info())\n","            return None, None\n","\n","    def train_model(self, sequence_tensor, target_tensor, epochs=100, batch_size=32, learning_rate=0.001):\n","        \"\"\"\n","        Train the transformer model on earthquake sequences.\n","\n","        Training Process:\n","        1. Model Initialization\n","          - Create transformer architecture if none exists\n","          - Configure input/output dimensions\n","          - Set up attention heads\n","\n","        2. Data Preparation\n","          - Create PyTorch datasets/dataloaders\n","          - Apply batch processing\n","          - Handle sequence padding\n","\n","        3. Training Loop\n","          - Forward pass through transformer\n","          - Loss calculation (MSE + count prediction)\n","          - Backpropagation\n","          - Gradient updates\n","          - Performance tracking\n","\n","        4. Optimization\n","          - Adam optimizer with learning rate scheduling\n","          - Early stopping on validation loss\n","          - Gradient clipping\n","          - Dropout regularization\n","\n","        5. Checkpointing\n","          - Save best model states\n","          - Track training metrics\n","          - Store optimization parameters\n","\n","        Error Handling:\n","        - Graceful handling of GPU memory issues\n","        - Batch failure recovery\n","        - Dimension mismatch detection\n","        - Invalid parameter checks\n","\n","        Args:\n","            sequence_tensor (torch.Tensor): Shape (N, seq_len, max_eq, features)\n","            target_tensor (torch.Tensor): Shape (N, max_eq, target_features)\n","            epochs (int): Maximum training epochs\n","            batch_size (int): Samples per batch\n","            learning_rate (float): Initial learning rate\n","\n","        Returns:\n","            list: Training history (loss values)\n","\n","        Raises:\n","            RuntimeError: For training failures\n","            ValueError: For invalid inputs\n","        \"\"\"\n","        try:\n","            if self.model is None:\n","                print(\"\\nInitializing model...\")\n","                input_dim = len(self.feature_columns)\n","                max_earthquakes = sequence_tensor.shape[2]\n","                num_heads = 4\n","\n","                print(f\"Input dimension: {input_dim}\")\n","                print(f\"Max earthquakes per day: {max_earthquakes}\")\n","                print(f\"Number of attention heads: {num_heads}\")\n","\n","                self.model = TransformerPredictor(\n","                    input_dim=input_dim,\n","                    hidden_dim=64,\n","                    num_layers=2,\n","                    num_heads=num_heads,\n","                    max_earthquakes=max_earthquakes\n","                )\n","\n","            optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n","            criterion = nn.MSELoss()\n","\n","            print(\"\\nPreparing dataset...\")\n","            print(f\"Sequence tensor shape: {sequence_tensor.shape}\")\n","            print(f\"Target tensor shape: {target_tensor.shape}\")\n","\n","            dataset = TensorDataset(sequence_tensor, target_tensor)\n","            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","            print(\"\\nStarting training...\")\n","            best_loss = float('inf')\n","            patience = 5\n","            patience_counter = 0\n","            training_history = []\n","\n","            for epoch in range(epochs):\n","                self.model.train()\n","                total_loss = 0\n","                num_batches = 0\n","\n","                for batch_sequences, batch_targets in dataloader:\n","                    optimizer.zero_grad()\n","                    try:\n","                        predictions, num_eq = self.model(batch_sequences)\n","                        loss = criterion(predictions, batch_targets)\n","                        loss.backward()\n","                        optimizer.step()\n","                        total_loss += loss.item()\n","                        num_batches += 1\n","                    except Exception as e:\n","                        print(f\"Error in batch: {e}\")\n","                        print(f\"Batch shapes - Input: {batch_sequences.shape}, Target: {batch_targets.shape}\")\n","                        continue\n","\n","                avg_loss = total_loss / num_batches if num_batches > 0 else float('inf')\n","                training_history.append(avg_loss)\n","\n","                if avg_loss < best_loss:\n","                    best_loss = avg_loss\n","                    patience_counter = 0\n","                    # Save best model\n","                    self.save_model_checkpoint(epoch, avg_loss)\n","                else:\n","                    patience_counter += 1\n","\n","                if patience_counter >= patience:\n","                    print(f\"\\nEarly stopping at epoch {epoch+1}\")\n","                    break\n","\n","                if (epoch + 1) % 10 == 0:\n","                    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n","\n","            return training_history\n","\n","        except Exception as e:\n","            print(f\"\\nError during training: {str(e)}\")\n","            print(\"Tensor shapes:\")\n","            print(f\"Sequence tensor: {sequence_tensor.shape}\")\n","            print(f\"Target tensor: {target_tensor.shape}\")\n","            raise\n","\n","    def predict_next_day(self, recent_data):\n","        \"\"\"\n","        Generate earthquake predictions for the next day.\n","\n","        Uses the trained model to predict:\n","        - Number of earthquakes\n","        - Location (latitude, longitude)\n","        - Magnitude\n","        for each predicted earthquake\n","\n","        Args:\n","            recent_data (pandas.DataFrame): Recent earthquake data for prediction\n","\n","        Returns:\n","            pandas.DataFrame: Predicted earthquake parameters\n","\n","        Example:\n","            >>> recent_data = pipeline.fetch_earthquake_data('day')\n","            >>> predictions = pipeline.predict_next_day(recent_data)\n","            >>> print(predictions)\n","        \"\"\"\n","        self.model.eval()\n","        with torch.no_grad():\n","            try:\n","                # Prepare the input data\n","                sequence_tensor, _ = self.prepare_data(recent_data, for_training=False)\n","\n","                # Generate predictions\n","                predictions, num_earthquakes_logits = self.model(sequence_tensor)\n","                predictions = predictions[0]  # Remove batch dimension\n","\n","                # Calculate predicted number of earthquakes\n","                historical_mean = len(recent_data)\n","                predicted_prob = torch.sigmoid(num_earthquakes_logits[0]).item()\n","                predicted_count = int(round(predicted_prob * historical_mean))\n","\n","                # Ensure reasonable bounds\n","                min_count = max(1, int(historical_mean * 0.5))  # At least 50% of historical\n","                max_count = min(int(historical_mean * 1.5), len(predictions))  # Can't be more than available predictions\n","                predicted_count = max(min_count, min(predicted_count, max_count))\n","\n","                print(f\"\\nPrediction Details:\")\n","                print(f\"Historical average earthquakes: {historical_mean}\")\n","                print(f\"Predicted probability: {predicted_prob:.3f}\")\n","                print(f\"Available predictions: {len(predictions)}\")\n","                print(f\"Final predicted count: {predicted_count}\")\n","\n","                # Get confidence scores for each prediction\n","                confidence_scores = torch.norm(predictions, dim=1)\n","\n","                # Select top k predictions based on confidence\n","                _, top_indices = torch.topk(confidence_scores, k=min(predicted_count, len(predictions)))\n","                selected_predictions = predictions[top_indices]\n","\n","                # Convert to DataFrame\n","                pred_df = pd.DataFrame(\n","                    self.target_scaler.inverse_transform(selected_predictions),\n","                    columns=['predicted_magnitude', 'predicted_latitude', 'predicted_longitude']\n","                )\n","\n","                return pred_df\n","\n","            except Exception as e:\n","                print(f\"Error during prediction: {str(e)}\")\n","                raise\n","\n","    def evaluate_predictions(self, predictions, actual_data):\n","        \"\"\"\n","        Evaluate prediction accuracy using multiple metrics.\n","\n","        Calculates:\n","        - Location error (km)\n","        - Magnitude error\n","        - False positives/negatives\n","        - Precision, recall, F1 score\n","\n","        Args:\n","            predictions (DataFrame): Predicted earthquakes\n","            actual_data (DataFrame): Actual earthquake data\n","\n","        Returns:\n","            dict: Evaluation metrics\n","\n","        Example:\n","            >>> predictions = pipeline.predict_next_day(recent_data)\n","            >>> next_day_data = pipeline.fetch_earthquake_data('day')\n","            >>> metrics = pipeline.evaluate_predictions(predictions, next_day_data)\n","            >>> print(f\"Average location error: {metrics['avg_location_error']:.2f} km\")\n","        \"\"\"\n","        def haversine_distance(lat1, lon1, lat2, lon2):\n","            \"\"\"Calculate distance between two points in km.\"\"\"\n","            R = 6371  # Earth's radius in km\n","\n","            lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n","            dlat = lat2 - lat1\n","            dlon = lon2 - lon1\n","\n","            a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n","            c = 2 * np.arcsin(np.sqrt(a))\n","            return R * c\n","\n","        try:\n","            # Initialize metrics dictionary\n","            metrics = {\n","                'location_errors': [],\n","                'magnitude_errors': [],\n","                'false_positives': 0,\n","                'false_negatives': 0,\n","                'correct_predictions': 0,\n","                'avg_location_error': 0,\n","                'avg_magnitude_error': 0,\n","                'precision': 0,\n","                'recall': 0,\n","                'f1_score': 0\n","        }\n","\n","            # Check if we have actual data to compare against\n","            if actual_data is None or actual_data.empty:\n","                print(\"No actual data available for evaluation\")\n","                return metrics\n","\n","            # Ensure actual data has required columns\n","            required_columns = ['magnitude', 'latitude', 'longitude']\n","            if not all(col in actual_data.columns for col in required_columns):\n","                print(\"Actual data missing required columns\")\n","                print(\"Available columns:\", actual_data.columns)\n","                print(\"Required columns:\", required_columns)\n","                return {\n","                    'location_errors': [],\n","                    'magnitude_errors': [],\n","                    'false_positives': len(predictions),\n","                    'false_negatives': 0,\n","                    'correct_predictions': 0,\n","                    'avg_location_error': 0,\n","                    'avg_magnitude_error': 0,\n","                    'precision': 0,\n","                    'recall': 0,\n","                    'f1_score': 0\n","                }\n","\n","            # Get actual events\n","            actual_events = actual_data[self.target_columns].values\n","            pred_events = predictions[['predicted_magnitude', 'predicted_latitude',\n","                                    'predicted_longitude']].values\n","\n","            print(\"\\nEvaluation Summary:\")\n","            print(f\"Number of predicted events: {len(pred_events)}\")\n","            print(f\"Number of actual events: {len(actual_events)}\")\n","\n","            # Track matched events\n","            matched_actual_indices = set()\n","\n","            # Evaluate each prediction\n","            for pred_idx, pred_event in enumerate(pred_events):\n","                min_dist = float('inf')\n","                best_match_idx = None\n","\n","                # Find closest actual earthquake\n","                for actual_idx, actual_event in enumerate(actual_events):\n","                    if actual_idx in matched_actual_indices:\n","                        continue\n","\n","                    dist = haversine_distance(\n","                        pred_event[1], pred_event[2],\n","                        actual_event[1], actual_event[2]\n","                    )\n","\n","                    if dist < min_dist:\n","                        min_dist = dist\n","                        best_match_idx = actual_idx\n","\n","                # Process match if found within 100km\n","                if min_dist < 100 and best_match_idx is not None:\n","                    metrics['correct_predictions'] += 1\n","                    metrics['location_errors'].append(min_dist)\n","\n","                    mag_error = abs(pred_event[0] - actual_events[best_match_idx][0])\n","                    metrics['magnitude_errors'].append(mag_error)\n","\n","                    matched_actual_indices.add(best_match_idx)\n","                else:\n","                    metrics['false_positives'] += 1\n","\n","            # Count unmatched actual events\n","            metrics['false_negatives'] = len(actual_events) - len(matched_actual_indices)\n","\n","            # Calculate averages and scores\n","            metrics['avg_location_error'] = np.mean(metrics['location_errors']) if metrics['location_errors'] else 0\n","            metrics['avg_magnitude_error'] = np.mean(metrics['magnitude_errors']) if metrics['magnitude_errors'] else 0\n","\n","            # Calculate precision, recall, F1\n","            if metrics['correct_predictions'] + metrics['false_positives'] > 0:\n","                metrics['precision'] = metrics['correct_predictions'] / (metrics['correct_predictions'] + metrics['false_positives'])\n","            else:\n","                metrics['precision'] = 0\n","\n","            if metrics['correct_predictions'] + metrics['false_negatives'] > 0:\n","                metrics['recall'] = metrics['correct_predictions'] / (metrics['correct_predictions'] + metrics['false_negatives'])\n","            else:\n","                metrics['recall'] = 0\n","\n","            if metrics['precision'] + metrics['recall'] > 0:\n","                metrics['f1_score'] = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'])\n","            else:\n","                metrics['f1_score'] = 0\n","\n","            # Print detailed results\n","            print(\"\\nDetailed Metrics:\")\n","            print(f\"Correct Predictions: {metrics['correct_predictions']}\")\n","            print(f\"False Positives: {metrics['false_positives']}\")\n","            print(f\"False Negatives: {metrics['false_negatives']}\")\n","            print(f\"Average Location Error: {metrics['avg_location_error']:.2f} km\")\n","            print(f\"Average Magnitude Error: {metrics['avg_magnitude_error']:.2f}\")\n","            print(f\"Precision: {metrics['precision']:.3f}\")\n","            print(f\"Recall: {metrics['recall']:.3f}\")\n","            print(f\"F1 Score: {metrics['f1_score']:.3f}\")\n","\n","            return metrics\n","\n","        except Exception as e:\n","            print(f\"Error during evaluation: {str(e)}\")\n","            print(f\"Predictions shape: {predictions.shape}\")\n","            print(f\"Actual data shape: {actual_data.shape if hasattr(actual_data, 'shape') else (0,0)}\")\n","            raise\n","\n","    def optimize_model(self, new_data, current_metrics, learning_rate=0.0001):\n","        \"\"\"\n","        Fine-tune model on new earthquake data.\n","\n","        Optimization Strategy:\n","        1. Data Integration\n","          - Process new earthquake events\n","          - Create sequence tensors\n","          - Apply consistent scaling\n","\n","        2. Loss Analysis\n","          - Calculate baseline performance\n","          - Identify prediction weaknesses\n","          - Weight loss components\n","\n","        3. Model Update\n","          - Incremental learning on new data\n","          - Maintain historical knowledge\n","          - Adaptive learning rates\n","          - Gradient accumulation\n","\n","        4. Performance Monitoring\n","          - Track metrics before/after\n","          - Validate improvements\n","          - Store optimization history\n","          - Update metadata\n","\n","        5. Model Management\n","          - Save optimized state\n","          - Update checkpoint information\n","          - Clean old checkpoints\n","          - Log changes\n","\n","        Optimization Parameters:\n","        - learning_rate: Small for stable updates\n","        - loss_weights: Balanced for all targets\n","        - gradient_clip: Prevent extreme updates\n","        - update_steps: Multiple passes if needed\n","\n","        Args:\n","            new_data (DataFrame): New earthquake events\n","            current_metrics (dict): Current performance metrics\n","            learning_rate (float): Fine-tuning rate\n","\n","        Raises:\n","            ValueError: For invalid optimization attempts\n","            RuntimeError: For optimization failures\n","        \"\"\"\n","        try:\n","            print(\"\\nOptimizing model...\")\n","\n","            if len(new_data) == 0:\n","                print(\"No data available for optimization\")\n","                return\n","\n","            # Create sequence using the new day's data\n","            sequence_data = new_data.copy()\n","            sequence_data['date'] = pd.to_datetime(sequence_data['time']).dt.date\n","\n","            dates = sorted(sequence_data['date'].unique())\n","            print(f\"Optimizing with data from: {dates[0]}\")\n","\n","            max_earthquakes = 60  # Match training dimensions\n","\n","            # Prepare day's data with padding\n","            day_data = sequence_data[self.feature_columns].values\n","            if len(day_data) < max_earthquakes:\n","                padding = np.zeros((max_earthquakes - len(day_data), len(self.feature_columns)))\n","                day_data = np.vstack([day_data, padding])\n","            elif len(day_data) > max_earthquakes:\n","                day_data = day_data[:max_earthquakes]\n","\n","            # Scale features\n","            scaled_features = self.feature_scaler.transform(day_data)\n","            sequence_tensor = torch.FloatTensor(scaled_features).unsqueeze(0).unsqueeze(0)\n","\n","            # Prepare target data\n","            target_data = sequence_data[self.target_columns].values\n","            if len(target_data) < max_earthquakes:\n","                padding = np.zeros((max_earthquakes - len(target_data), len(self.target_columns)))\n","                target_data = np.vstack([target_data, padding])\n","            elif len(target_data) > max_earthquakes:\n","                target_data = target_data[:max_earthquakes]\n","\n","            scaled_targets = self.target_scaler.transform(target_data)\n","            target_tensor = torch.FloatTensor(scaled_targets).unsqueeze(0)\n","\n","            print(f\"Optimization tensors - Input: {sequence_tensor.shape}, Target: {target_tensor.shape}\")\n","\n","            # Perform optimization\n","            optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n","            criterion = nn.MSELoss()\n","\n","            self.model.train()\n","            optimizer.zero_grad()\n","\n","            predictions, num_eq = self.model(sequence_tensor)\n","            loss = criterion(predictions.squeeze(0), target_tensor.squeeze(0))\n","            loss.backward()\n","            optimizer.step()\n","\n","            print(f\"Optimization step completed. Loss: {loss.item():.4f}\")\n","\n","            # Evaluate improvement\n","            self.model.eval()\n","            with torch.no_grad():\n","                new_predictions, _ = self.model(sequence_tensor)\n","                new_loss = criterion(new_predictions.squeeze(0), target_tensor.squeeze(0))\n","\n","            print(f\"Post-optimization loss: {new_loss.item():.4f}\")\n","\n","            # Update metadata\n","            self.model_dates['last_optimization_date'] = dates[0]\n","            self._save_metadata()\n","\n","        except Exception as e:\n","            print(f\"Error during optimization: {str(e)}\")\n","            print(\"\\nError details:\")\n","            print(f\"New data shape: {new_data.shape if hasattr(new_data, 'shape') else len(new_data)}\")\n","            print(\"Metrics:\", current_metrics)\n","            raise\n","\n","    def save_model_checkpoint(self, epoch, loss, metrics=None):\n","        \"\"\"\n","        Save model checkpoint with comprehensive metadata.\n","\n","        Saves:\n","        - Model state\n","        - Scalers\n","        - Training metrics\n","        - Date information\n","\n","        Args:\n","            epoch (int): Current training epoch\n","            loss (float): Current loss value\n","            metrics (dict, optional): Additional metrics to save\n","\n","        Example:\n","            >>> pipeline.save_model_checkpoint(epoch=50, loss=0.123, metrics={'f1': 0.85})\n","        \"\"\"\n","        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","\n","        # Create dated directory structure\n","        if self.model_dates['latest_data_date']:\n","            model_dir = os.path.join(self.dirs['models'],\n","                                   self.model_dates['latest_data_date'][:7])\n","            os.makedirs(model_dir, exist_ok=True)\n","        else:\n","            model_dir = self.dirs['models']\n","\n","        # Save model state\n","        model_path = os.path.join(model_dir, f'model_checkpoint_{timestamp}.pth')\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': self.model.state_dict(),\n","            'feature_scaler': self.feature_scaler,\n","            'target_scaler': self.target_scaler,\n","            'loss': loss,\n","            'metrics': metrics,\n","            'timestamp': timestamp,\n","            'model_dates': self.model_dates.copy()\n","        }\n","\n","        torch.save(checkpoint, model_path)\n","\n","        # Save configuration\n","        config_path = os.path.join(model_dir, f'model_config_{timestamp}.json')\n","        config = {\n","            'timestamp': timestamp,\n","            'epoch': epoch,\n","            'loss': float(loss),\n","            'sequence_length': self.seq_length,\n","            'feature_columns': self.feature_columns,\n","            'target_columns': self.target_columns,\n","            'model_dates': self.model_dates,\n","            'metrics': metrics\n","        }\n","\n","        with open(config_path, 'w') as f:\n","            json.dump(config, f, indent=4)\n","\n","        # Update metadata\n","        self.metadata['model_versions'].append({\n","            'timestamp': timestamp,\n","            'path': model_path,\n","            'config_path': config_path,\n","            'metrics': metrics\n","        })\n","        self._save_metadata()\n","\n","        print(f\"Model checkpoint saved: {model_path}\")\n","        print(f\"Configuration saved: {config_path}\")\n","\n","    def load_latest_model(self):\n","        \"\"\"\n","        Load the most recent model checkpoint.\n","\n","        Handles:\n","        - Finding the latest checkpoint\n","        - Loading model state\n","        - Restoring scalers\n","        - Updating metadata\n","\n","        Returns:\n","            bool: True if model loaded successfully, False otherwise\n","\n","        Example:\n","            >>> success = pipeline.load_latest_model()\n","            >>> if success:\n","            ...     print(\"Model loaded successfully\")\n","        \"\"\"\n","        try:\n","            # Find all model checkpoints\n","            checkpoint_pattern = os.path.join(self.dirs['models'], '**',\n","                                           'model_checkpoint_*.pth')\n","            model_files = glob.glob(checkpoint_pattern, recursive=True)\n","\n","            if not model_files:\n","                print(\"No saved models found\")\n","                return False\n","\n","            # Get the latest checkpoint\n","            latest_model = max(model_files, key=os.path.getctime)\n","            checkpoint = torch.load(latest_model)\n","\n","            # Load model state\n","            self.model.load_state_dict(checkpoint['model_state_dict'])\n","            self.feature_scaler = checkpoint['feature_scaler']\n","            self.target_scaler = checkpoint['target_scaler']\n","\n","            # Update model dates\n","            if 'model_dates' in checkpoint:\n","                self.model_dates.update(checkpoint['model_dates'])\n","\n","            print(f\"Loaded model from: {latest_model}\")\n","            print(f\"Checkpoint epoch: {checkpoint['epoch']}\")\n","            print(f\"Loss: {checkpoint['loss']}\")\n","\n","            if 'metrics' in checkpoint and checkpoint['metrics']:\n","                print(\"\\nMetrics at checkpoint:\")\n","                for metric, value in checkpoint['metrics'].items():\n","                    print(f\"{metric}: {value}\")\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"Error loading model: {str(e)}\")\n","            return False\n","\n","    def save_daily_data(self, df, date=None):\n","        \"\"\"\n","        Save daily earthquake data with comprehensive metadata.\n","\n","        Args:\n","            df (DataFrame): Earthquake data to save\n","            date (str, optional): Specific date for the data. Defaults to current date.\n","\n","        Example:\n","            >>> data = pipeline.fetch_earthquake_data('day')\n","            >>> pipeline.save_daily_data(data, '2024-11-18')\n","        \"\"\"\n","        if date is None:\n","            date = datetime.now().strftime('%Y-%m-%d')\n","\n","        # Create dated directory structure\n","        year_month = date[:7]  # YYYY-MM\n","        data_dir = os.path.join(self.dirs['data'], year_month)\n","        os.makedirs(data_dir, exist_ok=True)\n","\n","        # Save data\n","        filename = f'earthquake_data_{date}.csv'\n","        filepath = os.path.join(data_dir, filename)\n","        df.to_csv(filepath, index=False)\n","\n","        # Save daily summary\n","        summary_path = os.path.join(data_dir, f'summary_{date}.json')\n","        summary = {\n","            'date': date,\n","            'total_earthquakes': len(df),\n","            'magnitude_range': {\n","                'min': float(df['magnitude'].min()),\n","                'max': float(df['magnitude'].max()),\n","                'mean': float(df['magnitude'].mean())\n","            },\n","            'location_bounds': {\n","                'lat': {'min': float(df['latitude'].min()),\n","                       'max': float(df['latitude'].max())},\n","                'lon': {'min': float(df['longitude'].min()),\n","                       'max': float(df['longitude'].max())}\n","            },\n","            'depth_stats': {\n","                'min': float(df['depth'].min()),\n","                'max': float(df['depth'].max()),\n","                'mean': float(df['depth'].mean())\n","            },\n","            'saved_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","        }\n","\n","        with open(summary_path, 'w') as f:\n","            json.dump(summary, f, indent=4)\n","\n","        # Update metadata\n","        self.metadata['data_dates'].append({\n","            'date': date,\n","            'filepath': filepath,\n","            'summary_path': summary_path,\n","            'stats': summary\n","        })\n","        self._save_metadata(verbose=True)\n","\n","        # Update pipeline date tracking\n","        self.model_dates['latest_data_date'] = date\n","\n","        print(f\"Data saved: {filepath}\")\n","        print(f\"Summary saved: {summary_path}\")\n","\n","    def load_daily_data(self, date_str):\n","        \"\"\"\n","        Load earthquake data for a specific date from saved files.\n","\n","        Args:\n","            date_str (str): Date in 'YYYY-MM-DD' format\n","\n","        Returns:\n","            pandas.DataFrame: DataFrame containing earthquake data for the specified date\n","        \"\"\"\n","        try:\n","            # Construct the file path\n","            year_month = date_str[:7]  # YYYY-MM\n","            filename = f'earthquake_data_{date_str}.csv'\n","            filepath = os.path.join(self.dirs['data'], year_month, filename)\n","\n","            if os.path.exists(filepath):\n","                # Load the data\n","                df = pd.read_csv(filepath)\n","\n","                # Convert time column back to datetime\n","                df['time'] = pd.to_datetime(df['time'])\n","\n","                print(f\"Loaded data for {date_str}: {len(df)} earthquakes\")\n","                return df\n","            else:\n","                print(f\"No data file found for {date_str}\")\n","                return None\n","\n","        except Exception as e:\n","            print(f\"Error loading data for {date_str}: {str(e)}\")\n","            return None\n","\n","    def save_predictions(self, predictions, prediction_date, actual_data=None):\n","        \"\"\"\n","        Save predictions and optionally actual data for comparison.\n","\n","        Args:\n","            predictions (DataFrame): Predicted earthquake parameters\n","            prediction_date (str): Date these predictions are for (YYYY-MM-DD)\n","            actual_data (DataFrame, optional): Actual earthquake data if available\n","        \"\"\"\n","        try:\n","            # Create prediction directory structure\n","            year_month = prediction_date[:7]  # YYYY-MM\n","            pred_dir = os.path.join(self.dirs['predictions'], year_month)\n","            os.makedirs(pred_dir, exist_ok=True)\n","\n","            # Save predictions\n","            pred_filename = f'predictions_{prediction_date}.csv'\n","            pred_filepath = os.path.join(pred_dir, pred_filename)\n","            predictions.to_csv(pred_filepath, index=False)\n","            print(f\"\\nPredictions saved to: {pred_filepath}\")\n","\n","            # Save comparison if actual data is available\n","            if actual_data is not None:\n","                comparison = {\n","                    'date': prediction_date,\n","                    'num_predicted': len(predictions),\n","                    'num_actual': len(actual_data),\n","                    'prediction_stats': {\n","                        'magnitude': {\n","                            'mean': float(predictions['predicted_magnitude'].mean()),\n","                            'min': float(predictions['predicted_magnitude'].min()),\n","                            'max': float(predictions['predicted_magnitude'].max())\n","                        },\n","                        'location': {\n","                            'lat_range': [\n","                                float(predictions['predicted_latitude'].min()),\n","                                float(predictions['predicted_latitude'].max())\n","                            ],\n","                            'lon_range': [\n","                                float(predictions['predicted_longitude'].min()),\n","                                float(predictions['predicted_longitude'].max())\n","                            ]\n","                        }\n","                    },\n","                    'actual_stats': {\n","                        'magnitude': {\n","                            'mean': float(actual_data['magnitude'].mean()),\n","                            'min': float(actual_data['magnitude'].min()),\n","                            'max': float(actual_data['magnitude'].max())\n","                        },\n","                        'location': {\n","                            'lat_range': [\n","                                float(actual_data['latitude'].min()),\n","                                float(actual_data['latitude'].max())\n","                            ],\n","                            'lon_range': [\n","                                float(actual_data['longitude'].min()),\n","                                float(actual_data['longitude'].max())\n","                            ]\n","                        }\n","                    },\n","                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","                }\n","\n","                # Save comparison summary\n","                summary_filename = f'comparison_{prediction_date}.json'\n","                summary_filepath = os.path.join(pred_dir, summary_filename)\n","                with open(summary_filepath, 'w') as f:\n","                    json.dump(comparison, f, indent=4)\n","                print(f\"Comparison summary saved to: {summary_filepath}\")\n","\n","                # Update performance history\n","                self.performance_history.append({\n","                    'date': prediction_date,\n","                    'metrics': self.evaluate_predictions(predictions, actual_data),\n","                    'comparison': comparison\n","                })\n","\n","        except Exception as e:\n","            print(f\"\\nError saving predictions: {str(e)}\")\n","            print(f\"Prediction date: {prediction_date}\")\n","            print(f\"Number of predictions: {len(predictions)}\")\n","            if actual_data is not None:\n","                print(f\"Number of actual events: {len(actual_data)}\")\n","            raise\n","\n","    def load_predictions(self, prediction_date):\n","        \"\"\"\n","        Load saved predictions for a specific date.\n","\n","        Args:\n","            prediction_date (str): Date to load predictions for (YYYY-MM-DD)\n","\n","        Returns:\n","            DataFrame: Loaded predictions, or None if not found\n","        \"\"\"\n","        try:\n","            # Construct file path\n","            year_month = prediction_date[:7]  # YYYY-MM\n","            pred_dir = os.path.join(self.dirs['predictions'], year_month)\n","            pred_filepath = os.path.join(pred_dir, f'predictions_{prediction_date}.csv')\n","\n","            if os.path.exists(pred_filepath):\n","                predictions = pd.read_csv(pred_filepath)\n","                print(f\"\\nLoaded predictions for {prediction_date}\")\n","                print(f\"Number of predictions: {len(predictions)}\")\n","                return predictions\n","            else:\n","                print(f\"\\nNo predictions found for {prediction_date}\")\n","                return None\n","\n","        except Exception as e:\n","            print(f\"\\nError loading predictions: {str(e)}\")\n","            print(f\"Attempted to load from: {pred_filepath}\")\n","            return None\n","\n","    def plot_performance_history(self, save=True):\n","        \"\"\"\n","        Create comprehensive visualization of prediction performance metrics.\n","        \"\"\"\n","        if not self.performance_history:\n","            print(\"No performance history available\")\n","            return\n","\n","        try:\n","            # Extract data for plotting\n","            dates = []\n","            location_errors = []\n","            magnitude_errors = []\n","            correct_predictions = []\n","            false_positives = []\n","            false_negatives = []\n","            precision_scores = []\n","            recall_scores = []\n","            f1_scores = []\n","\n","            for entry in self.performance_history:\n","                # Convert string date to datetime if needed\n","                if isinstance(entry['date'], str):\n","                    plot_date = datetime.strptime(entry['date'], '%Y-%m-%d')\n","                else:\n","                    plot_date = entry['date']\n","                dates.append(plot_date)\n","\n","                metrics = entry['metrics']\n","                location_errors.append(metrics.get('avg_location_error', 0))\n","                magnitude_errors.append(metrics.get('avg_magnitude_error', 0))\n","                correct_predictions.append(metrics.get('correct_predictions', 0))\n","                false_positives.append(metrics.get('false_positives', 0))\n","                false_negatives.append(metrics.get('false_negatives', 0))\n","                precision_scores.append(metrics.get('precision', 0))\n","                recall_scores.append(metrics.get('recall', 0))\n","                f1_scores.append(metrics.get('f1_score', 0))\n","\n","            # Create figure with subplots\n","            plt.figure(figsize=(20, 15))\n","\n","            # 1. Error Metrics\n","            plt.subplot(3, 2, 1)\n","            plt.plot(dates, location_errors, marker='o', label='Location Error (km)',\n","                    color='blue')\n","            plt.title('Location Error Over Time')\n","            plt.xlabel('Date')\n","            plt.ylabel('Average Error (km)')\n","            plt.grid(True)\n","            plt.xticks(rotation=45)\n","            plt.legend()\n","\n","            # 2. Magnitude Error\n","            plt.subplot(3, 2, 2)\n","            plt.plot(dates, magnitude_errors, marker='o', label='Magnitude Error',\n","                    color='red')\n","            plt.title('Magnitude Error Over Time')\n","            plt.xlabel('Date')\n","            plt.ylabel('Average Error')\n","            plt.grid(True)\n","            plt.xticks(rotation=45)\n","            plt.legend()\n","\n","            # 3. Prediction Counts\n","            plt.subplot(3, 2, 3)\n","            x = np.arange(len(dates))\n","            width = 0.25\n","            plt.bar(x - width, correct_predictions, width, label='Correct',\n","                  color='green')\n","            plt.bar(x, false_positives, width, label='False Positives',\n","                  color='red', alpha=0.7)\n","            plt.bar(x + width, false_negatives, width, label='False Negatives',\n","                  color='orange', alpha=0.7)\n","            plt.title('Prediction Breakdown Over Time')\n","            plt.xlabel('Date')\n","            plt.ylabel('Count')\n","            # Use datetime objects for date formatting\n","            plt.xticks(x, [d.strftime('%Y-%m-%d') for d in dates], rotation=45)\n","            plt.legend()\n","            plt.grid(True)\n","\n","            # 4. Precision-Recall\n","            plt.subplot(3, 2, 4)\n","            plt.plot(dates, precision_scores, marker='o', label='Precision',\n","                    color='purple')\n","            plt.plot(dates, recall_scores, marker='s', label='Recall',\n","                    color='green')\n","            plt.title('Precision and Recall Over Time')\n","            plt.xlabel('Date')\n","            plt.ylabel('Score')\n","            plt.grid(True)\n","            plt.xticks(rotation=45)\n","            plt.legend()\n","\n","            # 5. F1 Score\n","            plt.subplot(3, 2, 5)\n","            plt.plot(dates, f1_scores, marker='o', label='F1 Score',\n","                    color='blue')\n","            plt.fill_between(dates, 0, f1_scores, alpha=0.2, color='blue')\n","            plt.title('F1 Score Over Time')\n","            plt.xlabel('Date')\n","            plt.ylabel('Score')\n","            plt.grid(True)\n","            plt.xticks(rotation=45)\n","            plt.legend()\n","\n","            # 6. Summary Statistics\n","            plt.subplot(3, 2, 6)\n","            summary_stats = {\n","                'Avg Location Error': np.mean(location_errors),\n","                'Avg Magnitude Error': np.mean(magnitude_errors),\n","                'Avg Precision': np.mean(precision_scores),\n","                'Avg Recall': np.mean(recall_scores),\n","                'Avg F1 Score': np.mean(f1_scores)\n","            }\n","\n","            y_pos = np.arange(len(summary_stats))\n","            plt.barh(y_pos, list(summary_stats.values()))\n","            plt.yticks(y_pos, list(summary_stats.keys()))\n","            plt.title('Overall Performance Summary')\n","            plt.xlabel('Average Value')\n","\n","            plt.tight_layout()\n","\n","            if save:\n","                # Save plots with timestamp\n","                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","                plot_path = os.path.join(self.dirs['plots'],\n","                                      f'performance_history_{timestamp}.png')\n","                plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n","\n","                # Save summary report\n","                report_path = os.path.join(self.dirs['plots'],\n","                                        f'performance_summary_{timestamp}.txt')\n","                with open(report_path, 'w') as f:\n","                    f.write(\"Performance Summary\\n\")\n","                    f.write(\"==================\\n\\n\")\n","                    for metric, value in summary_stats.items():\n","                        f.write(f\"{metric}: {value:.4f}\\n\")\n","                    f.write(\"\\nDetailed Metrics Available in Plot\\n\")\n","\n","                plt.close()\n","\n","                print(f\"\\nPerformance plot saved: {plot_path}\")\n","                print(f\"Summary report saved: {report_path}\")\n","\n","            # Print summary to console\n","            print(\"\\nPerformance Summary:\")\n","            print(\"===================\")\n","            for metric, value in summary_stats.items():\n","                print(f\"{metric}: {value:.4f}\")\n","\n","        except Exception as e:\n","            print(f\"Error creating performance plots: {str(e)}\")\n","            # Additional debugging information\n","            if self.performance_history:\n","                print(\"\\nFirst entry in performance history:\")\n","                print(f\"Date type: {type(self.performance_history[0]['date'])}\")\n","                print(f\"Date value: {self.performance_history[0]['date']}\")\n","            raise\n","\n","    def run_pipeline(self, days_to_process=30, continuous=False, update_interval=3600):\n","        \"\"\"\n","        Execute the complete earthquake prediction pipeline.\n","        \"\"\"\n","        try:\n","            print(\"\\nInitializing Earthquake Prediction Pipeline\")\n","            print(\"==========================================\")\n","\n","            # Initialize dates\n","            current_time = datetime.now()\n","            end_date = current_time - timedelta(days=1)  # Process until yesterday\n","            start_date = end_date - timedelta(days=days_to_process)\n","            print(f\"\\nProcessing historical period: {start_date.date()} to {end_date.date()}\")\n","\n","            # First collect initial training data (7 days)\n","            initial_end = start_date + timedelta(days=7)\n","            print(f\"\\nCollecting initial training data: {start_date.date()} to {initial_end.date()}\")\n","\n","            # Fetch and store initial training data\n","            daily_data = []\n","            current = start_date\n","            while current <= initial_end:\n","                data = self.fetch_earthquake_data(\n","                    start_time=current,\n","                    min_magnitude=2.5\n","                )\n","                if data is not None and len(data) >= 10:  # Ensure minimum data threshold\n","                    self.save_daily_data(data, current.strftime('%Y-%m-%d'))\n","                    daily_data.append(data)\n","                current += timedelta(days=1)\n","\n","            # Train initial model if we have enough data\n","            if len(daily_data) >= self.seq_length:\n","                print(\"\\nPreparing initial training data...\")\n","                training_data = pd.concat(daily_data)\n","                sequence_tensor, target_tensor = self.prepare_data(training_data)\n","\n","                if sequence_tensor is not None and target_tensor is not None:\n","                    print(\"\\nTraining initial model...\")\n","                    self.train_model(sequence_tensor, target_tensor)\n","                    self.model_dates['last_training_date'] = initial_end.strftime('%Y-%m-%d')\n","                else:\n","                    raise ValueError(\"Could not prepare training data\")\n","            else:\n","                raise ValueError(\"Insufficient data for initial training\")\n","\n","            # Process remaining historical days\n","            current = initial_end + timedelta(days=1)\n","            while current <= end_date:\n","                print(f\"\\nProcessing historical day: {current.date()}\")\n","\n","                data = self.fetch_earthquake_data(\n","                    start_time=current,\n","                    min_magnitude=2.5\n","                )\n","\n","                if data is not None and len(data) >= 10:\n","                    self.save_daily_data(data, current.strftime('%Y-%m-%d'))\n","\n","                    # Generate and evaluate predictions\n","                    try:\n","                        sequence_tensor, _ = self.prepare_data(data, for_training=False)\n","                        if sequence_tensor is not None:\n","                            predictions = self.predict_next_day(data)\n","                            if predictions is not None:\n","                                next_day = current + timedelta(days=1)\n","                                self.save_predictions(predictions, next_day.strftime('%Y-%m-%d'))\n","\n","                                # If we have data for the next day, evaluate predictions\n","                                if next_day <= end_date:\n","                                    next_day_data = self.fetch_earthquake_data(\n","                                        start_time=next_day,\n","                                        min_magnitude=2.5\n","                                    )\n","                                    if next_day_data is not None and len(next_day_data) >= 10:\n","                                        metrics = self.evaluate_predictions(predictions, next_day_data)\n","                                        self.optimize_model(next_day_data, metrics)\n","                    except Exception as e:\n","                        print(f\"Error processing day {current.date()}: {str(e)}\")\n","                        continue\n","\n","                current += timedelta(days=1)\n","\n","            print(\"\\nHistorical data processing completed\")\n","\n","            # Switch to continuous monitoring if requested\n","            if continuous:\n","                print(\"\\nSwitching to continuous monitoring mode...\")\n","\n","                # Wait until we have a complete day of data\n","                current_time = datetime.now()\n","                if current_time.hour < 23:  # If it's not near the end of the day\n","                    next_processing_time = datetime(\n","                        current_time.year,\n","                        current_time.month,\n","                        current_time.day,\n","                        23, 0  # Set to 11 PM\n","                    )\n","\n","                    wait_seconds = (next_processing_time - current_time).total_seconds()\n","                    print(f\"\\nCurrent time: {current_time}\")\n","                    print(f\"Waiting until {next_processing_time} to begin continuous monitoring...\")\n","                    print(f\"Will start continuous monitoring in {wait_seconds/3600:.1f} hours\")\n","\n","                    time.sleep(wait_seconds)\n","\n","                # Start continuous monitoring\n","                self.run_continuous_monitoring(update_interval=update_interval)\n","\n","        except Exception as e:\n","            print(f\"\\nError in pipeline execution: {str(e)}\")\n","            print(f\"Error details: {str(e)}\")\n","            raise\n","\n","    def run_continuous_monitoring(self, update_interval=3600):\n","        \"\"\"Run continuous monitoring of earthquakes and predictions.\"\"\"\n","        try:\n","            print(\"\\nStarting continuous monitoring...\")\n","\n","            while True:\n","                current_time = datetime.now()\n","\n","                # Only process complete days\n","                if current_time.hour < 23:\n","                    wait_seconds = (datetime(\n","                        current_time.year,\n","                        current_time.month,\n","                        current_time.day,\n","                        23, 0\n","                    ) - current_time).total_seconds()\n","\n","                    print(f\"\\nWaiting for complete day data...\")\n","                    print(f\"Next processing time: {current_time + timedelta(seconds=wait_seconds)}\")\n","                    time.sleep(wait_seconds)\n","                    continue\n","\n","                # Process the previous complete day\n","                process_date = current_time - timedelta(days=1)\n","                print(f\"\\nProcessing data for: {process_date.date()}\")\n","\n","                data = self.fetch_earthquake_data(\n","                    start_time=process_date,\n","                    min_magnitude=2.5\n","                )\n","\n","                if data is not None and len(data) >= 10:\n","                    try:\n","                        predictions = self.predict_next_day(data)\n","                        if predictions is not None:\n","                            self.save_predictions(\n","                                predictions,\n","                                current_time.strftime('%Y-%m-%d')\n","                            )\n","                            print(\"Predictions generated and saved successfully\")\n","                    except Exception as e:\n","                        print(f\"Error during prediction: {str(e)}\")\n","\n","                # Wait for next update interval\n","                print(f\"\\nWaiting {update_interval} seconds until next check...\")\n","                time.sleep(update_interval)\n","\n","        except KeyboardInterrupt:\n","            print(\"\\nContinuous monitoring stopped by user\")\n","        except Exception as e:\n","            print(f\"\\nError in continuous monitoring: {str(e)}\")\n","            raise"],"metadata":{"id":"sGdmtfqtfEif"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# First, mount Google Drive (if using Colab)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set up base directory in Google Drive\n","base_path = '/content/drive/My Drive/earthquake_data'\n","\n","# Initialize the pipeline\n","pipeline = EarthquakePipeline(drive_path=base_path)\n","\n","# Run historical processing then switch to continuous\n","pipeline.run_pipeline(days_to_process=30, continuous=True, update_interval=3600)\n","\n","# OR just run continuous monitoring with existing model\n","# pipeline.run_continuous_monitoring(update_interval=3600)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxAsrY__HMfc","outputId":"2a00272f-9c80-4002-e63d-cb845e2cb858"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","\n","Processing metadata for saving...\n","\n","Initializing Earthquake Prediction Pipeline\n","==========================================\n","\n","Processing historical period: 2024-10-19 to 2024-11-18\n","\n","Collecting initial training data: 2024-10-19 to 2024-10-26\n","Fetching data from 2024-10-19 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 1247\n","Date range: 2024-10-19 00:18:53.783000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133609.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-19.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-19.json\n","Fetching data from 2024-10-20 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 1208\n","Date range: 2024-10-20 00:51:12.994000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133610.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-20.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-20.json\n","Fetching data from 2024-10-21 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 1156\n","Date range: 2024-10-21 00:06:39.589000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133611.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-21.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-21.json\n","Fetching data from 2024-10-22 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 1107\n","Date range: 2024-10-22 00:24:20.266000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133611.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-22.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-22.json\n","Fetching data from 2024-10-23 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 1071\n","Date range: 2024-10-23 00:13:03.221000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133612.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-23.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-23.json\n","Fetching data from 2024-10-24 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 1030\n","Date range: 2024-10-24 00:38:44.840000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133613.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-24.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-24.json\n","Fetching data from 2024-10-25 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 979\n","Date range: 2024-10-25 00:00:31.838000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133613.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-25.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-25.json\n","Fetching data from 2024-10-26 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 937\n","Date range: 2024-10-26 01:17:23.700000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133614.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-26.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-26.json\n","\n","Preparing initial training data...\n","Processing data from 2024-10-19 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Training initial model...\n","\n","Initializing model...\n","Input dimension: 5\n","Max earthquakes per day: 60\n","Number of attention heads: 4\n","\n","Preparing dataset...\n","Sequence tensor shape: torch.Size([25, 7, 60, 5])\n","Target tensor shape: torch.Size([25, 60, 3])\n","\n","Starting training...\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133616.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133616.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133618.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133618.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133619.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133619.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133621.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133621.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133622.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133622.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133624.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133624.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133626.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133626.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133627.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133627.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133629.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133629.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133630.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133630.json\n","Epoch 10/100, Loss: 0.1901\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133631.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133631.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133633.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133633.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133634.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133634.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133635.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133635.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133637.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133637.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133639.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133639.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133641.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133641.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133642.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133642.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133644.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133644.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133645.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133645.json\n","Epoch 20/100, Loss: 0.0864\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133646.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133646.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133648.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133648.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133649.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133649.json\n","\n","Processing metadata for saving...\n","Model checkpoint saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_checkpoint_20241119_133650.pth\n","Configuration saved: /content/drive/My Drive/earthquake_data/models/2024-10/model_config_20241119_133650.json\n","\n","Early stopping at epoch 29\n","\n","Processing historical day: 2024-10-27\n","Fetching data from 2024-10-27 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 901\n","Date range: 2024-10-27 00:31:15.345000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133659.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-27.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-27.json\n","Processing data from 2024-10-27 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-10-27 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 901\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 450\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-10/predictions_2024-10-28.csv\n","Fetching data from 2024-10-28 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 856\n","Date range: 2024-10-28 00:14:44.985000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 856\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 856\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-10-28\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0929\n","Post-optimization loss: 0.0889\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-10-28\n","Fetching data from 2024-10-28 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 856\n","Date range: 2024-10-28 00:14:44.985000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133701.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-28.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-28.json\n","Processing data from 2024-10-28 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-10-28 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 856\n","Predicted probability: 0.631\n","Available predictions: 60\n","Final predicted count: 428\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-10/predictions_2024-10-29.csv\n","Fetching data from 2024-10-29 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 801\n","Date range: 2024-10-29 00:31:04.790000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 801\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 801\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-10-29\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0966\n","Post-optimization loss: 0.0879\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-10-29\n","Fetching data from 2024-10-29 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 801\n","Date range: 2024-10-29 00:31:04.790000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133702.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-29.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-29.json\n","Processing data from 2024-10-29 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-10-29 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 801\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 400\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-10/predictions_2024-10-30.csv\n","Fetching data from 2024-10-30 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 755\n","Date range: 2024-10-30 01:00:05.880000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 755\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 755\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-10-30\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0941\n","Post-optimization loss: 0.0871\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-10-30\n","Fetching data from 2024-10-30 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 755\n","Date range: 2024-10-30 01:00:05.880000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133704.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-30.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-30.json\n","Processing data from 2024-10-30 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-10-30 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 755\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 377\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-10/predictions_2024-10-31.csv\n","Fetching data from 2024-10-31 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 721\n","Date range: 2024-10-31 01:11:38.609000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 721\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 721\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-10-31\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0889\n","Post-optimization loss: 0.0862\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-10-31\n","Fetching data from 2024-10-31 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 721\n","Date range: 2024-10-31 01:11:38.609000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133706.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-10/earthquake_data_2024-10-31.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-10/summary_2024-10-31.json\n","Processing data from 2024-10-31 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-10-31 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 721\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 360\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-01.csv\n","Fetching data from 2024-11-01 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 685\n","Date range: 2024-11-01 01:05:29.721000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 685\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 685\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-01\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0874\n","Post-optimization loss: 0.0854\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-01\n","Fetching data from 2024-11-01 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 685\n","Date range: 2024-11-01 01:05:29.721000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133708.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-01.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-01.json\n","Processing data from 2024-11-01 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-01 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 685\n","Predicted probability: 0.631\n","Available predictions: 60\n","Final predicted count: 342\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-02.csv\n","Fetching data from 2024-11-02 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 645\n","Date range: 2024-11-02 01:36:48.147000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 645\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 645\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-02\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0910\n","Post-optimization loss: 0.0846\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-02\n","Fetching data from 2024-11-02 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 645\n","Date range: 2024-11-02 01:36:48.147000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133710.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-02.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-02.json\n","Processing data from 2024-11-02 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-02 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 645\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 322\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-03.csv\n","Fetching data from 2024-11-03 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 587\n","Date range: 2024-11-03 00:11:52 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 587\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 587\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-03\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0934\n","Post-optimization loss: 0.0838\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-03\n","Fetching data from 2024-11-03 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 587\n","Date range: 2024-11-03 00:11:52 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133711.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-03.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-03.json\n","Processing data from 2024-11-03 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-03 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 587\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 293\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-04.csv\n","Fetching data from 2024-11-04 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 542\n","Date range: 2024-11-04 00:00:02.708000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 542\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 542\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-04\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0852\n","Post-optimization loss: 0.0831\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-04\n","Fetching data from 2024-11-04 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 542\n","Date range: 2024-11-04 00:00:02.708000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133712.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-04.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-04.json\n","Processing data from 2024-11-04 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-04 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 542\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 271\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-05.csv\n","Fetching data from 2024-11-05 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 511\n","Date range: 2024-11-05 00:31:12.632000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 511\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 511\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-05\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0859\n","Post-optimization loss: 0.0824\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-05\n","Fetching data from 2024-11-05 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 511\n","Date range: 2024-11-05 00:31:12.632000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133714.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-05.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-05.json\n","Processing data from 2024-11-05 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-05 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 511\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 255\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-06.csv\n","Fetching data from 2024-11-06 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 480\n","Date range: 2024-11-06 01:14:37.330000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 480\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 480\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-06\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0875\n","Post-optimization loss: 0.0817\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-06\n","Fetching data from 2024-11-06 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 480\n","Date range: 2024-11-06 01:14:37.330000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133715.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-06.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-06.json\n","Processing data from 2024-11-06 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-06 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 480\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 240\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-07.csv\n","Fetching data from 2024-11-07 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 454\n","Date range: 2024-11-07 00:27:10.605000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 454\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 454\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-07\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0863\n","Post-optimization loss: 0.0811\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-07\n","Fetching data from 2024-11-07 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 454\n","Date range: 2024-11-07 00:27:10.605000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133716.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-07.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-07.json\n","Processing data from 2024-11-07 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-07 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 454\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 227\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-08.csv\n","Fetching data from 2024-11-08 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 420\n","Date range: 2024-11-08 00:04:56.863000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 420\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 420\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-08\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0837\n","Post-optimization loss: 0.0805\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-08\n","Fetching data from 2024-11-08 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 420\n","Date range: 2024-11-08 00:04:56.863000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133717.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-08.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-08.json\n","Processing data from 2024-11-08 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-08 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 420\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 210\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-09.csv\n","Fetching data from 2024-11-09 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 371\n","Date range: 2024-11-09 00:31:04.739000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 371\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 371\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-09\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0853\n","Post-optimization loss: 0.0799\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-09\n","Fetching data from 2024-11-09 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 371\n","Date range: 2024-11-09 00:31:04.739000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133718.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-09.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-09.json\n","Processing data from 2024-11-09 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-09 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 371\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 185\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-10.csv\n","Fetching data from 2024-11-10 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 331\n","Date range: 2024-11-10 00:48:32.393000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 331\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 331\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-10\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0839\n","Post-optimization loss: 0.0793\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-10\n","Fetching data from 2024-11-10 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 331\n","Date range: 2024-11-10 00:48:32.393000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.8\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133719.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-10.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-10.json\n","Processing data from 2024-11-10 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-10 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 331\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 165\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-11.csv\n","Fetching data from 2024-11-11 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 294\n","Date range: 2024-11-11 00:39:54.786000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.6\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 294\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 294\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-11\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0806\n","Post-optimization loss: 0.0788\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-11\n","Fetching data from 2024-11-11 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 294\n","Date range: 2024-11-11 00:39:54.786000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.6\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133720.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-11.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-11.json\n","Processing data from 2024-11-11 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-11 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 294\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 147\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-12.csv\n","Fetching data from 2024-11-12 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 271\n","Date range: 2024-11-12 00:59:35.440000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.6\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 271\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 271\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-12\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0829\n","Post-optimization loss: 0.0783\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-12\n","Fetching data from 2024-11-12 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 271\n","Date range: 2024-11-12 00:59:35.440000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.6\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133721.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-12.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-12.json\n","Processing data from 2024-11-12 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-12 to 2024-11-19\n","Using sequence length: 7\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 271\n","Predicted probability: 0.632\n","Available predictions: 60\n","Final predicted count: 135\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-13.csv\n","Fetching data from 2024-11-13 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 246\n","Date range: 2024-11-13 00:07:36.477000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.6\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 246\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 246\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-13\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0832\n","Post-optimization loss: 0.0779\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-13\n","Fetching data from 2024-11-13 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 246\n","Date range: 2024-11-13 00:07:36.477000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.6\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133722.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-13.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-13.json\n","Processing data from 2024-11-13 to 2024-11-19\n","Using sequence length: 6\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-13 to 2024-11-19\n","Using sequence length: 6\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 246\n","Predicted probability: 0.628\n","Available predictions: 60\n","Final predicted count: 123\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-14.csv\n","Fetching data from 2024-11-14 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 205\n","Date range: 2024-11-14 00:02:13.676000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.6\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 205\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 205\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-14\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0834\n","Post-optimization loss: 0.0774\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-14\n","Fetching data from 2024-11-14 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 205\n","Date range: 2024-11-14 00:02:13.676000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.6\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133723.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-14.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-14.json\n","Processing data from 2024-11-14 to 2024-11-19\n","Using sequence length: 5\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-14 to 2024-11-19\n","Using sequence length: 5\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 205\n","Predicted probability: 0.633\n","Available predictions: 60\n","Final predicted count: 102\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-15.csv\n","Fetching data from 2024-11-15 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 166\n","Date range: 2024-11-15 00:04:01.790000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.6\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 166\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 166\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-15\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0813\n","Post-optimization loss: 0.0770\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-15\n","Fetching data from 2024-11-15 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 166\n","Date range: 2024-11-15 00:04:01.790000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.6\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133724.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-15.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-15.json\n","Processing data from 2024-11-15 to 2024-11-19\n","Using sequence length: 4\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-15 to 2024-11-19\n","Using sequence length: 4\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 166\n","Predicted probability: 0.629\n","Available predictions: 60\n","Final predicted count: 83\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-16.csv\n","Fetching data from 2024-11-16 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 117\n","Date range: 2024-11-16 02:17:09.474000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.1\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 117\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 117\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-16\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0828\n","Post-optimization loss: 0.0767\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-16\n","Fetching data from 2024-11-16 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 117\n","Date range: 2024-11-16 02:17:09.474000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.1\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133724.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-16.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-16.json\n","Processing data from 2024-11-16 to 2024-11-19\n","Using sequence length: 3\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-16 to 2024-11-19\n","Using sequence length: 3\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 117\n","Predicted probability: 0.625\n","Available predictions: 60\n","Final predicted count: 60\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-17.csv\n","Fetching data from 2024-11-17 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 75\n","Date range: 2024-11-17 00:00:59.070000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.1\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 60\n","Number of actual events: 75\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 60\n","False Negatives: 75\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-17\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.0822\n","Post-optimization loss: 0.0763\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-17\n","Fetching data from 2024-11-17 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 75\n","Date range: 2024-11-17 00:00:59.070000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 6.1\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133725.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-17.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-17.json\n","Processing data from 2024-11-17 to 2024-11-19\n","Using sequence length: 2\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-17 to 2024-11-19\n","Using sequence length: 2\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 75\n","Predicted probability: 0.627\n","Available predictions: 60\n","Final predicted count: 47\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-18.csv\n","Fetching data from 2024-11-18 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 44\n","Date range: 2024-11-18 00:00:44.709000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 5.6\n","------------------------------\n","\n","Evaluation Summary:\n","Number of predicted events: 47\n","Number of actual events: 44\n","\n","Detailed Metrics:\n","Correct Predictions: 0\n","False Positives: 47\n","False Negatives: 44\n","Average Location Error: 0.00 km\n","Average Magnitude Error: 0.00\n","Precision: 0.000\n","Recall: 0.000\n","F1 Score: 0.000\n","\n","Optimizing model...\n","Optimizing with data from: 2024-11-18\n","Optimization tensors - Input: torch.Size([1, 1, 60, 5]), Target: torch.Size([1, 60, 3])\n","Optimization step completed. Loss: 0.1955\n","Post-optimization loss: 0.1907\n","\n","Processing metadata for saving...\n","\n","Processing historical day: 2024-11-18\n","Fetching data from 2024-11-18 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 44\n","Date range: 2024-11-18 00:00:44.709000 to 2024-11-19 12:56:02.383000\n","Magnitude range: 2.5 to 5.6\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_133725.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-18.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-18.json\n","Processing data from 2024-11-18 to 2024-11-19\n","Using sequence length: 1\n","\n","Maximum earthquakes per day: 60\n","Processing data from 2024-11-18 to 2024-11-19\n","Using sequence length: 1\n","\n","Maximum earthquakes per day: 60\n","\n","Prediction Details:\n","Historical average earthquakes: 44\n","Predicted probability: 0.630\n","Available predictions: 60\n","Final predicted count: 28\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-19.csv\n","\n","Historical data processing completed\n","\n","Switching to continuous monitoring mode...\n","\n","Current time: 2024-11-19 13:37:25.809025\n","Waiting until 2024-11-19 23:00:00 to begin continuous monitoring...\n","Will start continuous monitoring in 9.4 hours\n"]}]}]}