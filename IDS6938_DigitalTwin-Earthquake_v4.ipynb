{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOBuaoBYy7DMEJYgMWVzNVQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Earthquake Prediction Pipeline Documentation**\n"],"metadata":{"id":"GPhtaEIJBAVu"}},{"cell_type":"markdown","source":["## **Overview**\n","\n","The Earthquake Prediction Pipeline is a comprehensive system that automates the collection, processing, and analysis of USGS earthquake data to predict future seismic activity. The pipeline implements a transformer-based model that learns from historical patterns to predict the number of earthquakes likely to occur in the next 24-hour period."],"metadata":{"id":"a4jXwipSAPW7"}},{"cell_type":"markdown","source":["### **Key Features**\n","\n","*   Automated USGS data collection and processing\n","*   Daily data segmentation and storage\n","*   Transformer-based sequence modeling\n","*   Continuous prediction and evaluation\n","*   Automated model optimization\n","*   Performance visualization and tracking\n","*   Modular architecture with comprehensive error handling\n"],"metadata":{"id":"dPFH6Hz7_3rp"}},{"cell_type":"markdown","source":["\n","### **System Requirements**\n","\n","Python 3.x w/Required Libraries:\n","\n","*   pandas\n","*   numpy\n","*   torch (PyTorch)\n","*   requests\n","*   matplotlib\n","*   seaborn\n","*   scikit-learn\n"],"metadata":{"id":"BRHZKpLEAYYs"}},{"cell_type":"markdown","source":["### **Directory Structure**\n","\n","\n","```\n","/earthquake_data/\n","‚îú‚îÄ‚îÄ data/             # Raw daily earthquake data\n","‚îÇ   ‚îî‚îÄ‚îÄ YYYY-MM/      # Organized by year-month\n","‚îú‚îÄ‚îÄ models/           # Saved model checkpoints\n","‚îú‚îÄ‚îÄ predictions/      # Daily prediction outputs\n","‚îú‚îÄ‚îÄ plots/           # Performance visualizations\n","‚îî‚îÄ‚îÄ evaluations/     # Evaluation metrics\n","```"],"metadata":{"id":"T1CUq-hkAmZ5"}},{"cell_type":"markdown","source":["### **Core Components**\n","\n","1. Data Collection and Processing\n","\n","*   USGS API Integration: Automated fetching of earthquake data\n","*   Data Filtering: Configurable magnitude threshold (default: 2.5)\n","*   Data Storage: Daily CSV files with comprehensive metadata\n","*   Feature Extraction: Geographic and seismic parameters\n","\n","2. Model Architecture\n","\n","*   Type: Transformer-based sequence model\n","\n","*   Components:\n"," *   Input projection layer\n"," *   Positional encoding\n"," *   Multi-head attention layers\n"," *   Feed-forward networks\n"," *   Output projection layer\n","\n","*   Parameters:\n"," *   Sequence Length: Configurable (default: 7 days)\n"," *   Hidden Dimensions: 64\n"," *   Number of Layers: 2\n"," *   Attention Heads: 4\n","\n","3. Training Pipeline\n","*    **Baseline Training**\n","\n"," 1. Historical Data Processing\n","\n","  *   Fetches specified number of days (default: 31)\n","  *   Splits data into daily segments\n","  *   Creates initial training sequences\n","\n"," 2. Model Training\n","\n","  *   Sequences created from historical data\n","  *   Loss function: Mean Squared Error\n","  *   Optimizer: Adam\n","  *   Checkpoint saving based on performance\n","\n"," 3. Evaluation\n","\n","  *   Daily prediction accuracy\n","  *   Error metrics calculation\n","  *   Performance visualization\n","  *   Metadata tracking\n","\n","*    **Continuous Monitoring**\n","\n"," 1. Automated Data Collection\n","\n","  *   Configurable update interval (default: 1 hour)\n","  *   Real-time USGS data integration\n","\n"," 2. Prediction Generation\n","\n","  *   Daily earthquake count predictions\n","  *   Confidence interval calculation\n","  *   Prediction storage and tracking\n","\n"," 3. Model Optimization\n","\n","  *   Performance evaluation against actual data\n","  *   Incremental model updates\n","  *   Automated checkpoint management\n","\n","4. Performance Metrics\n","\n","  *   Prediction Error (absolute and relative)\n","  *   Confidence Interval Coverage\n","  *   Standard Deviation Analysis\n","  *   Visualization of Trends"],"metadata":{"id":"WuHgC8POAyD3"}},{"cell_type":"markdown","source":["### **File Naming Conventions**\n","\n","*   Data Files: *earthquake_data_YYYY-MM-DD.csv*\n","*   Predictions: *predictions_YYYY-MM-DD.csv*\n","*   Model Checkpoints: *model_checkpoint_YYYYMMDD_HHMMSS.pth*\n","*   Visualizations: *performance_Ndays_YYYYMMDD_HHMMSS.png*\n","*   Evaluations: *evaluation_YYYY-MM-DD.json*"],"metadata":{"id":"hjn2s5JcA-tU"}},{"cell_type":"markdown","source":["\n","### **Usage Examples**\n","##### **Initialize Pipeline**\n","```\n","pipeline = EarthquakePipeline(drive_path='/path/to/base/directory')\n","```\n","##### **Run Baseline Training**\n","```\n","pipeline.run_baseline_training(days_to_process=31)\n","```\n","##### **Start Continuous Monitoring**\n","```pipeline.run_continuous_monitoring(update_interval=3600)  # 1 hour interval\n","```"],"metadata":{"id":"O9qA20LVBKdA"}},{"cell_type":"markdown","source":["### **Future Enhancements**\n","\n","*   Integration with additional data sources\n","*   Enhanced feature engineering\n","*   Advanced visualization capabilities\n","*   Automated parameter optimization\n","*   Real-time alerting system\n","*   Web interface for monitoring"],"metadata":{"id":"CRKvCNnTBN0h"}},{"cell_type":"markdown","source":["### **Model Misc Info**\n","*   Authors: Stephen Moore, Steven Willhelm, Lynn Yingling\n","*   Version: 4.0\n","*   Last Updated: 19 November 2024"],"metadata":{"id":"9BUgX_acBQ9o"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"SiEjZK1o_o05"}},{"cell_type":"code","source":["# Required imports\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from datetime import datetime, timedelta\n","import requests\n","import os\n","import json\n","import glob\n","import time\n","from sklearn.preprocessing import MinMaxScaler\n","import pickle\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from google.colab import drive\n","\n","# Set random seed for reproducibility\n","torch.manual_seed(42)\n","np.random.seed(42)"],"metadata":{"id":"EnteAQmIWVm-","executionInfo":{"status":"ok","timestamp":1732047317943,"user_tz":300,"elapsed":141,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":["## Create Drive Directory"],"metadata":{"id":"CuDBqgmaBj_8"}},{"cell_type":"code","source":["def setup_drive_directory(base_path='earthquake_data'):\n","    \"\"\"Mount Google Drive and create necessary directories\"\"\"\n","    drive.mount('/content/drive')\n","    full_path = f'/content/drive/My Drive/{base_path}'\n","    if not os.path.exists(full_path):\n","        os.makedirs(full_path)\n","        print(f\"Created directory: {full_path}\")\n","    else:\n","        print(f\"Directory already exists: {full_path}\")\n","    return full_path"],"metadata":{"id":"jjF-I5ief3Vk","executionInfo":{"status":"ok","timestamp":1732047319171,"user_tz":300,"elapsed":188,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":["## Gather USGS Data"],"metadata":{"id":"QRiiBc7fBqdQ"}},{"cell_type":"code","source":["def fetch_earthquake_data(self, start_time=None, end_time=None, min_magnitude=2.5):\n","    \"\"\"\n","    Fetch earthquake data from USGS API for a specified time period.\n","\n","    Args:\n","        start_time (datetime): Start date for data collection. Defaults to yesterday if None.\n","        end_time (datetime): End date for data collection. Defaults to today if None.\n","        min_magnitude (float): Minimum earthquake magnitude to include (default: 2.5)\n","\n","    Returns:\n","        pandas.DataFrame: DataFrame containing earthquake data with columns:\n","            - time: Timestamp of earthquake occurrence\n","            - magnitude: Earthquake magnitude\n","            - place: Location description\n","            - longitude: Geographic longitude\n","            - latitude: Geographic latitude\n","            - depth: Depth in kilometers\n","            - type: Event type\n","            - alert: Alert level (if any)\n","            - tsunami: Tsunami warning flag\n","            - sig: Significance value\n","\n","    Raises:\n","        requests.RequestException: If API request fails\n","        ValueError: If date parameters are invalid\n","\n","    Example:\n","        >>> start = datetime(2024, 11, 1)\n","        >>> end = datetime(2024, 11, 2)\n","        >>> data = pipeline.fetch_earthquake_data(start, end, min_magnitude=3.0)\n","    \"\"\"\n","    try:\n","        base_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n","\n","        if start_time is None:\n","            start_time = datetime.now() - timedelta(days=1)\n","\n","        if end_time is None:\n","            end_time = start_time + timedelta(days=1)\n","\n","        params = {\n","            'format': 'geojson',\n","            'starttime': start_time.strftime('%Y-%m-%d'),\n","            'endtime': end_time.strftime('%Y-%m-%d'),\n","            'minmagnitude': min_magnitude,\n","            'orderby': 'time'\n","        }\n","\n","        print(f\"Fetching data for: {start_time.strftime('%Y-%m-%d')}\")\n","\n","        response = requests.get(base_url, params=params)\n","        response.raise_for_status()\n","\n","        data = response.json()\n","        earthquakes = data['features']\n","\n","        processed_data = []\n","        for quake in earthquakes:\n","            properties = quake['properties']\n","            coordinates = quake['geometry']['coordinates']\n","\n","            processed_data.append({\n","                'time': datetime.fromtimestamp(properties['time'] / 1000),\n","                'magnitude': properties['mag'],\n","                'place': properties['place'],\n","                'longitude': coordinates[0],\n","                'latitude': coordinates[1],\n","                'depth': coordinates[2],\n","                'type': properties['type'],\n","                'alert': properties.get('alert', 'none'),\n","                'tsunami': properties['tsunami'],\n","                'sig': properties['sig']\n","            })\n","\n","        df = pd.DataFrame(processed_data)\n","\n","        if len(df) > 0:\n","            print(\"\\nData Collection Summary:\")\n","            print(\"-\" * 30)\n","            print(f\"Total earthquakes collected: {len(df)}\")\n","            print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n","            print(f\"Magnitude range: {df['magnitude'].min():.1f} to {df['magnitude'].max():.1f}\")\n","            print(\"-\" * 30)\n","\n","        return df\n","\n","    except Exception as e:\n","        print(f\"Error fetching data: {e}\")\n","        return None"],"metadata":{"id":"MXTJ_CfRS5k7","executionInfo":{"status":"ok","timestamp":1732047320112,"user_tz":300,"elapsed":145,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["def fetch_training_data(self, start_date, end_date):\n","    \"\"\"Fetch training data for specified date range\"\"\"\n","    df = self.fetch_earthquake_data(\n","        start_time=start_date,\n","        end_time=end_date,\n","        min_magnitude=2.5\n","    )\n","\n","    if df is not None:\n","        # Save with date range\n","        filename = f'earthquake_data_{start_date.strftime(\"%Y%m%d\")}_to_{end_date.strftime(\"%Y%m%d\")}.csv'\n","        filepath = os.path.join(self.drive_path, filename)\n","        df.to_csv(filepath, index=False)\n","\n","        return df, filepath\n","    return None, None"],"metadata":{"id":"tb64jht8SuO9","executionInfo":{"status":"ok","timestamp":1732047320307,"user_tz":300,"elapsed":1,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["def fetch_new_data(self, last_timestamp):\n","    \"\"\"Fetch only new data since last recorded timestamp\"\"\"\n","    df = self.fetch_earthquake_data(\n","        start_time=last_timestamp,\n","        end_time=datetime.now(),\n","        min_magnitude=2.5\n","    )\n","\n","    if df is not None:\n","        # Filter to only new events\n","        new_data = df[df['time'] > last_timestamp]\n","\n","        if len(new_data) > 0:\n","            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","            filename = f'new_data_{timestamp}.csv'\n","            filepath = os.path.join(self.dirs['data'], filename)\n","            new_data.to_csv(filepath, index=False)\n","\n","            return new_data, filepath\n","    return None, None"],"metadata":{"id":"3U37EubPSwvi","executionInfo":{"status":"ok","timestamp":1732047320307,"user_tz":300,"elapsed":1,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":["## Create Data Structure for Transformer"],"metadata":{"id":"9bT3FHK2CBw8"}},{"cell_type":"code","source":["class EarthquakeDataset(Dataset):\n","    \"\"\"\n","    Custom dataset for handling earthquake sequence data.\n","\n","    This dataset creates sequences of earthquake data for training the transformer\n","    model, where each sequence consists of multiple days of data points.\n","\n","    Args:\n","        features (torch.Tensor): Input features for each earthquake event\n","        targets (torch.Tensor): Target values for prediction\n","        seq_length (int): Number of days in each sequence\n","\n","    Attributes:\n","        features (torch.Tensor): Storage for input features\n","        targets (torch.Tensor): Storage for target values\n","        seq_length (int): Length of each sequence\n","\n","    Methods:\n","        __len__: Returns the number of sequences in the dataset\n","        __getitem__: Returns a sequence and its corresponding target\n","\n","    Example:\n","        >>> features = torch.randn(100, 5)  # 100 events with 5 features each\n","        >>> targets = torch.randn(100, 1)   # Target count for each event\n","        >>> dataset = EarthquakeDataset(features, targets, seq_length=7)\n","        >>> sequence, target = dataset[0]  # Get first sequence and its target\n","    \"\"\"\n","\n","    def __init__(self, features, targets, seq_length):\n","        \"\"\"\n","        Initialize the dataset with features, targets, and sequence length.\n","\n","        Args:\n","            features (torch.Tensor): Input features for each earthquake event\n","            targets (torch.Tensor): Target values for prediction\n","            seq_length (int): Number of days to include in each sequence\n","        \"\"\"\n","        self.features = features\n","        self.targets = targets\n","        self.seq_length = seq_length\n","\n","    def __len__(self):\n","        \"\"\"Return the number of possible sequences in the dataset.\"\"\"\n","        return max(0, len(self.features) - self.seq_length)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Get a sequence of features and its corresponding target.\n","\n","        Args:\n","            idx (int): Index of the sequence to retrieve\n","\n","        Returns:\n","            tuple: (feature_sequence, target) where feature_sequence is a sequence of\n","                  'seq_length' days of data and target is the next day's parameters\n","        \"\"\"\n","        feature_seq = self.features[idx:idx + self.seq_length]\n","        target = self.targets[idx + self.seq_length - 1]\n","        return feature_seq, target"],"metadata":{"id":"miri2HbbefrN","executionInfo":{"status":"ok","timestamp":1732047322256,"user_tz":300,"elapsed":151,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":["## Create Transformer"],"metadata":{"id":"4qU7DAPPCdrJ"}},{"cell_type":"code","source":["class TransformerPredictor(nn.Module):\n","    \"\"\"\n","    Transformer-based model for earthquake count prediction.\n","\n","    This model uses a transformer architecture to learn temporal patterns in\n","    earthquake sequences and predict future occurrence counts.\n","\n","    Architecture:\n","        - Input projection layer\n","        - Positional encoding\n","        - Transformer encoder layers\n","        - Output projection layers\n","\n","    Args:\n","        input_dim (int): Dimension of input features\n","        hidden_dim (int): Dimension of hidden layers\n","        num_layers (int): Number of transformer layers\n","        num_heads (int): Number of attention heads\n","        max_seq_length (int): Maximum sequence length (default: 7)\n","\n","    Attributes:\n","        hidden_dim (int): Dimension of hidden layers\n","        input_projection (nn.Linear): Input projection layer\n","        pos_encoding (nn.Parameter): Positional encoding\n","        transformer (nn.TransformerEncoder): Transformer encoder\n","        output_projection (nn.Sequential): Output projection layers\n","\n","    Example:\n","        >>> model = TransformerPredictor(\n","                input_dim=1,\n","                hidden_dim=64,\n","                num_layers=2,\n","                num_heads=4\n","            )\n","        >>> input_sequence = torch.randn(32, 7, 1)  # (batch_size, seq_length, features)\n","        >>> predictions = model(input_sequence)\n","    \"\"\"\n","    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, max_seq_length=7):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.input_projection = nn.Linear(input_dim, hidden_dim)\n","        self.pos_encoding = nn.Parameter(torch.randn(1, max_seq_length, hidden_dim))\n","\n","        encoder_layer = nn.TransformerEncoderLayer(\n","            d_model=hidden_dim,\n","            nhead=num_heads,\n","            dim_feedforward=hidden_dim*4,\n","            dropout=0.1,\n","            batch_first=True\n","        )\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n","\n","        self.output_projection = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim//2),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim//2, 1)\n","        )\n","\n","    def forward(self, x):\n","        # Ensure input is the right shape (batch_size, seq_length, input_dim)\n","        if len(x.shape) == 2:\n","            x = x.unsqueeze(-1)\n","\n","        # Project input to hidden dimension\n","        x = self.input_projection(x)\n","\n","        # Add positional encoding\n","        x = x + self.pos_encoding[:, :x.size(1)]\n","\n","        # Apply transformer\n","        x = self.transformer(x)\n","\n","        # Take the last sequence element and project to output\n","        x = x[:, -1]\n","        x = self.output_projection(x)\n","\n","        return x"],"metadata":{"id":"fWLUJMHMfC7x","executionInfo":{"status":"ok","timestamp":1732047323612,"user_tz":300,"elapsed":157,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":["## Create Pipeline"],"metadata":{"id":"636EUZjOCiQc"}},{"cell_type":"code","source":["class EarthquakePipeline:\n","    \"\"\"\n","    A comprehensive pipeline for earthquake prediction using USGS data and transformer models.\n","\n","    The pipeline implements automated data collection, processing, model training, and continuous\n","    monitoring capabilities for predicting earthquake occurrences.\n","\n","    Attributes:\n","        drive_path (str): Base directory path for storing all pipeline data\n","        seq_length (int): Number of days to use in prediction sequences (default: 7)\n","        prediction_horizon (int): Days ahead to predict (default: 1)\n","        dirs (dict): Dictionary of directory paths for different data types\n","        model_dates (dict): Tracking dates for pipeline operations\n","        metadata (dict): Pipeline metadata and configuration information\n","        performance_history (list): List of historical prediction performance metrics\n","\n","    Directory Structure:\n","        /drive_path/\n","        ‚îú‚îÄ‚îÄ data/          - Raw earthquake data organized by date\n","        ‚îú‚îÄ‚îÄ models/        - Model checkpoints and configurations\n","        ‚îú‚îÄ‚îÄ predictions/   - Prediction outputs and evaluations\n","        ‚îú‚îÄ‚îÄ plots/         - Performance visualizations\n","        ‚îî‚îÄ‚îÄ evaluations/   - Detailed evaluation metrics\n","\n","    Example:\n","        >>> pipeline = EarthquakePipeline('/path/to/data')\n","        >>> pipeline.run_baseline_training(days_to_process=31)\n","        >>> pipeline.run_continuous_monitoring(update_interval=3600)\n","    \"\"\"\n","\n","    def __init__(self, drive_path, seq_length=7, prediction_horizon=1):\n","        \"\"\"Initialize earthquake prediction pipeline.\"\"\"\n","        self.drive_path = drive_path\n","\n","        # Create directory structure\n","        self.dirs = {\n","            'data': os.path.join(drive_path, 'data'),\n","            'models': os.path.join(drive_path, 'models'),\n","            'predictions': os.path.join(drive_path, 'predictions'),\n","            'plots': os.path.join(drive_path, 'plots'),\n","            'evaluations': os.path.join(drive_path, 'evaluations')\n","        }\n","\n","        for dir_path in self.dirs.values():\n","            os.makedirs(dir_path, exist_ok=True)\n","\n","        # Initialize parameters\n","        self.seq_length = seq_length\n","        self.prediction_horizon = prediction_horizon\n","        self.performance_history = []\n","\n","        # Define features and targets (for metadata)\n","        self.feature_columns = ['count']  # Simplified for count prediction\n","        self.target_columns = ['count']   # Single target\n","\n","        # Date tracking system\n","        self.model_dates = {\n","            'last_training_date': None,\n","            'last_optimization_date': None,\n","            'latest_data_date': None,\n","            'prediction_target_date': None,\n","            'first_data_date': None\n","        }\n","\n","        # Initialize model\n","        self.model = TransformerPredictor(\n","            input_dim=1,  # For count prediction\n","            hidden_dim=64,\n","            num_layers=2,\n","            num_heads=4,\n","            max_seq_length=seq_length\n","        )\n","\n","        # Initialize metadata\n","        self.metadata_path = os.path.join(drive_path, 'pipeline_metadata.json')\n","        self._load_or_create_metadata()\n","\n","        # Save initial setup\n","        self._save_metadata()\n","\n","    def _create_new_metadata(self):\n","        \"\"\"Create new metadata structure with all required fields.\"\"\"\n","        self.metadata = {\n","            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","            'data_dates': [],  # List to store dates of processed data\n","            'model_versions': [],  # List to store model version information\n","            'predictions': [],  # List to store prediction records\n","            'evaluations': [],  # List to store evaluation results\n","            'pipeline_config': {\n","                'sequence_length': self.seq_length,\n","                'prediction_horizon': self.prediction_horizon,\n","                'feature_columns': self.feature_columns,\n","                'target_columns': self.target_columns\n","            }\n","        }\n","        self._save_metadata()\n","\n","    def _ensure_metadata_structure(self):\n","        \"\"\"Ensure all required fields exist in metadata.\"\"\"\n","        required_fields = {\n","            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","            'data_dates': [],\n","            'model_versions': [],\n","            'predictions': [],\n","            'evaluations': [],\n","            'pipeline_config': {\n","                'sequence_length': self.seq_length,\n","                'prediction_horizon': self.prediction_horizon,\n","                'feature_columns': self.feature_columns,\n","                'target_columns': self.target_columns\n","            }\n","        }\n","\n","        # Add any missing fields\n","        for key, default_value in required_fields.items():\n","            if key not in self.metadata:\n","                self.metadata[key] = default_value\n","                print(f\"Added missing metadata field: {key}\")\n","\n","        # Add any missing nested fields in pipeline_config\n","        if 'pipeline_config' in self.metadata:\n","            for key, value in required_fields['pipeline_config'].items():\n","                if key not in self.metadata['pipeline_config']:\n","                    self.metadata['pipeline_config'][key] = value\n","                    print(f\"Added missing config field: {key}\")\n","\n","    def _save_metadata(self, verbose=False):\n","        \"\"\"\n","        Save pipeline metadata to JSON file.\n","\n","        Handles serialization of metadata including:\n","        - Pipeline configuration and status\n","        - Data tracking and ranges\n","        - Model versions and training history\n","        - Prediction history and performance metrics\n","        - File paths and timestamps\n","        \"\"\"\n","        try:\n","            from datetime import date, datetime\n","            import numpy as np\n","            import pandas as pd\n","\n","            # Create comprehensive metadata structure\n","            metadata = {\n","                'last_update': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","                'pipeline_info': {\n","                    'creation_date': self.metadata.get('creation_date'),\n","                    'sequence_length': self.seq_length,\n","                    'feature_columns': self.feature_columns,\n","                    'target_columns': self.target_columns\n","                },\n","                'data_range': {\n","                    'start': self.model_dates.get('first_data_date'),\n","                    'end': self.model_dates.get('latest_data_date'),\n","                    'total_days_processed': len(self.metadata.get('data_dates', []))\n","                },\n","                'training_info': {\n","                    'last_training': self.model_dates.get('last_training_date'),\n","                    'last_optimization': self.model_dates.get('last_optimization_date'),\n","                    'model_versions': self.metadata.get('model_versions', [])\n","                },\n","                'prediction_stats': self.performance_history,\n","                'file_paths': {\n","                    'data_directory': self.dirs['data'],\n","                    'model_directory': self.dirs['models'],\n","                    'predictions_directory': self.dirs['predictions'],\n","                    'plots_directory': self.dirs['plots']\n","                }\n","            }\n","\n","            # Ensure metadata is JSON serializable\n","            def convert_to_serializable(obj):\n","                if isinstance(obj, (np.integer, np.floating)):\n","                    return float(obj)\n","                elif isinstance(obj, np.ndarray):\n","                    return obj.tolist()\n","                elif isinstance(obj, datetime):\n","                    return obj.strftime('%Y-%m-%d %H:%M:%S')\n","                elif isinstance(obj, date):  # Add handling for date objects\n","                    return obj.strftime('%Y-%m-%d')\n","                elif pd.isnull(obj):  # Changed from isna to isnull\n","                    return None\n","                return obj\n","\n","            # Process all entries recursively\n","            def process_dict(d):\n","                result = {}\n","                for k, v in d.items():\n","                    if isinstance(v, dict):\n","                        result[k] = process_dict(v)\n","                    elif isinstance(v, list):\n","                        result[k] = [\n","                            process_dict(item) if isinstance(item, dict)\n","                            else convert_to_serializable(item)\n","                            for item in v\n","                        ]\n","                    else:\n","                        result[k] = convert_to_serializable(v)\n","                return result\n","\n","            # Process metadata\n","            print(\"\\nProcessing metadata for saving...\")\n","            serializable_metadata = process_dict(metadata)\n","\n","            # Save to file with pretty printing\n","            if verbose:\n","                print(\"Processing metadata for saving...\")\n","            with open(self.metadata_path, 'w') as f:\n","                json.dump(serializable_metadata, f, indent=4)\n","            if verbose:\n","                print(f\"Metadata saved successfully to: {self.metadata_path}\")\n","\n","            # Save a backup copy with timestamp\n","            backup_path = os.path.join(\n","                os.path.dirname(self.metadata_path),\n","                f'metadata_backup_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n","            )\n","            with open(backup_path, 'w') as f:\n","                json.dump(serializable_metadata, f, indent=4)\n","            if verbose:\n","                print(f\"Metadata backup saved to: {backup_path}\")\n","\n","        except Exception as e:\n","            print(f\"\\nError saving metadata: {str(e)}\")\n","            print(\"Metadata path:\", self.metadata_path)\n","            print(\"Error details:\", str(e))\n","\n","            # Additional debugging information\n","            print(\"\\nMetadata structure:\")\n","            for key, value in metadata.items():\n","                print(f\"{key}: {type(value)}\")\n","\n","            raise\n","\n","    def _load_or_create_metadata(self):\n","        \"\"\"Initialize or load existing metadata with all required fields.\"\"\"\n","        if os.path.exists(self.metadata_path):\n","            try:\n","                with open(self.metadata_path, 'r') as f:\n","                    self.metadata = json.load(f)\n","                # Ensure all required fields exist even in loaded metadata\n","                self._ensure_metadata_structure()\n","            except Exception as e:\n","                print(f\"Error loading metadata: {str(e)}. Creating new metadata.\")\n","                self._create_new_metadata()\n","        else:\n","            self._create_new_metadata()\n","\n","    def get_metadata_summary(self):\n","        \"\"\"\n","        Get a summary of current pipeline metadata.\n","\n","        Returns:\n","            dict: Summary of pipeline state and history\n","        \"\"\"\n","        return {\n","            'creation_date': self.metadata['creation_date'],\n","            'data_count': len(self.metadata['data_dates']),\n","            'model_versions': len(self.metadata['model_versions']),\n","            'predictions_made': len(self.metadata['predictions']),\n","            'latest_data': self.model_dates['latest_data_date'],\n","            'last_training': self.model_dates['last_training_date'],\n","            'last_optimization': self.model_dates['last_optimization_date']\n","        }\n","\n","    def fetch_earthquake_data(self, start_time=None, end_time=None, min_magnitude=2.5):\n","        \"\"\"\n","        Fetch earthquake data from USGS API for a specific time period.\n","\n","        Args:\n","            start_time (datetime): Start of time period\n","            end_time (datetime): End of time period\n","            min_magnitude (float): Minimum earthquake magnitude to include\n","        \"\"\"\n","        try:\n","            # Construct the query URL for the USGS API\n","            base_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n","\n","            # Format dates for the API\n","            if start_time is None:\n","                start_time = datetime.now() - timedelta(days=1)\n","            if end_time is None:\n","                end_time = datetime.now()\n","\n","            params = {\n","                'format': 'geojson',\n","                'starttime': start_time.strftime('%Y-%m-%d'),\n","                'endtime': (end_time + timedelta(days=1)).strftime('%Y-%m-%d'),  # Add 1 day to include full end date\n","                'minmagnitude': min_magnitude,\n","                'orderby': 'time'\n","            }\n","\n","            print(f\"Fetching data from {params['starttime']} to {params['endtime']}\")\n","\n","            # Make the API request\n","            response = requests.get(base_url, params=params)\n","            response.raise_for_status()\n","\n","            # Parse the JSON response\n","            data = response.json()\n","            earthquakes = data['features']\n","\n","            processed_data = []\n","            for quake in earthquakes:\n","                properties = quake['properties']\n","                coordinates = quake['geometry']['coordinates']\n","\n","                processed_data.append({\n","                    'time': datetime.fromtimestamp(properties['time'] / 1000),\n","                    'magnitude': properties['mag'],\n","                    'place': properties['place'],\n","                    'longitude': coordinates[0],\n","                    'latitude': coordinates[1],\n","                    'depth': coordinates[2],\n","                    'type': properties['type'],\n","                    'alert': properties.get('alert', 'none'),\n","                    'tsunami': properties['tsunami'],\n","                    'sig': properties['sig']\n","                })\n","\n","            df = pd.DataFrame(processed_data)\n","\n","            if len(df) > 0:\n","                print(\"\\nData Collection Summary:\")\n","                print(\"-\" * 30)\n","                print(f\"Total earthquakes collected: {len(df)}\")\n","                print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n","                print(f\"Magnitude range: {df['magnitude'].min():.1f} to {df['magnitude'].max():.1f}\")\n","                print(\"-\" * 30)\n","\n","            return df\n","\n","        except Exception as e:\n","            print(f\"Error fetching data: {e}\")\n","            return None\n","\n","    def prepare_data(self, df, for_training=True):\n","        \"\"\"\n","        Process earthquake data into sequences for model training or prediction.\n","\n","        Args:\n","            df (pandas.DataFrame): Raw earthquake data\n","            for_training (bool): If True, prepare data for training; if False, for prediction\n","\n","        Returns:\n","            tuple: (sequence_tensor, target_tensor)\n","                - sequence_tensor: torch.Tensor of shape (n_sequences, seq_length, features)\n","                - target_tensor: torch.Tensor of shape (n_sequences, 1) for counts\n","\n","        Notes:\n","            - Sequences are created by grouping earthquakes by day\n","            - Features are normalized using MinMaxScaler\n","            - Single-day support is implemented for prediction mode\n","\n","        Example:\n","            >>> data = pipeline.fetch_earthquake_data(start_date, end_date)\n","            >>> sequences, targets = pipeline.prepare_data(data, for_training=True)\n","        \"\"\"\n","        try:\n","            if df is None or len(df) == 0:\n","                print(\"No data to process\")\n","                return None, None\n","\n","            # Convert to daily counts\n","            df['date'] = pd.to_datetime(df['time']).dt.date\n","            daily_counts = df.groupby('date').size().reset_index(name='count')\n","            daily_counts = daily_counts.sort_values('date')\n","\n","            # Create sequences - now supporting single day\n","            sequences = []\n","            targets = []\n","\n","            # For single day, use the count directly\n","            if len(daily_counts) == 1:\n","                sequences = torch.FloatTensor([[daily_counts['count'].iloc[0]]])\n","                targets = torch.FloatTensor([[daily_counts['count'].iloc[0]]])\n","                return sequences, targets\n","\n","            # For multiple days, create proper sequences\n","            for i in range(len(daily_counts) - 1):  # -1 to always have a target\n","                seq = daily_counts['count'].iloc[i:i+1].values  # Take current day\n","                target = daily_counts['count'].iloc[i+1]  # Next day is target\n","                sequences.append(seq)\n","                targets.append(target)\n","\n","            if not sequences:\n","                return None, None\n","\n","            sequences = torch.FloatTensor(sequences)\n","            targets = torch.FloatTensor(targets).reshape(-1, 1)\n","\n","            return sequences, targets\n","\n","        except Exception as e:\n","            print(f\"Error preparing data: {str(e)}\")\n","            return None, None\n","\n","    def train_model(self, sequence_tensor, target_tensor, epochs=100, batch_size=32):\n","        \"\"\"\n","        Train the transformer model on earthquake sequence data.\n","\n","        Args:\n","            sequence_tensor (torch.Tensor): Input sequences of shape (n_sequences, seq_length, features)\n","            target_tensor (torch.Tensor): Target values of shape (n_sequences, 1)\n","            epochs (int): Number of training epochs (default: 100)\n","            batch_size (int): Batch size for training (default: 32)\n","\n","        Training Process:\n","            1. Data is batched and shuffled using DataLoader\n","            2. Model is trained using MSE loss and Adam optimizer\n","            3. Best model is saved based on validation loss\n","            4. Progress is logged with detailed metrics\n","\n","        Returns:\n","            None: Updates model in-place and saves checkpoints\n","\n","        Example:\n","            >>> sequences, targets = pipeline.prepare_data(training_data)\n","            >>> pipeline.train_model(sequences, targets, epochs=150, batch_size=64)\n","        \"\"\"\n","        try:\n","            if sequence_tensor is None or target_tensor is None:\n","                print(\"\\n‚ùå No valid training data provided\")\n","                return\n","\n","            print(\"\\nüîÑ Starting Model Training\")\n","            print(\"=\" * 50)\n","            print(f\"Training Details:\")\n","            print(f\"- Sequences: {len(sequence_tensor)}\")\n","            print(f\"- Batch Size: {batch_size}\")\n","            print(f\"- Epochs: {epochs}\")\n","            print(\"-\" * 50)\n","\n","            criterion = nn.MSELoss()\n","            optimizer = torch.optim.Adam(self.model.parameters())\n","\n","            dataset = TensorDataset(sequence_tensor, target_tensor)\n","            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","            best_loss = float('inf')\n","            for epoch in range(epochs):\n","                total_loss = 0\n","                for sequences, targets in dataloader:\n","                    optimizer.zero_grad()\n","                    predictions = self.model(sequences)\n","                    loss = criterion(predictions, targets)\n","                    loss.backward()\n","                    optimizer.step()\n","                    total_loss += loss.item()\n","\n","                avg_loss = total_loss / len(dataloader)\n","                if avg_loss < best_loss:\n","                    best_loss = avg_loss\n","                    self.save_model_checkpoint(epoch, avg_loss)\n","                    checkpoint_saved = \"‚úì\"\n","                else:\n","                    checkpoint_saved = \" \"\n","\n","                if epoch % 10 == 0:\n","                    print(f\"Epoch {epoch:3d}/{epochs} | Loss: {avg_loss:.4f} {checkpoint_saved}\")\n","\n","            print(\"\\n‚úÖ Training completed\")\n","            print(f\"Final Loss: {avg_loss:.4f}\")\n","            print(f\"Best Loss: {best_loss:.4f}\")\n","            print(\"=\" * 50)\n","\n","        except Exception as e:\n","            print(f\"\\n‚ùå Training error: {str(e)}\")\n","\n","    def predict_next_day(self, recent_data):\n","        \"\"\"\n","        Generate earthquake count predictions for the next day.\n","\n","        Args:\n","            recent_data (pandas.DataFrame): Recent earthquake data for prediction\n","\n","        Returns:\n","            dict: Prediction information including:\n","                - predicted_count (int): Predicted number of earthquakes\n","                - lower_bound (int): Lower bound of prediction interval\n","                - upper_bound (int): Upper bound of prediction interval\n","\n","        Notes:\n","            - Confidence bounds are set at ¬±10% of predicted value\n","            - Predictions use the most recent sequence of data\n","            - Values are rounded to integers for practical use\n","\n","        Example:\n","            >>> recent_data = pipeline.fetch_earthquake_data(start_date, end_date)\n","            >>> prediction = pipeline.predict_next_day(recent_data)\n","            >>> print(f\"Predicted earthquakes: {prediction['predicted_count']}\")\n","        \"\"\"\n","        sequence_tensor, _ = self.prepare_data(recent_data, for_training=False)\n","\n","        with torch.no_grad():\n","            predicted_count = self.model(sequence_tensor)\n","            # Take the last prediction since we only want the next day\n","            last_prediction = predicted_count[-1].item()\n","\n","        prediction_range = {\n","            'predicted_count': int(last_prediction),\n","            'lower_bound': int(last_prediction * 0.9),\n","            'upper_bound': int(last_prediction * 1.1)\n","        }\n","\n","        return prediction_range\n","\n","    def evaluate_predictions(self, predictions, actual_data, prediction_date=None):\n","        \"\"\"\n","        Evaluate prediction accuracy against actual earthquake data.\n","\n","        Args:\n","            predictions (dict): Prediction data with counts and bounds\n","            actual_data (pandas.DataFrame): Actual earthquake data for the period\n","            prediction_date (datetime): Date of the prediction (default: None)\n","\n","        Returns:\n","            dict: Evaluation metrics including:\n","                - date: Prediction date\n","                - predicted_count: Predicted number of earthquakes\n","                - actual_count: Actual number of earthquakes\n","                - prediction_error: Absolute error in count\n","                - relative_error: Percentage error\n","                - within_bounds: Boolean indicating if actual was within confidence bounds\n","\n","        Notes:\n","            - Metrics are saved to evaluation directory\n","            - Results are added to performance history\n","            - Detailed logs are generated for analysis\n","\n","        Example:\n","            >>> predictions = pipeline.predict_next_day(recent_data)\n","            >>> actual = pipeline.fetch_earthquake_data(target_date, target_date + timedelta(days=1))\n","            >>> metrics = pipeline.evaluate_predictions(predictions, actual, target_date)\n","        \"\"\"\n","        try:\n","            actual_count = len(actual_data)\n","            pred_count = predictions['predicted_count']\n","\n","            if prediction_date is None:\n","                prediction_date = actual_data['time'].dt.date.iloc[0]\n","\n","            metrics = {\n","                'date': prediction_date,\n","                'predicted_count': pred_count,\n","                'actual_count': actual_count,\n","                'prediction_error': abs(pred_count - actual_count),\n","                'within_bounds': (actual_count >= predictions['lower_bound'] and\n","                                actual_count <= predictions['upper_bound']),\n","                'relative_error': abs(pred_count - actual_count) / actual_count * 100\n","            }\n","\n","            self.performance_history.append(metrics)\n","\n","            # Save evaluation metrics using string date\n","            self.save_evaluation_metrics(metrics, prediction_date.strftime('%Y-%m-%d'))\n","\n","            print(\"\\nüìä Prediction Evaluation\")\n","            print(\"-\" * 40)\n","            print(f\"Date:            {prediction_date}\")\n","            print(f\"Predicted Count: {pred_count}\")\n","            print(f\"Actual Count:    {actual_count}\")\n","            print(f\"Error:           {metrics['prediction_error']} events\")\n","            print(f\"Relative Error:  {metrics['relative_error']:.1f}%\")\n","            print(f\"Within Bounds:   {'‚úÖ' if metrics['within_bounds'] else '‚ùå'}\")\n","            print(f\"Days Tracked:    {len(self.performance_history)}\")\n","            print(\"-\" * 40)\n","\n","            return metrics\n","\n","        except Exception as e:\n","            print(f\"\\n‚ùå Error evaluating predictions: {str(e)}\")\n","            return None\n","\n","    def save_evaluation_metrics(self, metrics, date_str):\n","        \"\"\"Save detailed evaluation metrics to JSON file\"\"\"\n","        try:\n","            os.makedirs(self.dirs['evaluations'], exist_ok=True)\n","\n","            year_month = date_str[:7]\n","            eval_dir = os.path.join(self.dirs['evaluations'], year_month)\n","            os.makedirs(eval_dir, exist_ok=True)\n","\n","            # Convert dates to strings for JSON serialization\n","            evaluation_data = {\n","                'date': date_str,\n","                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n","                'metrics': {\n","                    'predicted_count': int(metrics['predicted_count']),\n","                    'actual_count': int(metrics['actual_count']),\n","                    'prediction_error': float(metrics['prediction_error']),\n","                    'relative_error': float(metrics['relative_error']),\n","                    'within_bounds': bool(metrics['within_bounds'])\n","                },\n","                'model_info': {\n","                    'last_training': str(self.model_dates.get('last_training_date')),\n","                    'last_optimization': str(self.model_dates.get('last_optimization_date'))\n","                }\n","            }\n","\n","            filename = f'evaluation_{date_str}.json'\n","            filepath = os.path.join(eval_dir, filename)\n","            with open(filepath, 'w') as f:\n","                json.dump(evaluation_data, f, indent=4)\n","\n","            print(f\"üìä Evaluation metrics saved: {filepath}\")\n","\n","        except Exception as e:\n","            print(f\"‚ùå Error saving evaluation metrics: {str(e)}\")\n","\n","    def analyze_trends(self, df):\n","        df['date'] = pd.to_datetime(df['time']).dt.date\n","        daily_counts = df.groupby('date').size()\n","\n","        trends = {\n","            'moving_avg_7d': daily_counts.rolling(7).mean(),\n","            'moving_avg_30d': daily_counts.rolling(30).mean(),\n","            'std_dev': daily_counts.rolling(7).std(),\n","            'min_count': daily_counts.rolling(7).min(),\n","            'max_count': daily_counts.rolling(7).max()\n","        }\n","\n","        return trends\n","\n","    def optimize_model(self, new_data, metrics):\n","        \"\"\"Optimize model with single day of data\"\"\"\n","        try:\n","            if metrics is None:\n","                return\n","\n","            sequence_tensor, target_tensor = self.prepare_data(new_data)\n","            if sequence_tensor is None or target_tensor is None:\n","                return\n","\n","            print(\"\\nüîÑ Optimizing Model\")\n","            optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0001)\n","            criterion = nn.MSELoss()\n","\n","            # Run a few optimization steps\n","            for step in range(5):\n","                optimizer.zero_grad()\n","                predictions = self.model(sequence_tensor)\n","                loss = criterion(predictions, target_tensor)\n","                loss.backward()\n","                optimizer.step()\n","\n","                if step % 2 == 0:\n","                    print(f\"Step {step}: Loss = {loss.item():.4f}\")\n","\n","        except Exception as e:\n","            print(f\"Error optimizing model: {str(e)}\")\n","\n","    def save_model_checkpoint(self, epoch, loss, metrics=None):\n","        \"\"\"Save model checkpoint with reduced logging\"\"\"\n","        try:\n","            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","\n","            if self.model_dates['latest_data_date']:\n","                model_dir = os.path.join(self.dirs['models'],\n","                                      self.model_dates['latest_data_date'][:7])\n","                os.makedirs(model_dir, exist_ok=True)\n","            else:\n","                model_dir = self.dirs['models']\n","\n","            # Save model state\n","            model_path = os.path.join(model_dir, f'model_checkpoint_{timestamp}.pth')\n","            config_path = os.path.join(model_dir, f'model_config_{timestamp}.json')\n","\n","            checkpoint = {\n","                'epoch': epoch,\n","                'model_state_dict': self.model.state_dict(),\n","                'loss': loss,\n","                'metrics': metrics,\n","                'timestamp': timestamp,\n","                'model_dates': self.model_dates.copy()\n","            }\n","\n","            torch.save(checkpoint, model_path)\n","\n","            # Save minimal configuration\n","            config = {\n","                'timestamp': timestamp,\n","                'loss': float(loss),\n","                'metrics': metrics\n","            }\n","\n","            with open(config_path, 'w') as f:\n","                json.dump(config, f)\n","\n","        except Exception as e:\n","            print(f\"Error saving checkpoint: {str(e)}\")\n","\n","    def load_latest_model(self):\n","        try:\n","            checkpoint_pattern = os.path.join(self.dirs['models'], '**',\n","                                          'model_checkpoint_*.pth')\n","            model_files = glob.glob(checkpoint_pattern, recursive=True)\n","\n","            if not model_files:\n","                print(\"No saved models found\")\n","                return False\n","\n","            latest_model = max(model_files, key=os.path.getctime)\n","            checkpoint = torch.load(latest_model)\n","\n","            self.model.load_state_dict(checkpoint['model_state_dict'])\n","            if 'model_dates' in checkpoint:\n","                self.model_dates.update(checkpoint['model_dates'])\n","\n","            print(f\"Loaded model from: {latest_model}\")\n","            print(f\"Checkpoint epoch: {checkpoint['epoch']}\")\n","            print(f\"Loss: {checkpoint['loss']}\")\n","\n","            return True\n","\n","        except Exception as e:\n","            print(f\"Error loading model: {str(e)}\")\n","            return False\n","\n","    def save_daily_data(self, df, date=None):\n","        \"\"\"\n","        Save daily earthquake data with comprehensive metadata.\n","\n","        Args:\n","            df (DataFrame): Earthquake data to save\n","            date (str, optional): Specific date for the data. Defaults to current date.\n","\n","        Example:\n","            >>> data = pipeline.fetch_earthquake_data('day')\n","            >>> pipeline.save_daily_data(data, '2024-11-18')\n","        \"\"\"\n","        if date is None:\n","            date = datetime.now().strftime('%Y-%m-%d')\n","\n","        # Create dated directory structure\n","        year_month = date[:7]  # YYYY-MM\n","        data_dir = os.path.join(self.dirs['data'], year_month)\n","        os.makedirs(data_dir, exist_ok=True)\n","\n","        # Save data\n","        filename = f'earthquake_data_{date}.csv'\n","        filepath = os.path.join(data_dir, filename)\n","        df.to_csv(filepath, index=False)\n","\n","        # Save daily summary\n","        summary_path = os.path.join(data_dir, f'summary_{date}.json')\n","        summary = {\n","            'date': date,\n","            'total_earthquakes': len(df),\n","            'magnitude_range': {\n","                'min': float(df['magnitude'].min()),\n","                'max': float(df['magnitude'].max()),\n","                'mean': float(df['magnitude'].mean())\n","            },\n","            'location_bounds': {\n","                'lat': {'min': float(df['latitude'].min()),\n","                       'max': float(df['latitude'].max())},\n","                'lon': {'min': float(df['longitude'].min()),\n","                       'max': float(df['longitude'].max())}\n","            },\n","            'depth_stats': {\n","                'min': float(df['depth'].min()),\n","                'max': float(df['depth'].max()),\n","                'mean': float(df['depth'].mean())\n","            },\n","            'saved_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","        }\n","\n","        with open(summary_path, 'w') as f:\n","            json.dump(summary, f, indent=4)\n","\n","        # Update metadata\n","        self.metadata['data_dates'].append({\n","            'date': date,\n","            'filepath': filepath,\n","            'summary_path': summary_path,\n","            'stats': summary\n","        })\n","        self._save_metadata(verbose=True)\n","\n","        # Update pipeline date tracking\n","        self.model_dates['latest_data_date'] = date\n","\n","        print(f\"Data saved: {filepath}\")\n","        print(f\"Summary saved: {summary_path}\")\n","\n","    def load_daily_data(self, date_str):\n","        \"\"\"\n","        Load earthquake data for a specific date from saved files.\n","\n","        Args:\n","            date_str (str): Date in 'YYYY-MM-DD' format\n","\n","        Returns:\n","            pandas.DataFrame: DataFrame containing earthquake data for the specified date\n","        \"\"\"\n","        try:\n","            # Construct the file path\n","            year_month = date_str[:7]  # YYYY-MM\n","            filename = f'earthquake_data_{date_str}.csv'\n","            filepath = os.path.join(self.dirs['data'], year_month, filename)\n","\n","            if os.path.exists(filepath):\n","                # Load the data\n","                df = pd.read_csv(filepath)\n","\n","                # Convert time column back to datetime\n","                df['time'] = pd.to_datetime(df['time'])\n","\n","                print(f\"Loaded data for {date_str}: {len(df)} earthquakes\")\n","                return df\n","            else:\n","                print(f\"No data file found for {date_str}\")\n","                return None\n","\n","        except Exception as e:\n","            print(f\"Error loading data for {date_str}: {str(e)}\")\n","            return None\n","\n","    def save_predictions(self, predictions, prediction_date, actual_data=None):\n","        \"\"\"Save predictions and optionally actual data for comparison.\"\"\"\n","        try:\n","            # Create prediction directory structure\n","            year_month = prediction_date[:7]  # YYYY-MM\n","            pred_dir = os.path.join(self.dirs['predictions'], year_month)\n","            os.makedirs(pred_dir, exist_ok=True)\n","\n","            # Convert predictions dict to DataFrame\n","            pred_df = pd.DataFrame([{\n","                'date': prediction_date,\n","                'predicted_count': predictions['predicted_count'],\n","                'lower_bound': predictions['lower_bound'],\n","                'upper_bound': predictions['upper_bound']\n","            }])\n","\n","            # Save predictions\n","            pred_filename = f'predictions_{prediction_date}.csv'\n","            pred_filepath = os.path.join(pred_dir, pred_filename)\n","            pred_df.to_csv(pred_filepath, index=False)\n","            print(f\"\\nPredictions saved to: {pred_filepath}\")\n","\n","            # Save comparison if actual data is available\n","            if actual_data is not None:\n","                actual_count = len(actual_data)\n","                comparison = {\n","                    'date': prediction_date,\n","                    'predicted_count': predictions['predicted_count'],\n","                    'actual_count': actual_count,\n","                    'prediction_error': abs(predictions['predicted_count'] - actual_count),\n","                    'within_bounds': (actual_count >= predictions['lower_bound'] and\n","                                    actual_count <= predictions['upper_bound']),\n","                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n","                }\n","\n","                # Save comparison summary\n","                summary_filename = f'comparison_{prediction_date}.json'\n","                summary_filepath = os.path.join(pred_dir, summary_filename)\n","                with open(summary_filepath, 'w') as f:\n","                    json.dump(comparison, f, indent=4)\n","                print(f\"Comparison summary saved to: {summary_filepath}\")\n","\n","                # Update performance history\n","                self.performance_history.append(comparison)\n","\n","        except Exception as e:\n","            print(f\"\\nError saving predictions: {str(e)}\")\n","            print(f\"Prediction date: {prediction_date}\")\n","            print(f\"Number of predictions: {len(predictions)}\")\n","            if actual_data is not None:\n","                print(f\"Number of actual events: {len(actual_data)}\")\n","            raise\n","\n","    def load_predictions(self, prediction_date):\n","        \"\"\"\n","        Load saved predictions for a specific date.\n","\n","        Args:\n","            prediction_date (str): Date to load predictions for (YYYY-MM-DD)\n","\n","        Returns:\n","            DataFrame: Loaded predictions, or None if not found\n","        \"\"\"\n","        try:\n","            # Construct file path\n","            year_month = prediction_date[:7]  # YYYY-MM\n","            pred_dir = os.path.join(self.dirs['predictions'], year_month)\n","            pred_filepath = os.path.join(pred_dir, f'predictions_{prediction_date}.csv')\n","\n","            if os.path.exists(pred_filepath):\n","                predictions = pd.read_csv(pred_filepath)\n","                print(f\"\\nLoaded predictions for {prediction_date}\")\n","                print(f\"Number of predictions: {len(predictions)}\")\n","                return predictions\n","            else:\n","                print(f\"\\nNo predictions found for {prediction_date}\")\n","                return None\n","\n","        except Exception as e:\n","            print(f\"\\nError loading predictions: {str(e)}\")\n","            print(f\"Attempted to load from: {pred_filepath}\")\n","            return None\n","\n","    def plot_performance(self, actual_counts, predicted_counts, dates):\n","        plt.figure(figsize=(12, 6))\n","        plt.plot(dates, actual_counts, label='Actual', marker='o')\n","        plt.plot(dates, predicted_counts, label='Predicted', marker='x')\n","        plt.fill_between(dates,\n","                        [p*0.9 for p in predicted_counts],\n","                        [p*1.1 for p in predicted_counts],\n","                        alpha=0.2, label='10% Confidence Interval')\n","        plt.title('Earthquake Count Prediction Performance')\n","        plt.xlabel('Date')\n","        plt.ylabel('Number of Earthquakes')\n","        plt.legend()\n","        plt.grid(True)\n","\n","    def save_visualization(self, start_date=None, end_date=None):\n","        \"\"\"\n","        Generate and save visualization of model prediction performance.\n","\n","        Args:\n","            start_date (datetime): Start date for visualization window\n","            end_date (datetime): End date for visualization window\n","\n","        Creates:\n","            - Line plot comparing predicted vs actual counts\n","            - Confidence interval visualization\n","            - Error trend analysis\n","            - Performance metrics summary\n","\n","        Saves:\n","            - PNG file with timestamp in plots directory\n","            - Performance metrics in evaluation directory\n","\n","        Notes:\n","            - Uses seaborn for enhanced visualization\n","            - Automatically adjusts date range if not specified\n","            - Includes comprehensive performance metrics\n","\n","        Example:\n","            >>> pipeline.save_visualization(\n","                    start_date=datetime(2024, 10, 1),\n","                    end_date=datetime(2024, 11, 1)\n","                )\n","        \"\"\"\n","        if not self.performance_history:\n","            print(\"\\n‚ùå No performance data available for visualization\")\n","            return\n","\n","        try:\n","            print(\"\\nüìà Generating Performance Visualization\")\n","\n","            # Import required libraries\n","            import seaborn as sns\n","            import matplotlib.dates as mdates\n","\n","            # Create plot directory if needed\n","            os.makedirs(self.dirs['plots'], exist_ok=True)\n","\n","            # Convert performance history to DataFrame\n","            performance_df = pd.DataFrame(self.performance_history)\n","            performance_df['date'] = pd.to_datetime(performance_df['date'])\n","\n","            # Set up the figure with better styling\n","            sns.set_style(\"whitegrid\")\n","            plt.rcParams['figure.figsize'] = [15, 10]\n","            fig, (ax1, ax2) = plt.subplots(2, 1)\n","\n","            # Format dates for x-axis\n","            locator = mdates.AutoDateLocator(minticks=5, maxticks=10)\n","            formatter = mdates.DateFormatter('%Y-%m-%d')\n","\n","            # Plot predicted vs actual counts with seaborn color palette\n","            colors = sns.color_palette(\"deep\")\n","            ax1.plot(performance_df['date'], performance_df['predicted_count'],\n","                    label='Predicted', marker='o', linestyle='-', markersize=6, color=colors[0])\n","            ax1.plot(performance_df['date'], performance_df['actual_count'],\n","                    label='Actual', marker='x', linestyle='-', markersize=6, color=colors[1])\n","            ax1.fill_between(performance_df['date'],\n","                            performance_df['predicted_count'] * 0.9,\n","                            performance_df['predicted_count'] * 1.1,\n","                            alpha=0.2, label='10% Confidence Interval', color=colors[0])\n","\n","            # Configure first subplot\n","            ax1.set_title(f'Earthquake Count Prediction Performance\\n(Last {len(performance_df)} Days)',\n","                        pad=20, fontsize=12)\n","            ax1.set_xlabel('Date', fontsize=10)\n","            ax1.set_ylabel('Number of Earthquakes', fontsize=10)\n","            ax1.legend(fontsize=10)\n","            ax1.xaxis.set_major_locator(locator)\n","            ax1.xaxis.set_major_formatter(formatter)\n","            ax1.tick_params(axis='x', rotation=45)\n","\n","            # Plot prediction error\n","            sns.lineplot(data=performance_df, x='date', y='prediction_error',\n","                        marker='o', ax=ax2, color=colors[3], label='Prediction Error')\n","\n","            # Configure second subplot\n","            ax2.set_title('Prediction Error Over Time', pad=20, fontsize=12)\n","            ax2.set_xlabel('Date', fontsize=10)\n","            ax2.set_ylabel('Absolute Error', fontsize=10)\n","            ax2.xaxis.set_major_locator(locator)\n","            ax2.xaxis.set_major_formatter(formatter)\n","            ax2.tick_params(axis='x', rotation=45)\n","\n","            # Set date limits explicitly\n","            min_date = performance_df['date'].min() - pd.Timedelta(days=1)\n","            max_date = performance_df['date'].max() + pd.Timedelta(days=1)\n","            ax1.set_xlim(min_date, max_date)\n","            ax2.set_xlim(min_date, max_date)\n","\n","            # Add grid to both plots\n","            ax1.grid(True, alpha=0.3)\n","            ax2.grid(True, alpha=0.3)\n","\n","            # Adjust layout and save\n","            plt.tight_layout()\n","            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","            plot_path = os.path.join(\n","                self.dirs['plots'],\n","                f'performance_{len(performance_df)}days_{timestamp}.png'\n","            )\n","            plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n","            plt.close()\n","\n","            print(f\"‚úÖ Visualization saved: {plot_path}\")\n","\n","            # Print performance metrics\n","            metrics_summary = {\n","                'days_tracked': len(performance_df),\n","                'mean_error': float(performance_df['prediction_error'].mean()),\n","                'max_error': float(performance_df['prediction_error'].max()),\n","                'accuracy_within_bounds': float(performance_df['within_bounds'].mean() * 100)\n","            }\n","\n","            print(\"\\nüìä Performance Summary\")\n","            print(\"-\" * 40)\n","            print(f\"Days Tracked:      {metrics_summary['days_tracked']}\")\n","            print(f\"Mean Error:        {metrics_summary['mean_error']:.1f} events\")\n","            print(f\"Max Error:         {metrics_summary['max_error']:.1f} events\")\n","            print(f\"Within Bounds:     {metrics_summary['accuracy_within_bounds']:.1f}%\")\n","            print(\"-\" * 40)\n","\n","        except Exception as e:\n","            print(f\"\\n‚ùå Error creating visualization: {str(e)}\")\n","            import traceback\n","            print(traceback.format_exc())\n","\n","    def run_baseline_training(self, days_to_process=31):\n","        \"\"\"\n","        Execute baseline model training using historical earthquake data.\n","\n","        Args:\n","            days_to_process (int): Number of days of historical data to process (default: 31)\n","\n","        Process:\n","            1. Fetches historical data for specified period\n","            2. Processes data into daily sequences\n","            3. Trains initial model on historical patterns\n","            4. Generates and evaluates predictions\n","            5. Creates performance visualizations\n","            6. Saves model checkpoints and metrics\n","\n","        Returns:\n","            bool: True if training completed successfully\n","\n","        Example:\n","            >>> success = pipeline.run_baseline_training(days_to_process=60)\n","            >>> if success:\n","            >>>     print(\"Baseline training completed successfully\")\n","        \"\"\"\n","        print(\"\\nüöÄ Initializing Baseline Training Pipeline\")\n","        print(\"=\" * 60)\n","\n","        end_date = datetime.now() - timedelta(days=1)\n","        start_date = end_date - timedelta(days=days_to_process)\n","\n","        print(f\"\\nüìÖ Processing Range: {start_date.date()} to {end_date.date()}\")\n","        print(\"-\" * 60)\n","\n","        current_date = start_date\n","        all_data = []\n","\n","        while current_date <= end_date:\n","            print(f\"\\nüìÖ Processing Date: {current_date.date()}\")\n","\n","            # Get data for current day\n","            data = self.fetch_earthquake_data(\n","                start_time=current_date,\n","                end_time=current_date + timedelta(days=1)\n","            )\n","\n","            if data is not None:\n","                # Save the daily data\n","                self.save_daily_data(data, current_date.strftime('%Y-%m-%d'))\n","                all_data.append(data)\n","\n","                if len(all_data) >= 2:  # Need at least 2 days to train/predict\n","                    # Train/optimize on available data\n","                    combined_data = pd.concat(all_data[:-1])  # Use all but last day\n","                    sequence_tensor, target_tensor = self.prepare_data(combined_data)\n","\n","                    if sequence_tensor is not None and target_tensor is not None:\n","                        print(\"\\nüîÑ Training/Optimizing Model\")\n","                        self.train_model(sequence_tensor, target_tensor, epochs=50)\n","\n","                        # Generate prediction for the last day\n","                        next_date = current_date + timedelta(days=1)\n","                        prediction = self.predict_next_day(combined_data)\n","                        if prediction is not None:\n","                            print(\"\\nüîÆ Evaluating Prediction\")\n","                            metrics = self.evaluate_predictions(prediction, all_data[-1], next_date.date())\n","\n","                            # Save prediction and visualization\n","                            self.save_predictions(\n","                                prediction,\n","                                next_date.strftime('%Y-%m-%d')\n","                            )\n","                            self.save_visualization(start_date, current_date)\n","\n","            current_date += timedelta(days=1)\n","            print(\"-\" * 60)\n","\n","        print(\"\\n‚úÖ Baseline Training Completed\")\n","        print(\"=\" * 60)\n","\n","        # Generate final performance visualization\n","        if len(all_data) > 0:\n","            print(\"\\nüìä Final Performance Summary\")\n","            self.save_visualization(start_date, end_date)\n","\n","        return True\n","\n","    def run_continuous_monitoring(self, update_interval=3600):\n","        \"\"\"\n","        Run continuous monitoring and prediction pipeline.\n","\n","        Args:\n","            update_interval (int): Seconds between updates (default: 3600 for hourly)\n","\n","        Process:\n","            1. Continuously fetches new earthquake data\n","            2. Generates predictions for next period\n","            3. Evaluates predictions against actual data\n","            4. Optimizes model based on performance\n","            5. Updates visualizations and metrics\n","            6. Saves updated model checkpoints\n","\n","        Notes:\n","            - Runs indefinitely until interrupted\n","            - Handles API timeouts and errors\n","            - Maintains continuous performance logs\n","            - Automatic model optimization\n","\n","        Example:\n","            >>> try:\n","            >>>     pipeline.run_continuous_monitoring(update_interval=7200)  # 2-hour intervals\n","            >>> except KeyboardInterrupt:\n","            >>>     print(\"Monitoring stopped by user\")\n","        \"\"\"\n","        try:\n","            print(\"\\nüîÑ Starting Continuous Monitoring\")\n","            print(\"=\" * 60)\n","            print(f\"Update Interval: {update_interval} seconds\")\n","\n","            while True:\n","                current_time = datetime.now()\n","                process_date = current_time - timedelta(days=1)\n","\n","                print(f\"\\nüìÖ Processing Data for: {process_date.date()}\")\n","                print(\"-\" * 60)\n","\n","                # Fetch yesterday's data\n","                data = self.fetch_earthquake_data(\n","                    start_time=process_date,\n","                    end_time=current_time\n","                )\n","\n","                if data is not None:\n","                    self.save_daily_data(data, process_date.strftime('%Y-%m-%d'))\n","\n","                    prediction = self.predict_next_day(data)\n","                    if prediction is not None:\n","                        self.save_predictions(\n","                            prediction,\n","                            current_time.strftime('%Y-%m-%d')\n","                        )\n","\n","                        actual_data = self.fetch_earthquake_data(\n","                            start_time=current_time.replace(hour=0, minute=0, second=0),\n","                            end_time=current_time\n","                        )\n","\n","                        if actual_data is not None:\n","                            metrics = self.evaluate_predictions(\n","                                prediction,\n","                                actual_data,\n","                                current_time.date()\n","                            )\n","                            self.optimize_model(actual_data, metrics)\n","\n","                            self.save_model_checkpoint(\n","                                epoch=None,\n","                                loss=metrics.get('relative_error', 0),\n","                                metrics=metrics\n","                            )\n","\n","                            vis_start = current_time - timedelta(days=30)\n","                            self.save_visualization(vis_start, current_time)\n","\n","                next_update = datetime.now() + timedelta(seconds=update_interval)\n","                print(f\"\\n‚è∞ Next Update: {next_update.strftime('%Y-%m-%d %H:%M:%S')}\")\n","                print(\"=\" * 60)\n","                time.sleep(update_interval)\n","\n","        except KeyboardInterrupt:\n","            print(\"\\nüëã Monitoring stopped by user\")\n","        except Exception as e:\n","            print(f\"\\n‚ùå Monitoring error: {str(e)}\")\n","            raise"],"metadata":{"id":"sGdmtfqtfEif","executionInfo":{"status":"ok","timestamp":1732047326159,"user_tz":300,"elapsed":824,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":76,"outputs":[]},{"cell_type":"markdown","source":["## Run Code"],"metadata":{"id":"fXa3EW4fCp1t"}},{"cell_type":"code","source":["# First, mount Google Drive (if using Colab)\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set up base directory in Google Drive\n","base_path = '/content/drive/My Drive/earthquake_data'\n","\n","# Initialize the pipeline\n","pipeline = EarthquakePipeline(drive_path=base_path)\n","\n","# Run historical processing then switch to continuous\n","pipeline.run_baseline_training(days_to_process=31)\n","\n","# OR just run continuous monitoring with existing model\n","# pipeline.run_continuous_monitoring(update_interval=3600)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxAsrY__HMfc","outputId":"54c9aaf7-3769-41ff-8593-9e7d41a91dfe","executionInfo":{"status":"ok","timestamp":1732044148346,"user_tz":300,"elapsed":16301,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Added missing metadata field: creation_date\n","Added missing metadata field: data_dates\n","Added missing metadata field: model_versions\n","Added missing metadata field: predictions\n","Added missing metadata field: evaluations\n","Added missing metadata field: pipeline_config\n","\n","Processing metadata for saving...\n","\n","üîÑ Starting Continuous Monitoring\n","============================================================\n","Update Interval: 3600 seconds\n","\n","üìÖ Processing Data for: 2024-11-18\n","------------------------------------------------------------\n","Fetching data from 2024-11-18 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 46\n","Date range: 2024-11-18 00:00:44.709000 to 2024-11-19 18:48:09.220000\n","Magnitude range: 2.5 to 5.6\n","------------------------------\n","\n","Processing metadata for saving...\n","Processing metadata for saving...\n","Metadata saved successfully to: /content/drive/My Drive/earthquake_data/pipeline_metadata.json\n","Metadata backup saved to: /content/drive/My Drive/earthquake_data/metadata_backup_20241119_192213.json\n","Data saved: /content/drive/My Drive/earthquake_data/data/2024-11/earthquake_data_2024-11-18.csv\n","Summary saved: /content/drive/My Drive/earthquake_data/data/2024-11/summary_2024-11-18.json\n","\n","Predictions saved to: /content/drive/My Drive/earthquake_data/predictions/2024-11/predictions_2024-11-19.csv\n","Fetching data from 2024-11-19 to 2024-11-20\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 18\n","Date range: 2024-11-19 02:41:53.877000 to 2024-11-19 18:48:09.220000\n","Magnitude range: 2.5 to 5.6\n","------------------------------\n","üìä Evaluation metrics saved: /content/drive/My Drive/earthquake_data/evaluations/2024-11/evaluation_2024-11-19.json\n","\n","üìä Prediction Evaluation\n","----------------------------------------\n","Date:            2024-11-19\n","Predicted Count: 0\n","Actual Count:    18\n","Error:           18 events\n","Relative Error:  100.0%\n","Within Bounds:   ‚ùå\n","Days Tracked:    1\n","----------------------------------------\n","\n","üîÑ Optimizing Model\n","Step 0: Loss = 325.5925\n","Step 2: Loss = 322.8756\n","Step 4: Loss = 317.8074\n","Error saving checkpoint: Object of type date is not JSON serializable\n","\n","üìà Generating Performance Visualization\n","‚úÖ Visualization saved: /content/drive/My Drive/earthquake_data/plots/performance_1days_20241119_192213.png\n","\n","üìä Performance Summary\n","----------------------------------------\n","Days Tracked:      1\n","Mean Error:        18.0 events\n","Max Error:         18.0 events\n","Within Bounds:     0.0%\n","----------------------------------------\n","\n","‚è∞ Next Update: 2024-11-19 20:22:15\n","============================================================\n","\n","üëã Monitoring stopped by user\n"]}]}]}