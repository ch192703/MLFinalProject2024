{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMEpjN4ua2AJGDZseNsQdnu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from datetime import datetime, timedelta\n","import requests\n","import os\n","from sklearn.preprocessing import MinMaxScaler\n","from torch.utils.data import Dataset, DataLoader\n","import pickle\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import folium\n","from folium import plugins"],"metadata":{"id":"EnteAQmIWVm-","executionInfo":{"status":"ok","timestamp":1731873104186,"user_tz":300,"elapsed":626,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"pVLFmsHnU-aW","executionInfo":{"status":"ok","timestamp":1731873116458,"user_tz":300,"elapsed":295,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}},"outputId":"4ae1e914-e82c-44cc-f8e6-f5e2d0db3c0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Fetching data from https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_day.geojson\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 26\n","Date range: 2024-11-16 22:17:04.400000 to 2024-11-17 19:18:44.983000\n","Magnitude range: 2.6 to 6.1\n","------------------------------\n","\n","Sample of collected data:\n","------------------------------\n","                     time  magnitude                                   place  \\\n","0 2024-11-17 19:18:44.983        4.4           254 km S of Lembar, Indonesia   \n","1 2024-11-17 19:02:56.160        4.4     96 km SSW of Alo, Wallis and Futuna   \n","2 2024-11-17 18:55:37.688        5.2  122 km ESE of Kokopo, Papua New Guinea   \n","3 2024-11-17 18:07:09.067        4.2                               Banda Sea   \n","4 2024-11-17 15:42:33.864        3.2          44 km WNW of Ninilchik, Alaska   \n","\n","   longitude  latitude    depth        type alert  tsunami  sig  \n","0   116.3326  -11.0094   10.000  earthquake  None        0  298  \n","1  -178.3951  -15.1373  422.146  earthquake  None        0  298  \n","2   153.2820   -4.7722   69.305  earthquake  None        0  416  \n","3   129.8418   -6.9173  169.981  earthquake  None        0  271  \n","4  -152.4346   60.1553   86.400  earthquake  None        0  158  \n","\n","Dataset Information:\n","------------------------------\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 26 entries, 0 to 25\n","Data columns (total 10 columns):\n"," #   Column     Non-Null Count  Dtype         \n","---  ------     --------------  -----         \n"," 0   time       26 non-null     datetime64[ns]\n"," 1   magnitude  26 non-null     float64       \n"," 2   place      26 non-null     object        \n"," 3   longitude  26 non-null     float64       \n"," 4   latitude   26 non-null     float64       \n"," 5   depth      26 non-null     float64       \n"," 6   type       26 non-null     object        \n"," 7   alert      1 non-null      object        \n"," 8   tsunami    26 non-null     int64         \n"," 9   sig        26 non-null     int64         \n","dtypes: datetime64[ns](1), float64(4), int64(2), object(3)\n","memory usage: 2.2+ KB\n","None\n"]}],"source":["def fetch_earthquake_data(timeframe='day', min_magnitude=2.5):\n","    \"\"\"\n","    Fetch earthquake data from USGS API\n","\n","    Parameters:\n","    timeframe (str): 'hour', 'day', 'week', or 'month'\n","    min_magnitude (float): Minimum earthquake magnitude to include\n","\n","    Returns:\n","    pandas DataFrame with earthquake data\n","    \"\"\"\n","    # Define base URL for USGS GeoJSON feed\n","    base_url = \"https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/\"\n","\n","    # Construct the endpoint URL based on parameters\n","    endpoint = f\"{min_magnitude}_{timeframe}.geojson\"\n","    url = base_url + endpoint\n","\n","    try:\n","        # Make the API request\n","        print(f\"Fetching data from {url}\")\n","        response = requests.get(url)\n","        response.raise_for_status()  # Raise an exception for bad status codes\n","\n","        # Parse the JSON response\n","        data = response.json()\n","\n","        # Extract the features from the GeoJSON\n","        earthquakes = data['features']\n","\n","        # Create a list to store the processed data\n","        processed_data = []\n","\n","        # Process each earthquake\n","        for quake in earthquakes:\n","            properties = quake['properties']\n","            coordinates = quake['geometry']['coordinates']\n","\n","            # Extract relevant information\n","            processed_data.append({\n","                'time': datetime.fromtimestamp(properties['time'] / 1000),  # Convert milliseconds to datetime\n","                'magnitude': properties['mag'],\n","                'place': properties['place'],\n","                'longitude': coordinates[0],\n","                'latitude': coordinates[1],\n","                'depth': coordinates[2],\n","                'type': properties['type'],\n","                'alert': properties.get('alert', 'none'),  # Default to 'none' if no alert\n","                'tsunami': properties['tsunami'],\n","                'sig': properties['sig']  # Significance value\n","            })\n","\n","        # Create DataFrame\n","        df = pd.DataFrame(processed_data)\n","\n","        # Sort by time\n","        df = df.sort_values('time', ascending=False)\n","\n","        # Print confirmation of successful data collection\n","        print(\"\\nData Collection Summary:\")\n","        print(\"-\" * 30)\n","        print(f\"Total earthquakes collected: {len(df)}\")\n","        print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n","        print(f\"Magnitude range: {df['magnitude'].min():.1f} to {df['magnitude'].max():.1f}\")\n","        print(\"-\" * 30)\n","\n","        return df\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error fetching data: {e}\")\n","        return None\n","\n","# Test the function with a small sample\n","if __name__ == \"__main__\":\n","    # Fetch last week's earthquakes of magnitude 2.5 or greater\n","    df = fetch_earthquake_data(timeframe='day', min_magnitude=2.5)\n","\n","    if df is not None:\n","        # Display the first few rows\n","        print(\"\\nSample of collected data:\")\n","        print(\"-\" * 30)\n","        print(df.head())\n","\n","        # Display data types and non-null counts\n","        print(\"\\nDataset Information:\")\n","        print(\"-\" * 30)\n","        print(df.info())"]},{"cell_type":"code","source":["def setup_drive_directory(base_path='earthquake_data'):\n","    \"\"\"Mount Google Drive and create necessary directories\"\"\"\n","    drive.mount('/content/drive')\n","    full_path = f'/content/drive/My Drive/{base_path}'\n","    if not os.path.exists(full_path):\n","        os.makedirs(full_path)\n","        print(f\"Created directory: {full_path}\")\n","    else:\n","        print(f\"Directory already exists: {full_path}\")\n","    return full_path"],"metadata":{"id":"jjF-I5ief3Vk","executionInfo":{"status":"ok","timestamp":1731873122174,"user_tz":300,"elapsed":163,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":["class EarthquakeDataset(Dataset):\n","    def __init__(self, features, targets, seq_length):\n","        self.features = features\n","        self.targets = targets\n","        self.seq_length = seq_length\n","\n","    def __len__(self):\n","        return max(0, len(self.features) - self.seq_length)\n","\n","    def __getitem__(self, idx):\n","        # Get sequence of features\n","        feature_seq = self.features[idx:idx + self.seq_length]\n","        # Get corresponding target\n","        target = self.targets[idx + self.seq_length - 1]\n","        return feature_seq, target"],"metadata":{"id":"miri2HbbefrN","executionInfo":{"status":"ok","timestamp":1731873123842,"user_tz":300,"elapsed":184,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["class TransformerPredictor(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, output_dim):\n","        super().__init__()\n","\n","        # Adjust input dimension to be divisible by num_heads\n","        self.adjusted_dim = ((input_dim // num_heads) + 1) * num_heads\n","\n","        # Initial projection to make input_dim divisible by num_heads\n","        self.input_projection = nn.Linear(input_dim, self.adjusted_dim)\n","\n","        self.pos_encoder = nn.Sequential(\n","            nn.Linear(self.adjusted_dim, self.adjusted_dim),\n","            nn.ReLU()\n","        )\n","\n","        self.transformer = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(\n","                d_model=self.adjusted_dim,\n","                nhead=num_heads,\n","                dim_feedforward=hidden_dim,\n","                dropout=0.1,\n","                batch_first=True  # Important: handle batch dimension first\n","            ),\n","            num_layers=num_layers\n","        )\n","\n","        self.output_layer = nn.Linear(self.adjusted_dim, output_dim)\n","\n","    def forward(self, x):\n","        # x shape: [batch_size, seq_length, input_dim]\n","        batch_size, seq_length, _ = x.shape\n","\n","        # Project input to adjusted dimension\n","        x = self.input_projection(x)\n","\n","        # Add positional encoding\n","        position_encoding = self.pos_encoder(x)\n","        x = x + position_encoding\n","\n","        # Apply transformer\n","        x = self.transformer(x)\n","\n","        # Get prediction from last sequence element\n","        x = x[:, -1, :]  # Take last sequence element for each batch\n","\n","        # Project to output dimension\n","        x = self.output_layer(x)\n","\n","        return x"],"metadata":{"id":"fWLUJMHMfC7x","executionInfo":{"status":"ok","timestamp":1731873140973,"user_tz":300,"elapsed":154,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":["class EarthquakePipeline:\n","    def __init__(self, drive_path=None, seq_length=3, prediction_horizon=1):\n","        if drive_path is None:\n","            self.drive_path = setup_drive_directory()\n","        else:\n","            self.drive_path = drive_path\n","        self.seq_length = seq_length\n","        self.prediction_horizon = prediction_horizon\n","        self.feature_scaler = MinMaxScaler()\n","        self.target_scaler = MinMaxScaler()\n","        self.model = None\n","        self.feature_columns = ['magnitude', 'latitude', 'longitude', 'depth', 'sig']\n","        self.target_columns = ['magnitude', 'latitude', 'longitude']\n","\n","    def fetch_historical_week(self):\n","        \"\"\"Fetch the last 7 days of data\"\"\"\n","        all_data = []\n","        end_date = datetime.now()\n","        start_date = end_date - timedelta(days=7)\n","\n","        df = fetch_earthquake_data(timeframe='week', min_magnitude=2.5)\n","        if df is not None:\n","            df = df[df['time'].between(start_date, end_date)]\n","\n","            for i in range(7):\n","                day = end_date - timedelta(days=i)\n","                day_next = day + timedelta(days=1)\n","                day_data = df[df['time'].between(day, day_next)]\n","\n","                filename = f'earthquake_data_{day.strftime(\"%Y%m%d\")}.csv'\n","                filepath = os.path.join(self.drive_path, filename)\n","                day_data.to_csv(filepath, index=False)\n","\n","                all_data.append(day_data)\n","\n","        return all_data if all_data else None\n","\n","    def split_validation_data(self, data_list):\n","        \"\"\"Split the data into yesterday's data and training data with proper handling\"\"\"\n","        if not data_list or len(data_list) < 7:\n","            raise ValueError(\"Insufficient data for splitting\")\n","\n","        # Ensure data_list contains DataFrames and they're not empty\n","        data_list = [df for df in data_list if not df.empty]\n","        if not data_list:\n","            raise ValueError(\"No valid data found in data_list\")\n","\n","        # Yesterday's data is the first element (most recent)\n","        validation_data = data_list[0]\n","\n","        # Training data is the rest (older data)\n","        training_data = pd.concat(data_list[1:])\n","\n","        print(f\"\\nData Split Summary:\")\n","        print(f\"Training data size: {len(training_data)} events\")\n","        print(f\"Validation data size: {len(validation_data)} events\")\n","\n","        return validation_data, training_data\n","\n","    def prepare_data(self, df):\n","      \"\"\"Prepare data for the model with proper sequence handling\"\"\"\n","      df = df.sort_values('time')\n","\n","      # Extract features and targets\n","      features = df[self.feature_columns].values\n","      targets = df[self.target_columns].values\n","\n","      # Scale features and targets separately\n","      scaled_features = self.feature_scaler.fit_transform(features)\n","      scaled_targets = self.target_scaler.fit_transform(targets)\n","\n","      # Convert to PyTorch tensors\n","      features_tensor = torch.FloatTensor(scaled_features)\n","      targets_tensor = torch.FloatTensor(scaled_targets)\n","\n","      # Create dataset\n","      dataset = EarthquakeDataset(features_tensor, targets_tensor, self.seq_length)\n","\n","      # Verify we have enough data\n","      if len(dataset) < 1:\n","          raise ValueError(\"Not enough data points to create sequences\")\n","\n","      return dataset\n","\n","    def train_model(self, dataset, epochs=100, batch_size=32, learning_rate=0.001):\n","      \"\"\"Train the transformer model with proper batch and sequence handling\"\"\"\n","      input_dim = len(self.feature_columns)\n","      output_dim = len(self.target_columns)\n","\n","      if self.model is None:\n","          self.model = TransformerPredictor(\n","              input_dim=input_dim,\n","              hidden_dim=64,\n","              num_layers=2,\n","              num_heads=8,\n","              output_dim=output_dim\n","          )\n","\n","      # Adjust batch size if dataset is small\n","      batch_size = min(batch_size, len(dataset))\n","\n","      # Create data loader with collate function\n","      def collate_fn(batch):\n","          # Separate features and targets\n","          features = [item[0] for item in batch]\n","          targets = [item[1] for item in batch]\n","\n","          # Stack them into tensors\n","          feature_tensor = torch.stack(features)\n","          target_tensor = torch.stack(targets)\n","\n","          return feature_tensor, target_tensor\n","\n","      dataloader = DataLoader(\n","          dataset,\n","          batch_size=batch_size,\n","          shuffle=True,\n","          collate_fn=collate_fn,\n","          drop_last=True\n","      )\n","\n","      if len(dataloader) == 0:\n","          raise ValueError(\"Not enough data to create batches\")\n","\n","      optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n","      criterion = nn.MSELoss()\n","\n","      print(f\"\\nTraining with:\")\n","      print(f\"Batch size: {batch_size}\")\n","      print(f\"Sequence length: {self.seq_length}\")\n","      print(f\"Number of batches per epoch: {len(dataloader)}\")\n","      print(f\"Input dimension: {input_dim}\")\n","      print(f\"Output dimension: {output_dim}\")\n","\n","      best_loss = float('inf')\n","      patience = 5\n","      patience_counter = 0\n","\n","      for epoch in range(epochs):\n","          total_loss = 0\n","          batch_count = 0\n","          self.model.train()\n","\n","          for batch_features, batch_targets in dataloader:\n","              # Print shapes for debugging in first epoch\n","              if epoch == 0 and batch_count == 0:\n","                  print(f\"\\nBatch shapes:\")\n","                  print(f\"Features: {batch_features.shape}\")\n","                  print(f\"Targets: {batch_targets.shape}\")\n","\n","              optimizer.zero_grad()\n","\n","              # Forward pass\n","              predictions = self.model(batch_features)\n","\n","              # Compute loss\n","              loss = criterion(predictions, batch_targets)\n","\n","              # Backward pass\n","              loss.backward()\n","              optimizer.step()\n","\n","              total_loss += loss.item()\n","              batch_count += 1\n","\n","          if batch_count == 0:\n","              print(\"No valid batches in epoch\")\n","              continue\n","\n","          avg_loss = total_loss / batch_count\n","\n","          if (epoch + 1) % 10 == 0:\n","              print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n","\n","          if avg_loss < best_loss:\n","              best_loss = avg_loss\n","              patience_counter = 0\n","          else:\n","              patience_counter += 1\n","              if patience_counter >= patience:\n","                  print(f\"Early stopping at epoch {epoch+1}\")\n","                  break\n","\n","    def optimize_model(self, validation_data, current_metrics, learning_rate=0.0001):\n","        \"\"\"Optimize model based on validation performance\"\"\"\n","        print(\"\\nOptimizing model based on validation results...\")\n","\n","        val_dataset = self.prepare_data(validation_data)\n","        optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)\n","        criterion = nn.MSELoss()\n","\n","        best_loss = float('inf')\n","        patience = 3\n","        patience_counter = 0\n","\n","        for epoch in range(50):\n","            total_loss = 0\n","            self.model.train()\n","\n","            dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n","\n","            for batch_features, batch_targets in dataloader:\n","                optimizer.zero_grad()\n","                predictions = self.model(batch_features)\n","                loss = criterion(predictions, batch_targets)\n","                loss.backward()\n","                optimizer.step()\n","                total_loss += loss.item()\n","\n","            avg_loss = total_loss / len(dataloader)\n","\n","            if avg_loss < best_loss:\n","                best_loss = best_loss\n","                patience_counter = 0\n","                print(f\"Optimization Epoch {epoch+1}: Loss improved to {avg_loss:.4f}\")\n","            else:\n","                patience_counter += 1\n","                if patience_counter >= patience:\n","                    print(\"Early stopping optimization\")\n","                    break\n","\n","        return best_loss\n","\n","    def predict_for_validation(self, training_data):\n","        \"\"\"Generate predictions for validation period with error handling\"\"\"\n","        self.model.eval()\n","        try:\n","            with torch.no_grad():\n","                # Get the most recent sequence of data\n","                recent_data = training_data.sort_values('time').tail(self.seq_length)\n","\n","                if len(recent_data) < self.seq_length:\n","                    raise ValueError(f\"Insufficient data for sequence length {self.seq_length}\")\n","\n","                print(f\"\\nPrediction Data Summary:\")\n","                print(f\"Using {len(recent_data)} most recent events for prediction\")\n","                print(f\"Time range: {recent_data['time'].min()} to {recent_data['time'].max()}\")\n","\n","                scaled_features = self.feature_scaler.transform(recent_data[self.feature_columns].values)\n","                features_tensor = torch.FloatTensor(scaled_features)\n","\n","                # Add batch dimension\n","                features_tensor = features_tensor.unsqueeze(0)\n","\n","                # Get predictions\n","                predictions = self.model(features_tensor)\n","                unscaled_predictions = self.target_scaler.inverse_transform(predictions.numpy())\n","\n","                return pd.DataFrame(\n","                    unscaled_predictions,\n","                    columns=['predicted_magnitude', 'predicted_latitude', 'predicted_longitude']\n","                )\n","\n","        except Exception as e:\n","            print(f\"Error in prediction: {str(e)}\")\n","            return pd.DataFrame()\n","\n","    def predict_next_day(self, recent_data):\n","        \"\"\"Generate predictions for the next day\"\"\"\n","        self.model.eval()\n","        with torch.no_grad():\n","            recent_data = recent_data.sort_values('time').tail(self.seq_length)\n","            scaled_features = self.feature_scaler.transform(recent_data[self.feature_columns].values)\n","            features_tensor = torch.FloatTensor(scaled_features)\n","\n","            predictions = self.model(features_tensor.unsqueeze(0))\n","            unscaled_predictions = self.target_scaler.inverse_transform(predictions.numpy())\n","\n","            prediction_df = pd.DataFrame(\n","                unscaled_predictions,\n","                columns=['predicted_magnitude', 'predicted_latitude', 'predicted_longitude']\n","            )\n","\n","            prediction_df['predicted_time'] = datetime.now() + timedelta(days=1)\n","\n","            timestamp = datetime.now().strftime('%Y%m%d')\n","            prediction_file = os.path.join(self.drive_path, f'prediction_{timestamp}.csv')\n","            prediction_df.to_csv(prediction_file, index=False)\n","\n","            return prediction_df\n","\n","    def visualize_predictions(self, predictions, actual_data):\n","        \"\"\"Create comprehensive visualizations of predictions vs actual values\"\"\"\n","        # Create directory for visualizations if it doesn't exist\n","        vis_dir = os.path.join(self.drive_path, 'visualizations')\n","        os.makedirs(vis_dir, exist_ok=True)\n","\n","        # 1. Static Plots\n","        fig = plt.figure(figsize=(15, 10))\n","\n","        # Magnitude Comparison\n","        plt.subplot(2, 2, 1)\n","        plt.title('Earthquake Magnitude: Predicted vs Actual')\n","        plt.plot(actual_data['magnitude'].values, label='Actual', marker='o')\n","        plt.plot(predictions['predicted_magnitude'].values, label='Predicted', marker='x')\n","        plt.xlabel('Event Index')\n","        plt.ylabel('Magnitude')\n","        plt.legend()\n","        plt.grid(True)\n","\n","        # Error Distribution\n","        plt.subplot(2, 2, 2)\n","        errors = {\n","            'Magnitude Error': np.abs(predictions['predicted_magnitude'].values - actual_data['magnitude'].values),\n","            'Latitude Error': np.abs(predictions['predicted_latitude'].values - actual_data['latitude'].values),\n","            'Longitude Error': np.abs(predictions['predicted_longitude'].values - actual_data['longitude'].values)\n","        }\n","\n","        sns.boxplot(data=pd.DataFrame(errors))\n","        plt.title('Prediction Error Distribution')\n","        plt.ylabel('Absolute Error')\n","        plt.xticks(rotation=45)\n","\n","        # Time Series of Errors\n","        plt.subplot(2, 1, 2)\n","        for col, err in errors.items():\n","            plt.plot(err, label=col, marker='o')\n","        plt.title('Prediction Errors Over Time')\n","        plt.xlabel('Event Index')\n","        plt.ylabel('Absolute Error')\n","        plt.legend()\n","        plt.grid(True)\n","\n","        plt.tight_layout()\n","\n","        # Save static plots\n","        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","        static_plot_path = os.path.join(vis_dir, f'prediction_analysis_{timestamp}.png')\n","        plt.savefig(static_plot_path)\n","        plt.close()\n","\n","        # 2. Interactive Map\n","        # Calculate center point for the map\n","        center_lat = np.mean([actual_data['latitude'].mean(), predictions['predicted_latitude'].mean()])\n","        center_lon = np.mean([actual_data['longitude'].mean(), predictions['predicted_longitude'].mean()])\n","\n","        # Create map\n","        m = folium.Map(location=[center_lat, center_lon], zoom_start=4)\n","\n","        # Add actual earthquakes\n","        actual_fg = folium.FeatureGroup(name='Actual Earthquakes')\n","        for idx, row in actual_data.iterrows():\n","            folium.CircleMarker(\n","                location=[row['latitude'], row['longitude']],\n","                radius=row['magnitude'] * 2,  # Size based on magnitude\n","                popup=f\"Actual: Mag {row['magnitude']:.1f}\",\n","                color='blue',\n","                fill=True\n","            ).add_to(actual_fg)\n","        actual_fg.add_to(m)\n","\n","        # Add predicted earthquakes\n","        pred_fg = folium.FeatureGroup(name='Predicted Earthquakes')\n","        for idx, row in predictions.iterrows():\n","            folium.CircleMarker(\n","                location=[row['predicted_latitude'], row['predicted_longitude']],\n","                radius=row['predicted_magnitude'] * 2,\n","                popup=f\"Predicted: Mag {row['predicted_magnitude']:.1f}\",\n","                color='red',\n","                fill=True\n","            ).add_to(pred_fg)\n","        pred_fg.add_to(m)\n","\n","        # Add connection lines between actual and predicted locations\n","        lines_fg = folium.FeatureGroup(name='Prediction Lines')\n","        for idx in range(len(predictions)):\n","            folium.PolyLine(\n","                locations=[\n","                    [actual_data.iloc[idx]['latitude'], actual_data.iloc[idx]['longitude']],\n","                    [predictions.iloc[idx]['predicted_latitude'], predictions.iloc[idx]['predicted_longitude']]\n","                ],\n","                weight=1,\n","                color='gray',\n","                opacity=0.5\n","            ).add_to(lines_fg)\n","        lines_fg.add_to(m)\n","\n","        # Add layer control\n","        folium.LayerControl().add_to(m)\n","\n","        # Save interactive map\n","        map_path = os.path.join(vis_dir, f'prediction_map_{timestamp}.html')\n","        m.save(map_path)\n","\n","        print(f\"\\nVisualizations saved to:\")\n","        print(f\"1. Static plots: {static_plot_path}\")\n","        print(f\"2. Interactive map: {map_path}\")\n","\n","        # Create prediction summary\n","        summary_df = pd.DataFrame({\n","            'Metric': ['Average Magnitude Error', 'Average Location Error (km)', 'Max Magnitude Error', 'Max Location Error (km)'],\n","            'Value': [\n","                np.mean(errors['Magnitude Error']),\n","                np.mean(np.sqrt(errors['Latitude Error']**2 + errors['Longitude Error']**2) * 111),\n","                np.max(errors['Magnitude Error']),\n","                np.max(np.sqrt(errors['Latitude Error']**2 + errors['Longitude Error']**2) * 111)\n","            ]\n","        })\n","\n","        # Save summary\n","        summary_path = os.path.join(vis_dir, f'prediction_summary_{timestamp}.csv')\n","        summary_df.to_csv(summary_path, index=False)\n","\n","        print(f\"3. Summary statistics: {summary_path}\")\n","\n","        # Print summary to console\n","        print(\"\\nPrediction Summary:\")\n","        print(\"-\" * 50)\n","        print(summary_df.to_string(index=False))\n","\n","        return {\n","            'static_plot': static_plot_path,\n","            'interactive_map': map_path,\n","            'summary': summary_path\n","        }\n","\n","    def evaluate_predictions(self, predictions, validation_data):\n","        \"\"\"Enhanced evaluation with detailed statistics and visualizations\"\"\"\n","        if predictions.empty or validation_data.empty:\n","            print(\"Warning: Empty predictions or validation data\")\n","            return None\n","\n","        actual_data = validation_data[self.target_columns].head(len(predictions))\n","\n","        if len(actual_data) == 0:\n","            print(\"Warning: No matching validation data found\")\n","            return None\n","\n","        # Calculate basic metrics\n","        mse = np.mean((actual_data.values - predictions.values) ** 2, axis=0)\n","        mae = np.mean(np.abs(actual_data.values - predictions.values), axis=0)\n","\n","        # Calculate MAPE with handling for zero values\n","        mape = []\n","        for i in range(len(self.target_columns)):\n","            actual_vals = actual_data.values[:, i]\n","            pred_vals = predictions.values[:, i]\n","            valid_indices = actual_vals != 0\n","            if np.any(valid_indices):\n","                mape_val = np.mean(np.abs((actual_vals[valid_indices] - pred_vals[valid_indices]) /\n","                                        actual_vals[valid_indices])) * 100\n","            else:\n","                mape_val = np.nan\n","            mape.append(mape_val)\n","\n","        # Create detailed comparison DataFrame\n","        comparison_df = pd.DataFrame({\n","            'Event_Time': validation_data['time'].head(len(predictions)),\n","            'Actual_Magnitude': actual_data['magnitude'],\n","            'Predicted_Magnitude': predictions['predicted_magnitude'],\n","            'Actual_Latitude': actual_data['latitude'],\n","            'Predicted_Latitude': predictions['predicted_latitude'],\n","            'Actual_Longitude': actual_data['longitude'],\n","            'Predicted_Longitude': predictions['predicted_longitude']\n","        })\n","\n","        # Calculate errors\n","        comparison_df['Magnitude_Error'] = abs(comparison_df['Actual_Magnitude'] - comparison_df['Predicted_Magnitude'])\n","        comparison_df['Location_Error_km'] = np.sqrt(\n","            (comparison_df['Actual_Latitude'] - comparison_df['Predicted_Latitude'])**2 +\n","            (comparison_df['Actual_Longitude'] - comparison_df['Predicted_Longitude'])**2\n","        ) * 111  # Rough conversion to kilometers\n","\n","        # Print detailed evaluation\n","        print(\"\\nPrediction Evaluation:\")\n","        print(\"-\" * 50)\n","        print(\"\\nSummary Statistics:\")\n","        print(f\"Number of predictions: {len(predictions)}\")\n","\n","        for i, feature in enumerate(self.target_columns):\n","            print(f\"\\n{feature.capitalize()} Metrics:\")\n","            print(f\"MSE: {mse[i]:.4f}\")\n","            print(f\"MAE: {mae[i]:.4f}\")\n","            print(f\"MAPE: {mape[i]:.2f}%\")\n","\n","        print(\"\\nLocation Error Statistics (km):\")\n","        print(comparison_df['Location_Error_km'].describe())\n","\n","        # Generate visualizations\n","        self.visualize_predictions(predictions, actual_data)\n","\n","        # Save detailed results\n","        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","        results_path = os.path.join(self.drive_path, f'prediction_results_{timestamp}.csv')\n","        comparison_df.to_csv(results_path, index=False)\n","        print(f\"\\nDetailed results saved to: {results_path}\")\n","\n","        return {\n","            'mse': mse,\n","            'mae': mae,\n","            'mape': mape,\n","            'comparison': comparison_df,\n","            'summary_stats': {\n","                'location_error_mean': comparison_df['Location_Error_km'].mean(),\n","                'location_error_median': comparison_df['Location_Error_km'].median(),\n","                'magnitude_error_mean': comparison_df['Magnitude_Error'].mean(),\n","                'magnitude_error_median': comparison_df['Magnitude_Error'].median()\n","            }\n","        }\n","\n","    def save_model(self):\n","        \"\"\"Save model and scalers to Google Drive\"\"\"\n","        model_path = os.path.join(self.drive_path, 'earthquake_model.pth')\n","        feature_scaler_path = os.path.join(self.drive_path, 'feature_scaler.pkl')\n","        target_scaler_path = os.path.join(self.drive_path, 'target_scaler.pkl')\n","\n","        torch.save(self.model.state_dict(), model_path)\n","        with open(feature_scaler_path, 'wb') as f:\n","            pickle.dump(self.feature_scaler, f)\n","        with open(target_scaler_path, 'wb') as f:\n","            pickle.dump(self.target_scaler, f)\n","\n","    def load_model(self):\n","        \"\"\"Load model and scalers from Google Drive\"\"\"\n","        model_path = os.path.join(self.drive_path, 'earthquake_model.pth')\n","        feature_scaler_path = os.path.join(self.drive_path, 'feature_scaler.pkl')\n","        target_scaler_path = os.path.join(self.drive_path, 'target_scaler.pkl')\n","\n","        if all(os.path.exists(p) for p in [model_path, feature_scaler_path, target_scaler_path]):\n","            self.model.load_state_dict(torch.load(model_path))\n","            with open(feature_scaler_path, 'rb') as f:\n","                self.feature_scaler = pickle.load(f)\n","            with open(target_scaler_path, 'rb') as f:\n","                self.target_scaler = pickle.load(f)\n","\n","    def run_validation_pipeline(self):\n","        \"\"\"Run the initial validation pipeline with comprehensive error handling\"\"\"\n","        try:\n","            print(\"1. Fetching historical data...\")\n","            historical_data = self.fetch_historical_week()\n","\n","            if not historical_data:\n","                raise ValueError(\"No historical data retrieved\")\n","\n","            print(\"2. Splitting data into validation and training sets...\")\n","            validation_data, training_data = self.split_validation_data(historical_data)\n","\n","            print(\"3. Preparing training data...\")\n","            train_dataset = self.prepare_data(training_data)\n","            print(f\"Dataset size: {len(train_dataset)} sequences\")\n","\n","            print(\"4. Training model...\")\n","            self.train_model(train_dataset)\n","\n","            print(\"5. Generating predictions...\")\n","            predictions = self.predict_for_validation(training_data)\n","\n","            if predictions.empty:\n","                raise ValueError(\"Failed to generate predictions\")\n","\n","            print(\"6. Evaluating predictions...\")\n","            metrics = self.evaluate_predictions(predictions, validation_data)\n","\n","            if metrics is None:\n","                raise ValueError(\"Failed to compute evaluation metrics\")\n","\n","            return predictions, validation_data, metrics\n","\n","        except Exception as e:\n","            print(f\"Error in validation pipeline: {str(e)}\")\n","            return None, None, None\n","\n","    def run_continuous_pipeline(self):\n","        \"\"\"Run the continuous pipeline for iterative optimization and prediction\"\"\"\n","        try:\n","            print(\"\\nFetching today's actual data...\")\n","            today_data = fetch_earthquake_data(timeframe='day', min_magnitude=2.5)\n","\n","            yesterday = datetime.now() - timedelta(days=1)\n","            prediction_file = os.path.join(\n","                self.drive_path,\n","                f'prediction_{yesterday.strftime(\"%Y%m%d\")}.csv'\n","            )\n","\n","            if os.path.exists(prediction_file):\n","                print(\"\\nEvaluating yesterday's predictions...\")\n","                yesterday_pred = pd.read_csv(prediction_file)\n","\n","                # Evaluate prediction\n","                metrics = self.evaluate_predictions(yesterday_pred, today_data)\n","\n","                # Optimize model based on performance\n","                print(\"\\nOptimizing model...\")\n","                optimization_result = self.optimize_model(today_data, metrics)\n","\n","                print(f\"\\nModel optimization complete. Final loss: {optimization_result:.4f}\")\n","\n","            # Generate prediction for tomorrow\n","            print(\"\\nGenerating prediction for tomorrow...\")\n","            tomorrow_pred = self.predict_next_day(today_data)\n","\n","            print(\"\\nPrediction for tomorrow:\")\n","            print(tomorrow_pred)\n","\n","            # Save updated model\n","            self.save_model()\n","\n","            return {\n","                'today_data': today_data,\n","                'yesterday_prediction': yesterday_pred if 'yesterday_pred' in locals() else None,\n","                'tomorrow_prediction': tomorrow_pred,\n","                'metrics': metrics if 'metrics' in locals() else None\n","            }\n","\n","        except Exception as e:\n","            print(f\"Error in continuous pipeline: {str(e)}\")\n","            return None\n","\n","    def run_complete_pipeline(self, is_initial=False):\n","        \"\"\"Run either initial validation or continuous pipeline\"\"\"\n","        if is_initial:\n","            print(\"\\nRunning initial validation pipeline...\")\n","            return self.run_validation_pipeline()\n","        else:\n","            print(\"\\nRunning continuous optimization pipeline...\")\n","            return self.run_continuous_pipeline()"],"metadata":{"id":"sGdmtfqtfEif","executionInfo":{"status":"ok","timestamp":1731873170031,"user_tz":300,"elapsed":146,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["def run_pipeline():\n","    \"\"\"Run the complete earthquake prediction pipeline with error handling\"\"\"\n","    try:\n","        # Initialize the pipeline\n","        pipeline = EarthquakePipeline()\n","\n","        # First run: Establish baseline\n","        print(\"Establishing baseline model...\")\n","        predictions, validation_data, metrics = pipeline.run_complete_pipeline(is_initial=True)\n","\n","        if predictions is None:\n","            raise ValueError(\"Failed to establish baseline model\")\n","\n","        print(\"\\nBaseline Model Performance:\")\n","        print(\"-\" * 50)\n","        print(\"Validation Data Statistics:\")\n","        print(f\"Number of events: {len(validation_data)}\")\n","        print(\"\\nPrediction Metrics:\")\n","        for metric_name, values in metrics.items():\n","            print(f\"{metric_name.upper()}: {values}\")\n","\n","        # After baseline is established, run daily pipeline\n","        while True:\n","            user_input = input(\"\\nRun daily pipeline? (yes/no): \")\n","            if user_input.lower() != 'yes':\n","                print(\"\\nExiting pipeline. Model and data saved.\")\n","                break\n","\n","            print(\"\\nRunning daily pipeline update...\")\n","            results = pipeline.run_complete_pipeline(is_initial=False)\n","\n","            if results:\n","                print(\"\\nPipeline Results:\")\n","                print(\"-\" * 50)\n","\n","                # Print yesterday's performance if available\n","                if results['yesterday_prediction'] is not None:\n","                    print(\"\\nYesterday's Prediction Performance:\")\n","                    print(\"-\" * 30)\n","                    print(\"Predicted vs Actual:\")\n","                    comparison = pd.DataFrame({\n","                        'Predicted': results['yesterday_prediction'].values[:, 0],\n","                        'Actual': results['today_data'][pipeline.target_columns].values[0]\n","                    }, index=pipeline.target_columns)\n","                    print(comparison)\n","                    print(\"\\nMetrics:\")\n","                    for metric_name, values in results['metrics'].items():\n","                        print(f\"{metric_name.upper()}: {values}\")\n","\n","                # Print tomorrow's predictions\n","                print(\"\\nTomorrow's Predictions:\")\n","                print(\"-\" * 30)\n","                pred_df = results['tomorrow_prediction']\n","                for col in ['predicted_magnitude', 'predicted_latitude', 'predicted_longitude']:\n","                    print(f\"{col}: {pred_df[col].values[0]:.4f}\")\n","\n","                # Save results\n","                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n","                results_file = f'pipeline_results_{timestamp}.txt'\n","                with open(os.path.join(pipeline.drive_path, results_file), 'w') as f:\n","                    f.write(f\"Pipeline Results - {timestamp}\\n\")\n","                    f.write(\"-\" * 50 + \"\\n\")\n","                    if results['yesterday_prediction'] is not None:\n","                        f.write(\"\\nYesterday's Performance:\\n\")\n","                        f.write(str(results['metrics']))\n","                    f.write(\"\\n\\nTomorrow's Predictions:\\n\")\n","                    f.write(str(results['tomorrow_prediction']))\n","\n","                print(f\"\\nResults saved to {results_file}\")\n","\n","            else:\n","                print(\"\\nError: Failed to generate results for today\")\n","\n","            print(\"\\nWaiting for next day's data...\")\n","\n","    except Exception as e:\n","        print(f\"\\nError running pipeline: {str(e)}\")\n","        return False\n","\n","    return True\n","\n","if __name__ == \"__main__\":\n","    success = run_pipeline()\n","    if not success:\n","        print(\"\\nPipeline execution failed. Please check the error messages above.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F5BtEjNGfWop","executionInfo":{"status":"ok","timestamp":1731873496811,"user_tz":300,"elapsed":27234,"user":{"displayName":"Stephen M","userId":"14405601716912570098"}},"outputId":"42b0fac4-a1a4-479c-b942-634ff8e12cfd"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Directory already exists: /content/drive/My Drive/earthquake_data\n","Establishing baseline model...\n","\n","Running initial validation pipeline...\n","1. Fetching historical data...\n","Fetching data from https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_week.geojson\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 253\n","Date range: 2024-11-10 20:13:16.630000 to 2024-11-17 19:18:44.983000\n","Magnitude range: 2.5 to 6.6\n","------------------------------\n","2. Splitting data into validation and training sets...\n","\n","Data Split Summary:\n","Training data size: 200 events\n","Validation data size: 26 events\n","3. Preparing training data...\n","Dataset size: 197 sequences\n","4. Training model...\n","\n","Training with:\n","Batch size: 32\n","Sequence length: 3\n","Number of batches per epoch: 6\n","Input dimension: 5\n","Output dimension: 3\n","\n","Batch shapes:\n","Features: torch.Size([32, 3, 5])\n","Targets: torch.Size([32, 3])\n","Epoch 10/100, Loss: 0.0481\n","Epoch 20/100, Loss: 0.0234\n","Epoch 30/100, Loss: 0.0147\n","Early stopping at epoch 37\n","5. Generating predictions...\n","\n","Prediction Data Summary:\n","Using 3 most recent events for prediction\n","Time range: 2024-11-16 18:25:27.713000 to 2024-11-16 19:12:30.600000\n","6. Evaluating predictions...\n","\n","Prediction Evaluation:\n","--------------------------------------------------\n","\n","Summary Statistics:\n","Number of predictions: 1\n","\n","Magnitude Metrics:\n","MSE: 0.4184\n","MAE: 0.6468\n","MAPE: 14.70%\n","\n","Latitude Metrics:\n","MSE: 1908.9047\n","MAE: 43.6910\n","MAPE: 396.85%\n","\n","Longitude Metrics:\n","MSE: 32352.4646\n","MAE: 179.8679\n","MAPE: 154.62%\n","\n","Location Error Statistics (km):\n","count        1.000000\n","mean     20545.907901\n","std               NaN\n","min      20545.907901\n","25%      20545.907901\n","50%      20545.907901\n","75%      20545.907901\n","max      20545.907901\n","Name: Location_Error_km, dtype: float64\n","\n","Visualizations saved to:\n","1. Static plots: /content/drive/My Drive/earthquake_data/visualizations/prediction_analysis_20241117_195752.png\n","2. Interactive map: /content/drive/My Drive/earthquake_data/visualizations/prediction_map_20241117_195752.html\n","3. Summary statistics: /content/drive/My Drive/earthquake_data/visualizations/prediction_summary_20241117_195752.csv\n","\n","Prediction Summary:\n","--------------------------------------------------\n","                     Metric        Value\n","    Average Magnitude Error     0.646836\n","Average Location Error (km) 20545.907901\n","        Max Magnitude Error     0.646836\n","    Max Location Error (km) 20545.907901\n","\n","Detailed results saved to: /content/drive/My Drive/earthquake_data/prediction_results_20241117_195753.csv\n","\n","Baseline Model Performance:\n","--------------------------------------------------\n","Validation Data Statistics:\n","Number of events: 26\n","\n","Prediction Metrics:\n","MSE: [4.18396434e-01 1.90890470e+03 3.23524646e+04]\n","MAE: [  0.64683571  43.69101392 179.86790884]\n","MAPE: [14.700811559503734, 396.85190766180847, 154.6152229365549]\n","COMPARISON:                Event_Time  Actual_Magnitude  Predicted_Magnitude  \\\n","0 2024-11-17 19:18:44.983               4.4             3.753164   \n","\n","   Actual_Latitude  Predicted_Latitude  Actual_Longitude  Predicted_Longitude  \\\n","0         -11.0094           32.681614          116.3326           -63.535309   \n","\n","   Magnitude_Error  Location_Error_km  \n","0         0.646836       20545.907901  \n","SUMMARY_STATS: {'location_error_mean': 20545.907901116283, 'location_error_median': 20545.907901116283, 'magnitude_error_mean': 0.6468357086181644, 'magnitude_error_median': 0.6468357086181644}\n","\n","Run daily pipeline? (yes/no): yes\n","\n","Running daily pipeline update...\n","\n","Running continuous optimization pipeline...\n","\n","Fetching today's actual data...\n","Fetching data from https://earthquake.usgs.gov/earthquakes/feed/v1.0/summary/2.5_day.geojson\n","\n","Data Collection Summary:\n","------------------------------\n","Total earthquakes collected: 26\n","Date range: 2024-11-16 22:17:04.400000 to 2024-11-17 19:18:44.983000\n","Magnitude range: 2.6 to 6.1\n","------------------------------\n","\n","Generating prediction for tomorrow...\n","\n","Prediction for tomorrow:\n","   predicted_magnitude  predicted_latitude  predicted_longitude  \\\n","0             4.832536           11.050619           130.959717   \n","\n","              predicted_time  \n","0 2024-11-18 19:58:02.906660  \n","\n","Pipeline Results:\n","--------------------------------------------------\n","\n","Tomorrow's Predictions:\n","------------------------------\n","predicted_magnitude: 4.8325\n","predicted_latitude: 11.0506\n","predicted_longitude: 130.9597\n","\n","Results saved to pipeline_results_20241117_195802.txt\n","\n","Waiting for next day's data...\n","\n","Run daily pipeline? (yes/no): no\n","\n","Exiting pipeline. Model and data saved.\n"]}]}]}