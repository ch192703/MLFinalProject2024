{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ch192703/MLFinalProject2024/blob/main/IDS6938_DigitalTwin_Earthquake_v5_lynn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Earthquake Prediction Pipeline Documentation**\n"
      ],
      "metadata": {
        "id": "GPhtaEIJBAVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview**\n",
        "\n",
        "The Earthquake Prediction Pipeline is a comprehensive system that automates the collection, processing, and analysis of USGS earthquake data to predict future seismic activity. The pipeline implements a transformer-based model that learns from historical patterns to predict the number of earthquakes likely to occur in the next 24-hour period."
      ],
      "metadata": {
        "id": "a4jXwipSAPW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Key Features**\n",
        "\n",
        "*   Automated USGS data collection and processing\n",
        "*   Daily data segmentation and storage\n",
        "*   Transformer-based sequence modeling\n",
        "*   Continuous prediction and evaluation\n",
        "*   Automated model optimization\n",
        "*   Performance visualization and tracking\n",
        "*   Modular architecture with comprehensive error handling\n"
      ],
      "metadata": {
        "id": "dPFH6Hz7_3rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **System Requirements**\n",
        "\n",
        "Python 3.x w/Required Libraries:\n",
        "\n",
        "*   pandas\n",
        "*   numpy\n",
        "*   torch (PyTorch)\n",
        "*   requests\n",
        "*   matplotlib\n",
        "*   seaborn\n",
        "*   scikit-learn\n"
      ],
      "metadata": {
        "id": "BRHZKpLEAYYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Directory Structure**\n",
        "\n",
        "\n",
        "```\n",
        "/earthquake_data/\n",
        "├── data/             # Raw daily earthquake data\n",
        "│   └── YYYY-MM/      # Organized by year-month\n",
        "├── models/           # Saved model checkpoints\n",
        "├── predictions/      # Daily prediction outputs\n",
        "├── plots/           # Performance visualizations\n",
        "└── evaluations/     # Evaluation metrics\n",
        "```"
      ],
      "metadata": {
        "id": "T1CUq-hkAmZ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Core Components**\n",
        "\n",
        "1. Data Collection and Processing\n",
        "\n",
        "*   USGS API Integration: Automated fetching of earthquake data\n",
        "*   Data Filtering: Configurable magnitude threshold (default: 2.5)\n",
        "*   Data Storage: Daily CSV files with comprehensive metadata\n",
        "*   Feature Extraction: Geographic and seismic parameters\n",
        "\n",
        "2. Model Architecture\n",
        "\n",
        "*   Type: Transformer-based sequence model\n",
        "\n",
        "*   Components:\n",
        " *   Input projection layer\n",
        " *   Positional encoding\n",
        " *   Multi-head attention layers\n",
        " *   Feed-forward networks\n",
        " *   Output projection layer\n",
        "\n",
        "*   Parameters:\n",
        " *   Sequence Length: Configurable (default: 7 days)\n",
        " *   Hidden Dimensions: 64\n",
        " *   Number of Layers: 2\n",
        " *   Attention Heads: 4\n",
        "\n",
        "3. Training Pipeline\n",
        "*    **Baseline Training**\n",
        "\n",
        " 1. Historical Data Processing\n",
        "\n",
        "  *   Fetches specified number of days (default: 31)\n",
        "  *   Splits data into daily segments\n",
        "  *   Creates initial training sequences\n",
        "\n",
        " 2. Model Training\n",
        "\n",
        "  *   Sequences created from historical data\n",
        "  *   Loss function: Mean Squared Error\n",
        "  *   Optimizer: Adam\n",
        "  *   Checkpoint saving based on performance\n",
        "\n",
        " 3. Evaluation\n",
        "\n",
        "  *   Daily prediction accuracy\n",
        "  *   Error metrics calculation\n",
        "  *   Performance visualization\n",
        "  *   Metadata tracking\n",
        "\n",
        "*    **Continuous Monitoring**\n",
        "\n",
        " 1. Automated Data Collection\n",
        "\n",
        "  *   Configurable update interval (default: 1 hour)\n",
        "  *   Real-time USGS data integration\n",
        "\n",
        " 2. Prediction Generation\n",
        "\n",
        "  *   Daily earthquake count predictions\n",
        "  *   Confidence interval calculation\n",
        "  *   Prediction storage and tracking\n",
        "\n",
        " 3. Model Optimization\n",
        "\n",
        "  *   Performance evaluation against actual data\n",
        "  *   Incremental model updates\n",
        "  *   Automated checkpoint management\n",
        "\n",
        "4. Performance Metrics\n",
        "\n",
        "  *   Prediction Error (absolute and relative)\n",
        "  *   Confidence Interval Coverage\n",
        "  *   Standard Deviation Analysis\n",
        "  *   Visualization of Trends"
      ],
      "metadata": {
        "id": "WuHgC8POAyD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **File Naming Conventions**\n",
        "\n",
        "*   Data Files: *earthquake_data_YYYY-MM-DD.csv*\n",
        "*   Predictions: *predictions_YYYY-MM-DD.csv*\n",
        "*   Model Checkpoints: *model_checkpoint_YYYYMMDD_HHMMSS.pth*\n",
        "*   Visualizations: *performance_Ndays_YYYYMMDD_HHMMSS.png*\n",
        "*   Evaluations: *evaluation_YYYY-MM-DD.json*"
      ],
      "metadata": {
        "id": "hjn2s5JcA-tU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Usage Examples**\n",
        "##### **Initialize Pipeline**\n",
        "```\n",
        "pipeline = EarthquakePipeline(drive_path='/path/to/base/directory')\n",
        "```\n",
        "##### **Run Baseline Training**\n",
        "```\n",
        "pipeline.run_baseline_training(days_to_process=31)\n",
        "```\n",
        "##### **Start Continuous Monitoring**\n",
        "```pipeline.run_continuous_monitoring(update_interval=3600)  # 1 hour interval\n",
        "```"
      ],
      "metadata": {
        "id": "O9qA20LVBKdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Future Enhancements**\n",
        "\n",
        "*   Integration with additional data sources\n",
        "*   Enhanced feature engineering\n",
        "*   Advanced visualization capabilities\n",
        "*   Automated parameter optimization\n",
        "*   Real-time alerting system\n",
        "*   Web interface for monitoring"
      ],
      "metadata": {
        "id": "CRKvCNnTBN0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model Misc Info**\n",
        "*   Authors: Stephen Moore, Steven Willhelm, Lynn Yingling\n",
        "*   Version: 4.0\n",
        "*   Last Updated: 19 November 2024"
      ],
      "metadata": {
        "id": "9BUgX_acBQ9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Imports"
      ],
      "metadata": {
        "id": "SiEjZK1o_o05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Required imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "import time\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "EnteAQmIWVm-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Create Drive Directory\n",
        "**Lynn:** Made a few changes here for one key reason: to support regional data organization. Since we're moving from a global to a regional approach, we need separate subdirectories for each seismic region's data, models, predictions, and plots. This organizational structure allows us to:\n",
        "\n",
        "* Keep each region's data separate and organized\n",
        "* Store region-specific models and predictions\n",
        "* Manage regional visualizations independently\n",
        "\n",
        "The functionality is otherwise unchanged - it still mounts Google Drive and creates the base directory structure, just with added regional subdivisions."
      ],
      "metadata": {
        "id": "CuDBqgmaBj_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create Drive Directory\n",
        "def setup_drive_directory(base_path='earthquake_data'):\n",
        "    \"\"\"\n",
        "    Mount Google Drive and create necessary directories including regional subdirectories.\n",
        "\n",
        "    Args:\n",
        "        base_path (str): Base directory name for earthquake data\n",
        "\n",
        "    Returns:\n",
        "        str: Full path to the created directory\n",
        "\n",
        "    Creates directory structure:\n",
        "    /base_path/\n",
        "    ├── data/\n",
        "    │   ├── pacific_northwest/\n",
        "    │   ├── california/\n",
        "    │   ├── alaska/\n",
        "    │   ├── hawaii/\n",
        "    │   └── central_us/\n",
        "    ├── models/\n",
        "    │   ├── pacific_northwest/\n",
        "    │   ├── california/\n",
        "    │   ├── alaska/\n",
        "    │   ├── hawaii/\n",
        "    │   └── central_us/\n",
        "    ├── predictions/\n",
        "    │   ├── pacific_northwest/\n",
        "    │   ├── california/\n",
        "    │   ├── alaska/\n",
        "    │   ├── hawaii/\n",
        "    │   └── central_us/\n",
        "    └── plots/\n",
        "        ├── pacific_northwest/\n",
        "        ├── california/\n",
        "        ├── alaska/\n",
        "        ├── hawaii/\n",
        "        └── central_us/\n",
        "    \"\"\"\n",
        "    # Mount Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Create base directory path\n",
        "    full_path = f'/content/drive/My Drive/{base_path}'\n",
        "\n",
        "    # Create main directories\n",
        "    subdirs = ['data', 'models', 'predictions', 'plots']\n",
        "\n",
        "    # Create base directories\n",
        "    for subdir in subdirs:\n",
        "        dir_path = os.path.join(full_path, subdir)\n",
        "        if not os.path.exists(dir_path):\n",
        "            os.makedirs(dir_path)\n",
        "            print(f\"Created directory: {dir_path}\")\n",
        "\n",
        "        # Create regional subdirectories\n",
        "        for region in SEISMIC_REGIONS.keys():\n",
        "            region_path = os.path.join(dir_path, region)\n",
        "            if not os.path.exists(region_path):\n",
        "                os.makedirs(region_path)\n",
        "                print(f\"Created regional directory: {region_path}\")\n",
        "\n",
        "    print(f\"Directory setup complete at: {full_path}\")\n",
        "    return full_path"
      ],
      "metadata": {
        "id": "jjF-I5ief3Vk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Gather USGS Data\n",
        "**Lynn:** The revisions to Chunk 3 were made purely for documentation clarity - to highlight that the latitude/longitude data will be used for regional assignment later in the pipeline. No functional changes were needed since the raw data collection requirements remain the same regardless of regional processing."
      ],
      "metadata": {
        "id": "QRiiBc7fBqdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Gather USGS Data\n",
        "def fetch_earthquake_data(self, start_time=None, end_time=None, min_magnitude=2.5):\n",
        "    \"\"\"\n",
        "    Fetch earthquake data from USGS API for a specified time period.\n",
        "    Returns data suitable for regional processing.\n",
        "\n",
        "    Args:\n",
        "        start_time (datetime): Start date for data collection. Defaults to yesterday if None.\n",
        "        end_time (datetime): End date for data collection. Defaults to today if None.\n",
        "        min_magnitude (float): Minimum earthquake magnitude to include (default: 2.5)\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame containing earthquake data with columns:\n",
        "            - time: Timestamp of earthquake occurrence\n",
        "            - magnitude: Earthquake magnitude\n",
        "            - place: Location description\n",
        "            - longitude: Geographic longitude\n",
        "            - latitude: Geographic latitude (needed for regional assignment)\n",
        "            - depth: Depth in kilometers\n",
        "            - type: Event type\n",
        "            - alert: Alert level (if any)\n",
        "            - tsunami: Tsunami warning flag\n",
        "            - sig: Significance value\n",
        "\n",
        "    Raises:\n",
        "        requests.RequestException: If API request fails\n",
        "        ValueError: If date parameters are invalid\n",
        "    \"\"\"\n",
        "    try:\n",
        "        base_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
        "\n",
        "        if start_time is None:\n",
        "            start_time = datetime.now() - timedelta(days=1)\n",
        "\n",
        "        if end_time is None:\n",
        "            end_time = datetime.now()\n",
        "\n",
        "        params = {\n",
        "            'format': 'geojson',\n",
        "            'starttime': start_time.strftime('%Y-%m-%d'),\n",
        "            'endtime': (end_time + timedelta(days=1)).strftime('%Y-%m-%d'),\n",
        "            'minmagnitude': min_magnitude,\n",
        "            'orderby': 'time'\n",
        "        }\n",
        "\n",
        "        print(f\"Fetching data from {params['starttime']} to {params['endtime']}\")\n",
        "\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        data = response.json()\n",
        "        earthquakes = data['features']\n",
        "\n",
        "        processed_data = []\n",
        "        for quake in earthquakes:\n",
        "            properties = quake['properties']\n",
        "            coordinates = quake['geometry']['coordinates']\n",
        "\n",
        "            processed_data.append({\n",
        "                'time': datetime.fromtimestamp(properties['time'] / 1000),\n",
        "                'magnitude': properties['mag'],\n",
        "                'place': properties['place'],\n",
        "                'longitude': coordinates[0],\n",
        "                'latitude': coordinates[1],\n",
        "                'depth': coordinates[2],\n",
        "                'type': properties['type'],\n",
        "                'alert': properties.get('alert', 'none'),\n",
        "                'tsunami': properties['tsunami'],\n",
        "                'sig': properties['sig']\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(processed_data)\n",
        "\n",
        "        if len(df) > 0:\n",
        "            print(\"\\nData Collection Summary:\")\n",
        "            print(\"-\" * 30)\n",
        "            print(f\"Total earthquakes collected: {len(df)}\")\n",
        "            print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n",
        "            print(f\"Magnitude range: {df['magnitude'].min():.1f} to {df['magnitude'].max():.1f}\")\n",
        "            print(\"-\" * 30)\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "MXTJ_CfRS5k7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Fetch Training Data\n",
        "**Lynn:** Revisions were made to:\n",
        "\n",
        "* Add regional data separation by assigning each earthquake to its appropriate region\n",
        "* Save data in region-specific files rather than one global file\n",
        "* Support the regional training structure that will be used later in the pipeline"
      ],
      "metadata": {
        "id": "pJA7dT1zXNq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Fetch training data\n",
        "def fetch_training_data(self, start_date, end_date):\n",
        "    \"\"\"Fetch training data for specified date range and organize by region\"\"\"\n",
        "    df = self.fetch_earthquake_data(\n",
        "        start_time=start_date,\n",
        "        end_time=end_date,\n",
        "        min_magnitude=2.5\n",
        "    )\n",
        "\n",
        "    if df is not None:\n",
        "        # Add region assignment to data\n",
        "        df['region'] = df.apply(\n",
        "            lambda row: self.assign_region(row['latitude'], row['longitude']),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Create separate files for each region\n",
        "        for region in SEISMIC_REGIONS.keys():\n",
        "            region_data = df[df['region'] == region]\n",
        "            if len(region_data) > 0:\n",
        "                filename = f'{region}_data_{start_date.strftime(\"%Y%m%d\")}_to_{end_date.strftime(\"%Y%m%d\")}.csv'\n",
        "                filepath = os.path.join(self.drive_path, 'data', region, filename)\n",
        "                region_data.to_csv(filepath, index=False)\n",
        "\n",
        "        return df, filepath\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "tb64jht8SuO9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Fetch New Data\n",
        "**Lynn:** Revisions were made to:\n",
        "\n",
        "* Add regional assignment to new data\n",
        "* Save data separately by region instead of in one file\n",
        "* Return a dictionary of filepaths organized by region instead of a single filepath"
      ],
      "metadata": {
        "id": "NZI9fNxwXe8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Fetch new data\n",
        "def fetch_new_data(self, last_timestamp):\n",
        "    \"\"\"Fetch and organize new earthquake data by region since last timestamp\"\"\"\n",
        "    df = self.fetch_earthquake_data(\n",
        "        start_time=last_timestamp,\n",
        "        end_time=datetime.now(),\n",
        "        min_magnitude=2.5\n",
        "    )\n",
        "\n",
        "    if df is not None:\n",
        "        # Filter to only new events and assign regions\n",
        "        new_data = df[df['time'] > last_timestamp]\n",
        "        new_data['region'] = new_data.apply(\n",
        "            lambda row: self.assign_region(row['latitude'], row['longitude']),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        if len(new_data) > 0:\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "            # Save data by region\n",
        "            regional_files = {}\n",
        "            for region in SEISMIC_REGIONS.keys():\n",
        "                region_data = new_data[new_data['region'] == region]\n",
        "                if len(region_data) > 0:\n",
        "                    filename = f'{region}_new_data_{timestamp}.csv'\n",
        "                    filepath = os.path.join(self.dirs['data'], region, filename)\n",
        "                    region_data.to_csv(filepath, index=False)\n",
        "                    regional_files[region] = filepath\n",
        "\n",
        "            return new_data, regional_files\n",
        "    return None, None"
      ],
      "metadata": {
        "id": "3U37EubPSwvi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.Create Data Structure for Transformer\n",
        "\n",
        "**Lynn:** Changes made:\n",
        "\n",
        "* Added 'region' parameter to track data source\n",
        "* Added explanatory comments for each line of code\n",
        "* Maintained comprehensive docstrings\n",
        "* Updated example to show regional usage\n",
        "* Kept core functionality intact"
      ],
      "metadata": {
        "id": "9bT3FHK2CBw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Create Data Structure for Transformer\n",
        "class EarthquakeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset for handling earthquake sequence data.\n",
        "    This dataset creates sequences of earthquake data for training the transformer\n",
        "    model, where each sequence consists of multiple days of data points.\n",
        "\n",
        "    Args:\n",
        "        features (torch.Tensor): Input features for each earthquake event\n",
        "        targets (torch.Tensor): Target values for prediction\n",
        "        seq_length (int): Number of days in each sequence\n",
        "        region (str): Identifier for the seismic region this data represents\n",
        "\n",
        "    Attributes:\n",
        "        features (torch.Tensor): Storage for input features\n",
        "        targets (torch.Tensor): Storage for target values\n",
        "        seq_length (int): Length of each sequence\n",
        "        region (str): Region identifier for tracking and analysis\n",
        "\n",
        "    Methods:\n",
        "        __len__: Returns the number of sequences in the dataset\n",
        "        __getitem__: Returns a sequence and its corresponding target\n",
        "\n",
        "    Example:\n",
        "        >>> features = torch.randn(100, 5)  # 100 events with 5 features each\n",
        "        >>> targets = torch.randn(100, 1)   # Target count for each event\n",
        "        >>> dataset = EarthquakeDataset(features, targets, seq_length=7, region='california')\n",
        "        >>> sequence, target = dataset[0]  # Get first sequence and its target\n",
        "    \"\"\"\n",
        "    def __init__(self, features, targets, seq_length, region):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with features, targets, sequence length, and region.\n",
        "\n",
        "        Args:\n",
        "            features (torch.Tensor): Input features for each earthquake event\n",
        "            targets (torch.Tensor): Target values for prediction\n",
        "            seq_length (int): Number of days to include in each sequence\n",
        "            region (str): Identifier for the seismic region\n",
        "        \"\"\"\n",
        "        # Store the input features tensor for sequence creation\n",
        "        self.features = features\n",
        "        # Store the target values tensor for prediction\n",
        "        self.targets = targets\n",
        "        # Store sequence length for windowing the data\n",
        "        self.seq_length = seq_length\n",
        "        # Store region identifier for tracking and analysis\n",
        "        self.region = region\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the number of possible sequences in the dataset.\n",
        "        Accounts for sequence length when calculating available sequences.\n",
        "        \"\"\"\n",
        "        # Calculate maximum number of sequences possible given the data length and sequence length\n",
        "        return max(0, len(self.features) - self.seq_length)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a sequence of features and its corresponding target.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sequence to retrieve\n",
        "\n",
        "        Returns:\n",
        "            tuple: (feature_sequence, target) where feature_sequence is a sequence of\n",
        "                  'seq_length' days of data and target is the next day's parameters\n",
        "        \"\"\"\n",
        "        # Extract sequence of features starting at index\n",
        "        feature_seq = self.features[idx:idx + self.seq_length]\n",
        "        # Get corresponding target value\n",
        "        target = self.targets[idx + self.seq_length - 1]\n",
        "\n",
        "        return feature_seq, target"
      ],
      "metadata": {
        "id": "miri2HbbefrN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.Create Transformer\n",
        "\n",
        "**Lynn:** Changes made:\n",
        "\n",
        "* Added clear code comments for each operation\n",
        "* Added dropout parameter with default value\n",
        "* Updated documentation to reflect regional context\n",
        "* Maintained comprehensive docstrings\n",
        "* Kept core architecture unchanged as it works for both global and regional predictions"
      ],
      "metadata": {
        "id": "4qU7DAPPCdrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Create Transformer\n",
        "class TransformerPredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer-based model for regional earthquake count prediction.\n",
        "\n",
        "    This model uses a transformer architecture to learn temporal patterns in\n",
        "    earthquake sequences and predict future occurrence counts for specific regions.\n",
        "\n",
        "    Architecture:\n",
        "        - Input projection layer\n",
        "        - Positional encoding\n",
        "        - Transformer encoder layers\n",
        "        - Output projection layers\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): Dimension of input features\n",
        "        hidden_dim (int): Dimension of hidden layers\n",
        "        num_layers (int): Number of transformer layers\n",
        "        num_heads (int): Number of attention heads\n",
        "        max_seq_length (int): Maximum sequence length (default: 7)\n",
        "        dropout (float): Dropout rate (default: 0.1)\n",
        "\n",
        "    Attributes:\n",
        "        hidden_dim (int): Dimension of hidden layers\n",
        "        input_projection (nn.Linear): Input projection layer\n",
        "        pos_encoding (nn.Parameter): Positional encoding\n",
        "        transformer (nn.TransformerEncoder): Transformer encoder\n",
        "        output_projection (nn.Sequential): Output projection layers\n",
        "\n",
        "    Example:\n",
        "        >>> model = TransformerPredictor(\n",
        "                input_dim=1,\n",
        "                hidden_dim=64,\n",
        "                num_layers=2,\n",
        "                num_heads=4\n",
        "            )\n",
        "        >>> input_sequence = torch.randn(32, 7, 1)  # (batch_size, seq_length, features)\n",
        "        >>> predictions = model(input_sequence)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads, max_seq_length=7, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Store hidden dimension for use in forward pass\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Project input features to hidden dimension space\n",
        "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # Create learnable positional encoding for sequence positions\n",
        "        self.pos_encoding = nn.Parameter(torch.randn(1, max_seq_length, hidden_dim))\n",
        "\n",
        "        # Create transformer encoder layer with specified parameters\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim*4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Stack multiple encoder layers\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "        # Project transformer output to prediction space\n",
        "        self.output_projection = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim//2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Process input sequence through transformer model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, seq_length, input_dim)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predictions of shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # Ensure input has correct dimensionality\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(-1)\n",
        "\n",
        "        # Project input to hidden dimension\n",
        "        x = self.input_projection(x)\n",
        "\n",
        "        # Add positional encoding to input\n",
        "        x = x + self.pos_encoding[:, :x.size(1)]\n",
        "\n",
        "        # Apply transformer layers\n",
        "        x = self.transformer(x)\n",
        "\n",
        "        # Take final sequence element for prediction\n",
        "        x = x[:, -1]\n",
        "\n",
        "        # Project to output dimension\n",
        "        x = self.output_projection(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "fWLUJMHMfC7x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8.Regional Pipeline"
      ],
      "metadata": {
        "id": "yUo8z9JPak9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Create Regional Pipeline (Lynn)\n",
        "# Part 1 (Initialization))\n",
        "\n",
        "\n",
        "class RegionalEarthquakePipeline:\n",
        "    \"\"\"\n",
        "    Enhanced earthquake prediction pipeline with regional prediction capabilities.\n",
        "    Implements region-based data collection, processing, and model management.\n",
        "    \"\"\"\n",
        "    def __init__(self, drive_path, seq_length=7, prediction_horizon=1):\n",
        "        \"\"\"Initialize regional earthquake pipeline.\"\"\"\n",
        "        # Store basic configuration\n",
        "        self.drive_path = drive_path\n",
        "        self.seq_length = seq_length\n",
        "        self.prediction_horizon = prediction_horizon\n",
        "\n",
        "        # Set up regional components\n",
        "        self.regions = SEISMIC_REGIONS\n",
        "        self.regional_models = {}\n",
        "        self.regional_scalers = {}\n",
        "        self.regional_performance_history = {region: [] for region in SEISMIC_REGIONS.keys()}\n",
        "\n",
        "        # Create directory structure\n",
        "        self.dirs = {\n",
        "            'data': os.path.join(drive_path, 'data'),\n",
        "            'models': os.path.join(drive_path, 'models'),\n",
        "            'predictions': os.path.join(drive_path, 'predictions'),\n",
        "            'plots': os.path.join(drive_path, 'plots'),\n",
        "            'evaluations': os.path.join(drive_path, 'evaluations')\n",
        "        }\n",
        "\n",
        "        # Create regional subdirectories\n",
        "        for dir_path in self.dirs.values():\n",
        "            for region_id in self.regions.keys():\n",
        "                os.makedirs(os.path.join(dir_path, region_id), exist_ok=True)\n",
        "\n",
        "        # Initialize transformer models for each region\n",
        "        for region_id in self.regions.keys():\n",
        "            self.regional_models[region_id] = TransformerPredictor(\n",
        "                input_dim=1,  # For count prediction\n",
        "                hidden_dim=64,\n",
        "                num_layers=2,\n",
        "                num_heads=4,\n",
        "                max_seq_length=seq_length\n",
        "            )\n",
        "\n",
        "        # Initialize metadata tracking\n",
        "        self.metadata_path = os.path.join(drive_path, 'pipeline_metadata.json')\n",
        "        self.model_dates = {\n",
        "            'last_training_date': None,\n",
        "            'last_optimization_date': None,\n",
        "            'latest_data_date': None,\n",
        "            'prediction_target_date': None,\n",
        "            'first_data_date': None\n",
        "        }\n",
        "        self._load_or_create_metadata()\n",
        "\n",
        "    def _create_new_metadata(self):\n",
        "        \"\"\"Create new metadata structure with region-specific tracking.\"\"\"\n",
        "        self.metadata = {\n",
        "            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'data_dates': [],\n",
        "            'model_versions': {region: [] for region in self.regions.keys()},\n",
        "            'predictions': {region: [] for region in self.regions.keys()},\n",
        "            'evaluations': {region: [] for region in self.regions.keys()},\n",
        "            'pipeline_config': {\n",
        "                'sequence_length': self.seq_length,\n",
        "                'prediction_horizon': self.prediction_horizon,\n",
        "                'regions': list(self.regions.keys())\n",
        "            }\n",
        "        }\n",
        "        self._save_metadata()\n",
        "\n",
        "    def _load_or_create_metadata(self):\n",
        "        \"\"\"Initialize or load existing metadata.\"\"\"\n",
        "        if os.path.exists(self.metadata_path):\n",
        "            try:\n",
        "                with open(self.metadata_path, 'r') as f:\n",
        "                    self.metadata = json.load(f)\n",
        "                self._ensure_metadata_structure()\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading metadata: {str(e)}. Creating new metadata.\")\n",
        "                self._create_new_metadata()\n",
        "        else:\n",
        "            self._create_new_metadata()\n",
        "\n",
        "    def _ensure_metadata_structure(self):\n",
        "        \"\"\"Ensure all required fields exist in metadata.\"\"\"\n",
        "        required_fields = {\n",
        "            'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'data_dates': [],\n",
        "            'model_versions': {region: [] for region in self.regions.keys()},\n",
        "            'predictions': {region: [] for region in self.regions.keys()},\n",
        "            'evaluations': {region: [] for region in self.regions.keys()},\n",
        "            'pipeline_config': {\n",
        "                'sequence_length': self.seq_length,\n",
        "                'prediction_horizon': self.prediction_horizon,\n",
        "                'regions': list(self.regions.keys())\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for key, default_value in required_fields.items():\n",
        "            if key not in self.metadata:\n",
        "                self.metadata[key] = default_value\n",
        "                print(f\"Added missing metadata field: {key}\")\n",
        "\n",
        "        for region in self.regions.keys():\n",
        "            for field in ['model_versions', 'predictions', 'evaluations']:\n",
        "                if region not in self.metadata[field]:\n",
        "                    self.metadata[field][region] = []\n",
        "                    print(f\"Added missing {field} for region: {region}\")\n",
        "\n",
        "    def _save_metadata(self, verbose=False):\n",
        "        \"\"\"Save pipeline metadata to JSON file.\"\"\"\n",
        "        try:\n",
        "            metadata = {\n",
        "                'last_update': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "                'pipeline_info': {\n",
        "                    'creation_date': self.metadata.get('creation_date'),\n",
        "                    'sequence_length': self.seq_length,\n",
        "                    'regions': list(self.regions.keys())\n",
        "                },\n",
        "                'data_range': {\n",
        "                    'start': self.model_dates.get('first_data_date'),\n",
        "                    'end': self.model_dates.get('latest_data_date'),\n",
        "                    'total_days_processed': len(self.metadata.get('data_dates', []))\n",
        "                },\n",
        "                'training_info': {\n",
        "                    'last_training': self.model_dates.get('last_training_date'),\n",
        "                    'last_optimization': self.model_dates.get('last_optimization_date'),\n",
        "                    'model_versions': self.metadata.get('model_versions', {})\n",
        "                },\n",
        "                'regional_data': {\n",
        "                    region: {\n",
        "                        'predictions': self.metadata['predictions'][region],\n",
        "                        'evaluations': self.metadata['evaluations'][region]\n",
        "                    }\n",
        "                    for region in self.regions.keys()\n",
        "                }\n",
        "            }\n",
        "\n",
        "            def convert_to_serializable(obj):\n",
        "                if isinstance(obj, (np.integer, np.floating)):\n",
        "                    return float(obj)\n",
        "                elif isinstance(obj, np.ndarray):\n",
        "                    return obj.tolist()\n",
        "                elif isinstance(obj, datetime):\n",
        "                    return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
        "                elif isinstance(obj, pd.Timestamp):\n",
        "                    return obj.strftime('%Y-%m-%d %H:%M:%S')\n",
        "                return obj\n",
        "\n",
        "            def process_dict(d):\n",
        "                result = {}\n",
        "                for k, v in d.items():\n",
        "                    if isinstance(v, dict):\n",
        "                        result[k] = process_dict(v)\n",
        "                    elif isinstance(v, list):\n",
        "                        result[k] = [\n",
        "                            process_dict(item) if isinstance(item, dict)\n",
        "                            else convert_to_serializable(item)\n",
        "                            for item in v\n",
        "                        ]\n",
        "                    else:\n",
        "                        result[k] = convert_to_serializable(v)\n",
        "                return result\n",
        "\n",
        "            serializable_metadata = process_dict(metadata)\n",
        "\n",
        "            with open(self.metadata_path, 'w') as f:\n",
        "                json.dump(serializable_metadata, f, indent=4)\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Metadata saved to: {self.metadata_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving metadata: {str(e)}\")\n",
        "\n",
        "# Part 2 (Data Processing) (Lynn)\n",
        "\n",
        "    def fetch_earthquake_data(self, start_time=None, end_time=None, min_magnitude=2.5):\n",
        "        \"\"\"Fetch earthquake data from USGS API.\"\"\"\n",
        "        try:\n",
        "            base_url = \"https://earthquake.usgs.gov/fdsnws/event/1/query\"\n",
        "\n",
        "            if start_time is None:\n",
        "                start_time = datetime.now() - timedelta(days=1)\n",
        "            if end_time is None:\n",
        "                end_time = datetime.now()\n",
        "\n",
        "            params = {\n",
        "                'format': 'geojson',\n",
        "                'starttime': start_time.strftime('%Y-%m-%d'),\n",
        "                'endtime': (end_time + timedelta(days=1)).strftime('%Y-%m-%d'),\n",
        "                'minmagnitude': min_magnitude,\n",
        "                'orderby': 'time'\n",
        "            }\n",
        "\n",
        "            print(f\"Fetching data from {params['starttime']} to {params['endtime']}\")\n",
        "\n",
        "            response = requests.get(base_url, params=params)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            data = response.json()\n",
        "            earthquakes = data['features']\n",
        "\n",
        "            processed_data = []\n",
        "            for quake in earthquakes:\n",
        "                properties = quake['properties']\n",
        "                coordinates = quake['geometry']['coordinates']\n",
        "\n",
        "                processed_data.append({\n",
        "                    'time': datetime.fromtimestamp(properties['time'] / 1000),\n",
        "                    'magnitude': properties['mag'],\n",
        "                    'place': properties['place'],\n",
        "                    'longitude': coordinates[0],\n",
        "                    'latitude': coordinates[1],\n",
        "                    'depth': coordinates[2],\n",
        "                    'type': properties['type'],\n",
        "                    'alert': properties.get('alert', 'none'),\n",
        "                    'tsunami': properties['tsunami'],\n",
        "                    'sig': properties['sig']\n",
        "                })\n",
        "\n",
        "            df = pd.DataFrame(processed_data)\n",
        "\n",
        "            if len(df) > 0:\n",
        "                self._log_data_summary(df)\n",
        "\n",
        "            return df\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _log_data_summary(self, df):\n",
        "        \"\"\"Log summary of fetched data.\"\"\"\n",
        "        print(\"\\nData Collection Summary:\")\n",
        "        print(\"-\" * 30)\n",
        "        print(f\"Total earthquakes collected: {len(df)}\")\n",
        "        print(f\"Date range: {df['time'].min()} to {df['time'].max()}\")\n",
        "        print(f\"Magnitude range: {df['magnitude'].min():.1f} to {df['magnitude'].max():.1f}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    def assign_region(self, lat, lon):\n",
        "        \"\"\"Assign earthquake to appropriate seismic region based on coordinates.\"\"\"\n",
        "        for region_id, region_info in self.regions.items():\n",
        "            bounds = region_info['bounds']\n",
        "            if (bounds['min_lat'] <= lat <= bounds['max_lat'] and\n",
        "                bounds['min_lon'] <= lon <= bounds['max_lon']):\n",
        "                return region_id\n",
        "        return 'other'\n",
        "\n",
        "    def process_regional_data(self, df):\n",
        "        \"\"\"Split earthquake data into regional datasets.\"\"\"\n",
        "        if df is None or len(df) == 0:\n",
        "            return {}\n",
        "\n",
        "        # Assign region to each earthquake\n",
        "        df['region'] = df.apply(\n",
        "            lambda row: self.assign_region(row['latitude'], row['longitude']),\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Split into regional dataframes\n",
        "        regional_data = {\n",
        "            region_id: df[df['region'] == region_id].copy()\n",
        "            for region_id in self.regions.keys()\n",
        "        }\n",
        "\n",
        "        # Add 'other' region for events outside main regions\n",
        "        regional_data['other'] = df[df['region'] == 'other'].copy()\n",
        "\n",
        "        return regional_data\n",
        "\n",
        "    def prepare_regional_sequences(self, df, region_id, for_training=True):\n",
        "        \"\"\"Process earthquake data into sequences for a specific region.\"\"\"\n",
        "        try:\n",
        "            if df is None or len(df) == 0:\n",
        "                return None, None\n",
        "\n",
        "            # Convert to daily counts\n",
        "            df['date'] = pd.to_datetime(df['time']).dt.date\n",
        "            daily_counts = df.groupby('date').size().reset_index(name='count')\n",
        "            daily_counts = daily_counts.sort_values('date')\n",
        "\n",
        "            # Create sequences with proper length\n",
        "            sequences = []\n",
        "            targets = []\n",
        "\n",
        "            # Ensure we have enough data for a sequence\n",
        "            if len(daily_counts) >= self.seq_length + 1:\n",
        "                for i in range(len(daily_counts) - self.seq_length):\n",
        "                    # Create sequence using proper window\n",
        "                    seq = daily_counts['count'].iloc[i:i+self.seq_length].values\n",
        "                    target = daily_counts['count'].iloc[i+self.seq_length]\n",
        "\n",
        "                    sequences.append(seq)\n",
        "                    targets.append(target)\n",
        "\n",
        "                if sequences:\n",
        "                    sequences = torch.FloatTensor(sequences)\n",
        "                    targets = torch.FloatTensor(targets).reshape(-1, 1)\n",
        "                    return sequences, targets\n",
        "\n",
        "            # Handle cases with insufficient data\n",
        "            print(f\"Warning: Insufficient data for region {region_id}. \"\n",
        "                  f\"Need at least {self.seq_length + 1} days, got {len(daily_counts)}\")\n",
        "            return None, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error preparing sequences for region {region_id}: {str(e)}\")\n",
        "            return None, None\n",
        "\n",
        "    def save_regional_data(self, df, date_str):\n",
        "        \"\"\"Save earthquake data separated by region.\"\"\"\n",
        "        if df is None or len(df) == 0:\n",
        "            return\n",
        "\n",
        "        year_month = date_str[:7]  # YYYY-MM\n",
        "        regional_data = self.process_regional_data(df)\n",
        "\n",
        "        for region_id, region_df in regional_data.items():\n",
        "            if len(region_df) > 0:\n",
        "                # Create region-specific directory\n",
        "                region_dir = os.path.join(self.dirs['data'], region_id, year_month)\n",
        "                os.makedirs(region_dir, exist_ok=True)\n",
        "\n",
        "                # Save data\n",
        "                filename = f'earthquake_data_{date_str}.csv'\n",
        "                filepath = os.path.join(region_dir, filename)\n",
        "                region_df.to_csv(filepath, index=False)\n",
        "\n",
        "                # Save summary\n",
        "                self._save_regional_summary(region_id, region_df, date_str, region_dir)\n",
        "\n",
        "        # Update metadata\n",
        "        self.metadata['data_dates'].append(date_str)\n",
        "        self.model_dates['latest_data_date'] = date_str\n",
        "        self._save_metadata()\n",
        "\n",
        "    def _save_regional_summary(self, region_id, df, date_str, region_dir):\n",
        "        \"\"\"Save summary statistics for regional data.\"\"\"\n",
        "        summary = {\n",
        "            'date': date_str,\n",
        "            'region': region_id,\n",
        "            'total_events': len(df),\n",
        "            'magnitude_stats': {\n",
        "                'min': float(df['magnitude'].min()),\n",
        "                'max': float(df['magnitude'].max()),\n",
        "                'mean': float(df['magnitude'].mean())\n",
        "            },\n",
        "            'saved_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "        }\n",
        "\n",
        "        summary_path = os.path.join(region_dir, f'summary_{date_str}.json')\n",
        "        with open(summary_path, 'w') as f:\n",
        "            json.dump(summary, f, indent=4)\n",
        "\n",
        "    def load_regional_data(self, region_id, date_str):\n",
        "        \"\"\"Load earthquake data for a specific region and date.\"\"\"\n",
        "        try:\n",
        "            year_month = date_str[:7]\n",
        "            filename = f'earthquake_data_{date_str}.csv'\n",
        "            filepath = os.path.join(self.dirs['data'], region_id, year_month, filename)\n",
        "\n",
        "            if os.path.exists(filepath):\n",
        "                df = pd.read_csv(filepath)\n",
        "                df['time'] = pd.to_datetime(df['time'])\n",
        "                return df\n",
        "            else:\n",
        "                print(f\"No data file found for region {region_id} on {date_str}\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data for region {region_id}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# Part 3 (Training and Prediction) (Lynn)\n",
        "\n",
        "    def train_regional_model(self, region_id, sequence_tensor, target_tensor, epochs=100, batch_size=32):\n",
        "        \"\"\"Train transformer model for a specific region.\"\"\"\n",
        "        try:\n",
        "            print(f\"\\n🔄 Training model for region: {self.regions[region_id]['name']}\")\n",
        "\n",
        "            model = self.regional_models[region_id]\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "            # Create data loader\n",
        "            dataset = TensorDataset(sequence_tensor, target_tensor)\n",
        "            dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "            best_loss = float('inf')\n",
        "            for epoch in range(epochs):\n",
        "                total_loss = 0\n",
        "                for sequences, targets in dataloader:\n",
        "                    # Forward pass\n",
        "                    optimizer.zero_grad()\n",
        "                    predictions = model(sequences)\n",
        "                    loss = criterion(predictions, targets)\n",
        "\n",
        "                    # Backward pass\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                avg_loss = total_loss / len(dataloader)\n",
        "                if avg_loss < best_loss:\n",
        "                    best_loss = avg_loss\n",
        "                    self.save_model_checkpoint(region_id, epoch, avg_loss)\n",
        "                    checkpoint_saved = \"✓\"\n",
        "                else:\n",
        "                    checkpoint_saved = \" \"\n",
        "\n",
        "                if epoch % 10 == 0:\n",
        "                    print(f\"Epoch {epoch:3d}/{epochs} | Loss: {avg_loss:.4f} {checkpoint_saved}\")\n",
        "\n",
        "            return best_loss\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error training model for region {region_id}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def predict_regional_events(self, region_id, recent_data):\n",
        "        \"\"\"Generate predictions for a specific region.\"\"\"\n",
        "        try:\n",
        "            model = self.regional_models[region_id]\n",
        "            sequence_tensor, _ = self.prepare_regional_sequences(recent_data, region_id, for_training=False)\n",
        "\n",
        "            if sequence_tensor is not None:\n",
        "                with torch.no_grad():\n",
        "                    predicted_count = model(sequence_tensor)\n",
        "                    last_prediction = predicted_count[-1].item()\n",
        "\n",
        "                    prediction = {\n",
        "                        'predicted_count': int(last_prediction),\n",
        "                        'lower_bound': int(last_prediction * 0.9),\n",
        "                        'upper_bound': int(last_prediction * 1.1),\n",
        "                        'region_name': self.regions[region_id]['name']\n",
        "                    }\n",
        "\n",
        "                    return prediction\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating predictions for region {region_id}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def optimize_regional_model(self, region_id, new_data, performance_metrics):\n",
        "        \"\"\"Optimize model for a specific region using new data.\"\"\"\n",
        "        try:\n",
        "            if performance_metrics is None:\n",
        "                return\n",
        "\n",
        "            sequence_tensor, target_tensor = self.prepare_regional_sequences(new_data, region_id)\n",
        "            if sequence_tensor is None or target_tensor is None:\n",
        "                return\n",
        "\n",
        "            print(f\"\\n🔄 Optimizing model for region: {self.regions[region_id]['name']}\")\n",
        "\n",
        "            model = self.regional_models[region_id]\n",
        "            optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "            criterion = nn.MSELoss()\n",
        "\n",
        "            # Run optimization steps\n",
        "            for step in range(5):\n",
        "                optimizer.zero_grad()\n",
        "                predictions = model(sequence_tensor)\n",
        "                loss = criterion(predictions, target_tensor)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                if step % 2 == 0:\n",
        "                    print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
        "\n",
        "            return loss.item()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error optimizing model for region {region_id}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def save_model_checkpoint(self, region_id, epoch, loss, metrics=None):\n",
        "        \"\"\"Save model checkpoint for a specific region.\"\"\"\n",
        "        try:\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "            # Create region-specific model directory\n",
        "            model_dir = os.path.join(self.dirs['models'], region_id)\n",
        "            if self.model_dates['latest_data_date']:\n",
        "                model_dir = os.path.join(model_dir, self.model_dates['latest_data_date'][:7])\n",
        "            os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "            # Save model state\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': self.regional_models[region_id].state_dict(),\n",
        "                'loss': loss,\n",
        "                'metrics': metrics,\n",
        "                'timestamp': timestamp\n",
        "            }\n",
        "\n",
        "            model_path = os.path.join(model_dir, f'model_checkpoint_{timestamp}.pth')\n",
        "            torch.save(checkpoint, model_path)\n",
        "\n",
        "            # Update metadata\n",
        "            self.metadata['model_versions'][region_id].append({\n",
        "                'timestamp': timestamp,\n",
        "                'loss': float(loss),\n",
        "                'metrics': metrics\n",
        "            })\n",
        "            self._save_metadata()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving checkpoint for region {region_id}: {str(e)}\")\n",
        "\n",
        "    def load_regional_models(self):\n",
        "        \"\"\"Load latest model checkpoints for all regions.\"\"\"\n",
        "        success = True\n",
        "        for region_id in self.regions.keys():\n",
        "            try:\n",
        "                checkpoint_pattern = os.path.join(\n",
        "                    self.dirs['models'],\n",
        "                    region_id,\n",
        "                    '**',\n",
        "                    'model_checkpoint_*.pth'\n",
        "                )\n",
        "                model_files = glob.glob(checkpoint_pattern, recursive=True)\n",
        "\n",
        "                if model_files:\n",
        "                    latest_model = max(model_files, key=os.path.getctime)\n",
        "                    checkpoint = torch.load(latest_model)\n",
        "\n",
        "                    self.regional_models[region_id].load_state_dict(\n",
        "                        checkpoint['model_state_dict']\n",
        "                    )\n",
        "\n",
        "                    print(f\"Loaded model for {self.regions[region_id]['name']}\")\n",
        "                else:\n",
        "                    print(f\"No saved model found for {self.regions[region_id]['name']}\")\n",
        "                    success = False\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading model for {region_id}: {str(e)}\")\n",
        "                success = False\n",
        "\n",
        "        return success\n",
        "\n",
        "# Part 4 (Evaluation and Visualization) (Lynn)\n",
        "\n",
        "    def evaluate_regional_predictions(self, region_id, predictions, actual_data, prediction_date=None):\n",
        "        \"\"\"Evaluate predictions for a specific region.\"\"\"\n",
        "        try:\n",
        "            if predictions is None or actual_data is None:\n",
        "                return None\n",
        "\n",
        "            actual_count = len(actual_data)\n",
        "            pred_count = predictions['predicted_count']\n",
        "\n",
        "            if prediction_date is None:\n",
        "                prediction_date = actual_data['time'].dt.date.iloc[0]\n",
        "\n",
        "            metrics = {\n",
        "                'date': prediction_date,\n",
        "                'region': self.regions[region_id]['name'],\n",
        "                'predicted_count': pred_count,\n",
        "                'actual_count': actual_count,\n",
        "                'prediction_error': abs(pred_count - actual_count),\n",
        "                'within_bounds': (actual_count >= predictions['lower_bound'] and\n",
        "                                actual_count <= predictions['upper_bound']),\n",
        "                'relative_error': abs(pred_count - actual_count) / max(1, actual_count) * 100\n",
        "            }\n",
        "\n",
        "            # Update performance history and save evaluation\n",
        "            self.regional_performance_history[region_id].append(metrics)\n",
        "            self.save_evaluation_metrics(region_id, metrics, prediction_date)\n",
        "\n",
        "            # Print evaluation summary\n",
        "            print(f\"\\n📊 Evaluation for {self.regions[region_id]['name']}:\")\n",
        "            print(f\"Predicted Count: {pred_count}\")\n",
        "            print(f\"Actual Count:    {actual_count}\")\n",
        "            print(f\"Error:           {metrics['prediction_error']} events\")\n",
        "            print(f\"Relative Error:  {metrics['relative_error']:.1f}%\")\n",
        "            print(f\"Within Bounds:   {'✅' if metrics['within_bounds'] else '❌'}\")\n",
        "\n",
        "            return metrics\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating predictions for region {region_id}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def save_evaluation_metrics(self, region_id, metrics, date_str):\n",
        "        \"\"\"Save evaluation metrics for a specific region.\"\"\"\n",
        "        try:\n",
        "            # Convert date to string if it's a date object\n",
        "            if hasattr(date_str, 'strftime'):\n",
        "                date_str = date_str.strftime('%Y-%m-%d')\n",
        "\n",
        "            year_month = date_str[:7]\n",
        "            eval_dir = os.path.join(self.dirs['evaluations'], region_id, year_month)\n",
        "            os.makedirs(eval_dir, exist_ok=True)\n",
        "\n",
        "            evaluation_data = {\n",
        "                'date': str(date_str),\n",
        "                'region': self.regions[region_id]['name'],\n",
        "                'metrics': {\n",
        "                    'predicted_count': int(metrics['predicted_count']),\n",
        "                    'actual_count': int(metrics['actual_count']),\n",
        "                    'prediction_error': float(metrics['prediction_error']),\n",
        "                    'relative_error': float(metrics['relative_error']),\n",
        "                    'within_bounds': bool(metrics['within_bounds'])\n",
        "                },\n",
        "                'model_info': {\n",
        "                    'last_training': str(self.model_dates.get('last_training_date')),\n",
        "                    'last_optimization': str(self.model_dates.get('last_optimization_date'))\n",
        "                }\n",
        "            }\n",
        "\n",
        "            filepath = os.path.join(eval_dir, f'evaluation_{date_str}.json')\n",
        "            with open(filepath, 'w') as f:\n",
        "                json.dump(evaluation_data, f, indent=4)\n",
        "\n",
        "            self.metadata['evaluations'][region_id].append(evaluation_data)\n",
        "            self._save_metadata()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving evaluation metrics for region {region_id}: {str(e)}\")\n",
        "\n",
        "    def save_regional_visualization(self, start_date=None, end_date=None):\n",
        "        \"\"\"Create visualization for each region's performance.\"\"\"\n",
        "        try:\n",
        "            print(\"\\n📈 Generating Regional Performance Visualization\")\n",
        "\n",
        "            # Check for available data\n",
        "            has_data = any(len(hist) > 0 for hist in self.regional_performance_history.values())\n",
        "            if not has_data:\n",
        "                print(\"No performance data available for visualization\")\n",
        "                return\n",
        "\n",
        "            # Create subplots for each region\n",
        "            n_regions = len(self.regions)\n",
        "            fig, axes = plt.subplots(n_regions, 1, figsize=(15, 5*n_regions))\n",
        "\n",
        "            for i, (region_id, region_info) in enumerate(self.regions.items()):\n",
        "                if not self.regional_performance_history[region_id]:\n",
        "                    continue\n",
        "\n",
        "                df = pd.DataFrame(self.regional_performance_history[region_id])\n",
        "                ax = axes[i] if n_regions > 1 else axes\n",
        "\n",
        "                # Plot predicted vs actual\n",
        "                self._plot_region_performance(ax, df, region_info)\n",
        "                self._add_performance_metrics(ax, df)\n",
        "\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save the plot\n",
        "            self._save_visualization_plot()\n",
        "\n",
        "            # Print performance summary\n",
        "            self._print_performance_summary()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating visualization: {str(e)}\")\n",
        "            import traceback\n",
        "            print(traceback.format_exc())\n",
        "\n",
        "    def _plot_region_performance(self, ax, df, region_info):\n",
        "        \"\"\"Plot performance data for a specific region.\"\"\"\n",
        "        # Plot predicted values\n",
        "        ax.plot(df['date'], df['predicted_count'],\n",
        "               label='Predicted', marker='o', linestyle='-',\n",
        "               color=region_info.get('color', 'blue'))\n",
        "\n",
        "        # Plot actual values\n",
        "        ax.plot(df['date'], df['actual_count'],\n",
        "               label='Actual', marker='x', linestyle='-',\n",
        "               color='green')\n",
        "\n",
        "        # Add confidence interval\n",
        "        ax.fill_between(df['date'],\n",
        "                       df['predicted_count'] * 0.9,\n",
        "                       df['predicted_count'] * 1.1,\n",
        "                       alpha=0.2, color=region_info.get('color', 'blue'),\n",
        "                       label='90% Confidence Interval')\n",
        "\n",
        "        # Customize plot\n",
        "        ax.set_title(f'{region_info[\"name\"]} Prediction Performance')\n",
        "        ax.set_xlabel('Date')\n",
        "        ax.set_ylabel('Number of Earthquakes')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    def _add_performance_metrics(self, ax, df):\n",
        "        \"\"\"Add performance metrics to plot.\"\"\"\n",
        "        avg_error = df['prediction_error'].mean()\n",
        "        accuracy = (df['within_bounds'].sum() / len(df)) * 100\n",
        "        text = f'Avg Error: {avg_error:.1f}\\nAccuracy: {accuracy:.1f}%'\n",
        "        ax.text(0.02, 0.98, text, transform=ax.transAxes,\n",
        "               verticalalignment='top',\n",
        "               bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    def _save_visualization_plot(self):\n",
        "        \"\"\"Save visualization plot to file.\"\"\"\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        plot_path = os.path.join(\n",
        "            self.dirs['plots'],\n",
        "            f'regional_performance_{timestamp}.png'\n",
        "        )\n",
        "        plt.savefig(plot_path, bbox_inches='tight', dpi=300)\n",
        "        plt.close()\n",
        "        print(f\"✅ Regional visualization saved: {plot_path}\")\n",
        "\n",
        "    def _print_performance_summary(self):\n",
        "        \"\"\"Print summary of performance metrics for all regions.\"\"\"\n",
        "        print(\"\\n📊 Overall Performance Summary\")\n",
        "        print(\"-\" * 40)\n",
        "        for region_id, region_info in self.regions.items():\n",
        "            if self.regional_performance_history[region_id]:\n",
        "                df = pd.DataFrame(self.regional_performance_history[region_id])\n",
        "                print(f\"\\n{region_info['name']}:\")\n",
        "                print(f\"Average Error: {df['prediction_error'].mean():.1f} events\")\n",
        "                print(f\"Accuracy: {(df['within_bounds'].sum() / len(df)) * 100:.1f}%\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "# Part 5: Pipeline Execution (Lynn)\n",
        "    def run_baseline_training(self, days_to_process=31):\n",
        "        \"\"\"Execute baseline training for each region.\"\"\"\n",
        "        print(\"\\n🚀 Initializing Regional Baseline Training Pipeline\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Set date range\n",
        "        end_date = datetime.now() - timedelta(days=1)\n",
        "        start_date = end_date - timedelta(days=days_to_process)\n",
        "\n",
        "        print(f\"\\n📅 Processing Range: {start_date.date()} to {end_date.date()}\")\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        try:\n",
        "            # Fetch historical data\n",
        "            historical_data = self.fetch_earthquake_data(start_date, end_date)\n",
        "            if historical_data is None:\n",
        "                print(\"❌ Failed to fetch historical data\")\n",
        "                return False\n",
        "\n",
        "            # Process data by region\n",
        "            regional_data = self.process_regional_data(historical_data)\n",
        "\n",
        "            # Train models for each region\n",
        "            for region_id, region_df in regional_data.items():\n",
        "                if region_id == 'other' or len(region_df) == 0:\n",
        "                    continue\n",
        "\n",
        "                print(f\"\\n🔄 Processing {self.regions[region_id]['name']}\")\n",
        "\n",
        "                # Prepare sequences and train\n",
        "                sequences, targets = self.prepare_regional_sequences(region_df, region_id)\n",
        "                if sequences is not None and targets is not None:\n",
        "                    self.train_regional_model(region_id, sequences, targets)\n",
        "\n",
        "                    # Generate and evaluate predictions\n",
        "                    predictions = self.predict_regional_events(region_id, region_df)\n",
        "                    if predictions:\n",
        "                        # Get actual data for next day\n",
        "                        next_day = end_date + timedelta(days=1)\n",
        "                        actual_data = self.fetch_earthquake_data(\n",
        "                            start_time=next_day,\n",
        "                            end_time=next_day + timedelta(days=1)\n",
        "                        )\n",
        "                        if actual_data is not None:\n",
        "                            actual_regional = self.process_regional_data(actual_data)\n",
        "                            if region_id in actual_regional:\n",
        "                                self.evaluate_regional_predictions(\n",
        "                                    region_id,\n",
        "                                    predictions,\n",
        "                                    actual_regional[region_id],\n",
        "                                    next_day.date()\n",
        "                                )\n",
        "\n",
        "            # Save final visualization\n",
        "            self.save_regional_visualization()\n",
        "            print(\"\\n✅ Regional Baseline Training Completed\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Error in baseline training: {str(e)}\")\n",
        "            return False\n",
        "\n",
        "    def run_continuous_monitoring(self, update_interval=3600):\n",
        "        \"\"\"Run continuous monitoring for all regions.\"\"\"\n",
        "        try:\n",
        "            print(\"\\n🔄 Starting Regional Continuous Monitoring\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"Update Interval: {update_interval} seconds\")\n",
        "\n",
        "            while True:\n",
        "                current_time = datetime.now()\n",
        "                process_date = current_time - timedelta(days=1)\n",
        "\n",
        "                print(f\"\\n📅 Processing Data for: {process_date.date()}\")\n",
        "                print(\"-\" * 60)\n",
        "\n",
        "                # Fetch and process data\n",
        "                data = self.fetch_earthquake_data(\n",
        "                    start_time=process_date,\n",
        "                    end_time=current_time\n",
        "                )\n",
        "\n",
        "                if data is not None:\n",
        "                    regional_data = self.process_regional_data(data)\n",
        "\n",
        "                    # Process each region\n",
        "                    for region_id, region_df in regional_data.items():\n",
        "                        if region_id == 'other' or len(region_df) == 0:\n",
        "                            continue\n",
        "\n",
        "                        print(f\"\\n🔄 Processing {self.regions[region_id]['name']}\")\n",
        "\n",
        "                        # Generate predictions\n",
        "                        predictions = self.predict_regional_events(region_id, region_df)\n",
        "\n",
        "                        if predictions:\n",
        "                            # Get actual data for current period\n",
        "                            actual_data = self.fetch_earthquake_data(\n",
        "                                start_time=current_time.replace(hour=0, minute=0, second=0),\n",
        "                                end_time=current_time\n",
        "                            )\n",
        "\n",
        "                            if actual_data is not None:\n",
        "                                actual_regional = self.process_regional_data(actual_data)\n",
        "                                if region_id in actual_regional:\n",
        "                                    # Evaluate predictions\n",
        "                                    metrics = self.evaluate_regional_predictions(\n",
        "                                        region_id,\n",
        "                                        predictions,\n",
        "                                        actual_regional[region_id],\n",
        "                                        current_time.date()\n",
        "                                    )\n",
        "\n",
        "                                    # Optimize model if needed\n",
        "                                    if metrics and metrics['relative_error'] > 20:  # 20% threshold\n",
        "                                        self.optimize_regional_model(\n",
        "                                            region_id,\n",
        "                                            actual_regional[region_id],\n",
        "                                            metrics\n",
        "                                        )\n",
        "\n",
        "                    # Update visualizations\n",
        "                    self.save_regional_visualization()\n",
        "\n",
        "                # Schedule next update\n",
        "                next_update = datetime.now() + timedelta(seconds=update_interval)\n",
        "                print(f\"\\n⏰ Next Update: {next_update.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "                print(\"=\" * 60)\n",
        "                time.sleep(update_interval)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n👋 Monitoring stopped by user\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Monitoring error: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "MB_6jDT6aiVH"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9.Pipeline Initialization and Execution\n",
        "* After running, you will be prompted to enter (1): Training or (2): Monitoring\n",
        "** If (1) is selected, you will be prompted to enter a number 1-31 (default is 31) for days to train\n",
        "** If (2) is selected, you will be prompted to enter the update interval in seconds (default is 3600)"
      ],
      "metadata": {
        "id": "EAyH7VvB2sI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Pipeline Initialization and Execution\n",
        "\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Define seismic regions\n",
        "SEISMIC_REGIONS = {\n",
        "    'pacific_northwest': {\n",
        "        'name': 'Pacific Northwest',\n",
        "        'bounds': {'min_lat': 40.0, 'max_lat': 49.0, 'min_lon': -125.0, 'max_lon': -116.0},\n",
        "        'description': 'Cascadia Subduction Zone region',\n",
        "        'color': '#1f77b4'\n",
        "    },\n",
        "    'california': {\n",
        "        'name': 'California',\n",
        "        'bounds': {'min_lat': 32.0, 'max_lat': 42.0, 'min_lon': -124.0, 'max_lon': -114.0},\n",
        "        'description': 'San Andreas Fault region',\n",
        "        'color': '#ff7f0e'\n",
        "    },\n",
        "    'alaska': {\n",
        "        'name': 'Alaska',\n",
        "        'bounds': {'min_lat': 52.0, 'max_lat': 71.0, 'min_lon': -169.0, 'max_lon': -130.0},\n",
        "        'description': 'Alaska-Aleutian region',\n",
        "        'color': '#2ca02c'\n",
        "    },\n",
        "    'hawaii': {\n",
        "        'name': 'Hawaii',\n",
        "        'bounds': {'min_lat': 18.0, 'max_lat': 23.0, 'min_lon': -160.0, 'max_lon': -154.0},\n",
        "        'description': 'Hawaiian volcanic region',\n",
        "        'color': '#d62728'\n",
        "    },\n",
        "    'central_us': {\n",
        "        'name': 'Central US',\n",
        "        'bounds': {'min_lat': 35.0, 'max_lat': 40.0, 'min_lon': -97.0, 'max_lon': -89.0},\n",
        "        'description': 'New Madrid Seismic Zone',\n",
        "        'color': '#9467bd'\n",
        "    }\n",
        "}\n",
        "\n",
        "def run_earthquake_pipeline():\n",
        "    try:\n",
        "        # Mount Google Drive\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "        # Set up base directory\n",
        "        base_path = '/content/drive/My Drive/earthquake_data'\n",
        "\n",
        "        # Initialize pipeline\n",
        "        pipeline = RegionalEarthquakePipeline(drive_path=base_path)\n",
        "\n",
        "        print(\"\\nMonitoring the following seismic regions:\")\n",
        "        for region_id, info in SEISMIC_REGIONS.items():\n",
        "            print(f\"- {info['name']}: {info['description']}\")\n",
        "\n",
        "        # Choose pipeline mode\n",
        "        mode = input(\"\\nSelect mode (1: Training, 2: Monitoring): \").strip()\n",
        "\n",
        "        if mode == \"1\":\n",
        "            days = int(input(\"Enter number of days for training (default 31): \") or \"31\")\n",
        "            print(\"\\nStarting baseline training...\")\n",
        "            pipeline.run_baseline_training(days_to_process=days)\n",
        "        elif mode == \"2\":\n",
        "            interval = int(input(\"Enter update interval in seconds (default 3600): \") or \"3600\")\n",
        "            print(\"\\nStarting continuous monitoring...\")\n",
        "            pipeline.run_continuous_monitoring(update_interval=interval)\n",
        "        else:\n",
        "            print(\"Invalid mode selected\")\n",
        "            return\n",
        "\n",
        "        print(\"\\nPipeline execution completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Pipeline execution failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_earthquake_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHkLKl5q3V1l",
        "outputId": "1b1fa9c0-a295-4a6a-dd12-ced658c3fff0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Added missing metadata field: creation_date\n",
            "Added missing metadata field: data_dates\n",
            "Added missing metadata field: model_versions\n",
            "Added missing metadata field: predictions\n",
            "Added missing metadata field: evaluations\n",
            "Added missing metadata field: pipeline_config\n",
            "\n",
            "Monitoring the following seismic regions:\n",
            "- Pacific Northwest: Cascadia Subduction Zone region\n",
            "- California: San Andreas Fault region\n",
            "- Alaska: Alaska-Aleutian region\n",
            "- Hawaii: Hawaiian volcanic region\n",
            "- Central US: New Madrid Seismic Zone\n",
            "\n",
            "Select mode (1: Training, 2: Monitoring): 2\n",
            "Enter update interval in seconds (default 3600): 15\n",
            "\n",
            "Starting continuous monitoring...\n",
            "\n",
            "🔄 Starting Regional Continuous Monitoring\n",
            "============================================================\n",
            "Update Interval: 15 seconds\n",
            "\n",
            "📅 Processing Data for: 2024-11-19\n",
            "------------------------------------------------------------\n",
            "Fetching data from 2024-11-19 to 2024-11-21\n",
            "\n",
            "Data Collection Summary:\n",
            "------------------------------\n",
            "Total earthquakes collected: 44\n",
            "Date range: 2024-11-19 02:41:53.877000 to 2024-11-20 10:42:57.906000\n",
            "Magnitude range: 2.5 to 5.6\n",
            "------------------------------\n",
            "\n",
            "🔄 Processing California\n",
            "Warning: Insufficient data for region california. Need at least 8 days, got 2\n",
            "\n",
            "🔄 Processing Alaska\n",
            "Warning: Insufficient data for region alaska. Need at least 8 days, got 2\n",
            "\n",
            "📈 Generating Regional Performance Visualization\n",
            "No performance data available for visualization\n",
            "\n",
            "⏰ Next Update: 2024-11-20 14:33:16\n",
            "============================================================\n",
            "\n",
            "📅 Processing Data for: 2024-11-19\n",
            "------------------------------------------------------------\n",
            "Fetching data from 2024-11-19 to 2024-11-21\n",
            "\n",
            "Data Collection Summary:\n",
            "------------------------------\n",
            "Total earthquakes collected: 44\n",
            "Date range: 2024-11-19 02:41:53.877000 to 2024-11-20 10:42:57.906000\n",
            "Magnitude range: 2.5 to 5.6\n",
            "------------------------------\n",
            "\n",
            "🔄 Processing California\n",
            "Warning: Insufficient data for region california. Need at least 8 days, got 2\n",
            "\n",
            "🔄 Processing Alaska\n",
            "Warning: Insufficient data for region alaska. Need at least 8 days, got 2\n",
            "\n",
            "📈 Generating Regional Performance Visualization\n",
            "No performance data available for visualization\n",
            "\n",
            "⏰ Next Update: 2024-11-20 14:33:31\n",
            "============================================================\n",
            "\n",
            "📅 Processing Data for: 2024-11-19\n",
            "------------------------------------------------------------\n",
            "Fetching data from 2024-11-19 to 2024-11-21\n",
            "\n",
            "Data Collection Summary:\n",
            "------------------------------\n",
            "Total earthquakes collected: 44\n",
            "Date range: 2024-11-19 02:41:53.877000 to 2024-11-20 10:42:57.906000\n",
            "Magnitude range: 2.5 to 5.6\n",
            "------------------------------\n",
            "\n",
            "🔄 Processing California\n",
            "Warning: Insufficient data for region california. Need at least 8 days, got 2\n",
            "\n",
            "🔄 Processing Alaska\n",
            "Warning: Insufficient data for region alaska. Need at least 8 days, got 2\n",
            "\n",
            "📈 Generating Regional Performance Visualization\n",
            "No performance data available for visualization\n",
            "\n",
            "⏰ Next Update: 2024-11-20 14:33:46\n",
            "============================================================\n",
            "\n",
            "📅 Processing Data for: 2024-11-19\n",
            "------------------------------------------------------------\n",
            "Fetching data from 2024-11-19 to 2024-11-21\n",
            "\n",
            "Data Collection Summary:\n",
            "------------------------------\n",
            "Total earthquakes collected: 44\n",
            "Date range: 2024-11-19 02:41:53.877000 to 2024-11-20 10:42:57.906000\n",
            "Magnitude range: 2.5 to 5.6\n",
            "------------------------------\n",
            "\n",
            "🔄 Processing California\n",
            "Warning: Insufficient data for region california. Need at least 8 days, got 2\n",
            "\n",
            "🔄 Processing Alaska\n",
            "Warning: Insufficient data for region alaska. Need at least 8 days, got 2\n",
            "\n",
            "📈 Generating Regional Performance Visualization\n",
            "No performance data available for visualization\n",
            "\n",
            "⏰ Next Update: 2024-11-20 14:34:01\n",
            "============================================================\n",
            "\n",
            "📅 Processing Data for: 2024-11-19\n",
            "------------------------------------------------------------\n",
            "Fetching data from 2024-11-19 to 2024-11-21\n",
            "\n",
            "Data Collection Summary:\n",
            "------------------------------\n",
            "Total earthquakes collected: 44\n",
            "Date range: 2024-11-19 02:41:53.877000 to 2024-11-20 10:42:57.906000\n",
            "Magnitude range: 2.5 to 5.6\n",
            "------------------------------\n",
            "\n",
            "🔄 Processing California\n",
            "Warning: Insufficient data for region california. Need at least 8 days, got 2\n",
            "\n",
            "🔄 Processing Alaska\n",
            "Warning: Insufficient data for region alaska. Need at least 8 days, got 2\n",
            "\n",
            "📈 Generating Regional Performance Visualization\n",
            "No performance data available for visualization\n",
            "\n",
            "⏰ Next Update: 2024-11-20 14:34:17\n",
            "============================================================\n",
            "\n",
            "📅 Processing Data for: 2024-11-19\n",
            "------------------------------------------------------------\n",
            "Fetching data from 2024-11-19 to 2024-11-21\n",
            "\n",
            "Data Collection Summary:\n",
            "------------------------------\n",
            "Total earthquakes collected: 44\n",
            "Date range: 2024-11-19 02:41:53.877000 to 2024-11-20 10:42:57.906000\n",
            "Magnitude range: 2.5 to 5.6\n",
            "------------------------------\n",
            "\n",
            "🔄 Processing California\n",
            "Warning: Insufficient data for region california. Need at least 8 days, got 2\n",
            "\n",
            "🔄 Processing Alaska\n",
            "Warning: Insufficient data for region alaska. Need at least 8 days, got 2\n",
            "\n",
            "📈 Generating Regional Performance Visualization\n",
            "No performance data available for visualization\n",
            "\n",
            "⏰ Next Update: 2024-11-20 14:34:32\n",
            "============================================================\n",
            "\n",
            "📅 Processing Data for: 2024-11-19\n",
            "------------------------------------------------------------\n",
            "Fetching data from 2024-11-19 to 2024-11-21\n",
            "\n",
            "Data Collection Summary:\n",
            "------------------------------\n",
            "Total earthquakes collected: 44\n",
            "Date range: 2024-11-19 02:41:53.877000 to 2024-11-20 10:42:57.906000\n",
            "Magnitude range: 2.5 to 5.6\n",
            "------------------------------\n",
            "\n",
            "🔄 Processing California\n",
            "Warning: Insufficient data for region california. Need at least 8 days, got 2\n",
            "\n",
            "🔄 Processing Alaska\n",
            "Warning: Insufficient data for region alaska. Need at least 8 days, got 2\n",
            "\n",
            "📈 Generating Regional Performance Visualization\n",
            "No performance data available for visualization\n",
            "\n",
            "⏰ Next Update: 2024-11-20 14:34:47\n",
            "============================================================\n",
            "\n",
            "📅 Processing Data for: 2024-11-19\n",
            "------------------------------------------------------------\n",
            "Fetching data from 2024-11-19 to 2024-11-21\n",
            "\n",
            "Data Collection Summary:\n",
            "------------------------------\n",
            "Total earthquakes collected: 44\n",
            "Date range: 2024-11-19 02:41:53.877000 to 2024-11-20 10:42:57.906000\n",
            "Magnitude range: 2.5 to 5.6\n",
            "------------------------------\n",
            "\n",
            "🔄 Processing California\n",
            "Warning: Insufficient data for region california. Need at least 8 days, got 2\n",
            "\n",
            "🔄 Processing Alaska\n",
            "Warning: Insufficient data for region alaska. Need at least 8 days, got 2\n",
            "\n",
            "📈 Generating Regional Performance Visualization\n",
            "No performance data available for visualization\n",
            "\n",
            "⏰ Next Update: 2024-11-20 14:35:02\n",
            "============================================================\n",
            "\n",
            "📅 Processing Data for: 2024-11-19\n",
            "------------------------------------------------------------\n",
            "Fetching data from 2024-11-19 to 2024-11-21\n",
            "\n",
            "Data Collection Summary:\n",
            "------------------------------\n",
            "Total earthquakes collected: 44\n",
            "Date range: 2024-11-19 02:41:53.877000 to 2024-11-20 10:42:57.906000\n",
            "Magnitude range: 2.5 to 5.6\n",
            "------------------------------\n",
            "\n",
            "🔄 Processing California\n",
            "Warning: Insufficient data for region california. Need at least 8 days, got 2\n",
            "\n",
            "🔄 Processing Alaska\n",
            "Warning: Insufficient data for region alaska. Need at least 8 days, got 2\n",
            "\n",
            "📈 Generating Regional Performance Visualization\n",
            "No performance data available for visualization\n",
            "\n",
            "⏰ Next Update: 2024-11-20 14:35:17\n",
            "============================================================\n",
            "\n",
            "📅 Processing Data for: 2024-11-19\n",
            "------------------------------------------------------------\n",
            "Fetching data from 2024-11-19 to 2024-11-21\n",
            "\n",
            "Data Collection Summary:\n",
            "------------------------------\n",
            "Total earthquakes collected: 44\n",
            "Date range: 2024-11-19 02:41:53.877000 to 2024-11-20 10:42:57.906000\n",
            "Magnitude range: 2.5 to 5.6\n",
            "------------------------------\n",
            "\n",
            "🔄 Processing California\n",
            "Warning: Insufficient data for region california. Need at least 8 days, got 2\n",
            "\n",
            "🔄 Processing Alaska\n",
            "Warning: Insufficient data for region alaska. Need at least 8 days, got 2\n",
            "\n",
            "📈 Generating Regional Performance Visualization\n",
            "No performance data available for visualization\n",
            "\n",
            "⏰ Next Update: 2024-11-20 14:35:32\n",
            "============================================================\n",
            "\n",
            "👋 Monitoring stopped by user\n",
            "\n",
            "Pipeline execution completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what's missing and what needs to be added:\n",
        "\n",
        "Currently Implemented:\n",
        "\n",
        "* Deep Learning: Via the transformer model for regression\n",
        "* Basic Regression: Daily earthquake count prediction\n",
        "\n",
        "Missing Components:\n",
        "\n",
        "* Classification (from Steve's document): Could add earthquake severity classification (e.g., minor, moderate, major)\n",
        "* Dimensionality Reduction: Could apply PCA or t-SNE to analyze patterns in multi-dimensional features (magnitude, depth, location)\n",
        "* More advanced regression: Could add multivariate regression using additional features"
      ],
      "metadata": {
        "id": "Mbsmmz4ACPBB"
      }
    }
  ]
}